{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f50ebfe-b381-45ce-96f5-8a22e189ade7",
   "metadata": {},
   "source": [
    "# Forecasting with Vertex AI Tabular Workflows\n",
    "\n",
    "> Tabular Workflows is a set of integrated, fully managed, and scalable pipelines for end-to-end ML with tabular data. It leverages Google's technology for model development and provides you with customization options to fit your needs.\n",
    "\n",
    "see [Tabular Workflows: Overview](https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/overview) for more details "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575bd58-e1c0-4b16-b05c-d420fb40f726",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6c248-87d9-493a-83c8-0495027df93b",
   "metadata": {},
   "source": [
    "In this tutorial, you will use a few Vertex AI Tabular Workflows pipelines to train AutoML models using different configurations. You will see:\n",
    "- how `get_l2l_forecasting_pipeline_and_parameters` gives you the ability to customize the default AutoML Tabular pipeline\n",
    "- how `get_l2l_forecasting_pipeline_and_parameters` allows you to reduce the training time and cost for an AutoML model by using the tuning results from a previous pipeline run.\n",
    "- how `get_time_series_dense_encoder_forecasting_pipeline_and_parameters` allows you to train FastNN model\n",
    "- how to enable probabilistic inference for forecasting training pipelines\n",
    "- how to perform the batch prediction with the forecasting model trained with Tabular workflow.\n",
    "\n",
    "Learn more about [Tabular Workflow for E2E AutoML](https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/e2e-automl)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f1535-e794-42df-9bd9-0994a63b1382",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9527c03-3a89-44da-9a36-751177ea6baf",
   "metadata": {},
   "source": [
    "In this tutorial, we'll use the [Iowa Liquor dataset](https://www.kaggle.com/datasets/residentmario/iowa-liquor-sales), which is available in BigQuery Public datasets:\n",
    "\n",
    "```\n",
    "bq://bigquery-public-data.iowa_liquor_sales_forecasting.2020_sales_train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e90a5-3a9f-4caa-abe0-226e761bdef8",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6b933c-4929-40a5-8ca1-d9171e8935fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-forecas-repo/02-vf-sdk-examples\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57371f47-10df-4dba-bbca-83c625ab10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-cloud-pipeline-components --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df70960-c221-4034-b86e-c7d4e5a63e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.1.2\n",
      "google_cloud_pipeline_components version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f008eaad-bd42-471f-a26e-d3d5d6b366a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX = tab-forecast-v1\n"
     ]
    }
   ],
   "source": [
    "# naming convention for all cloud resources\n",
    "VERSION        = \"v1\"              # TODO\n",
    "PREFIX         = f'tab-forecast-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX = {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd308c4-eed9-4aa9-a205-90a8f959766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID       : hybrid-vertex\n",
      "PROJECT_NUM      : 934903580331\n",
      "LOCATION         : us-central1\n",
      "REGION           : us-central1\n",
      "BQ_LOCATION      : US\n",
      "VERTEX_SA        : 934903580331-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "PROJECT_NUM              = !gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\n",
    "PROJECT_NUM              = PROJECT_NUM[0]\n",
    "\n",
    "# locations / regions for cloud resources\n",
    "LOCATION                 = 'us-central1'                                         # TODO\n",
    "REGION                   = LOCATION                                              # TODO\n",
    "BQ_LOCATION              = 'US'                                                  # TODO\n",
    "\n",
    "# TODO: Service Account address\n",
    "VERTEX_SA                = f'{PROJECT_NUM}-compute@developer.gserviceaccount.com'  # TODO\n",
    "\n",
    "print(f\"PROJECT_ID       : {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM      : {PROJECT_NUM}\")\n",
    "print(f\"LOCATION         : {LOCATION}\")\n",
    "print(f\"REGION           : {REGION}\")\n",
    "print(f\"BQ_LOCATION      : {BQ_LOCATION}\")\n",
    "print(f\"VERTEX_SA        : {VERTEX_SA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c101afd7-b7a7-499b-98f2-a8d11bdc0b28",
   "metadata": {},
   "source": [
    "### create GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8311a1a8-7632-425f-bfa6-dfa0d683ee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_URI : gs://tab-forecast-v1-hybrid-vertex-bucket\n"
     ]
    }
   ],
   "source": [
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "print(f\"BUCKET_URI : {BUCKET_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b56dea78-57ad-4bea-944f-3eb35c8fa23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://tab-forecast-v1-hybrid-vertex-bucket/...\n"
     ]
    }
   ],
   "source": [
    "# create bucket\n",
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2144fc8-6b00-438e-ae2b-3bb03e5c978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0b1d66-c604-4727-9c61-80ac2df791b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://tab-forecast-v1-hybrid-vertex-bucket/\n",
      "No changes made to gs://tab-forecast-v1-hybrid-vertex-bucket/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{VERTEX_SA}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{VERTEX_SA}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafcd70-143b-465b-b591-da88f366edce",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1923c97b-686a-49ed-b22f-088e3c68a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Import required modules\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from google_cloud_pipeline_components.preview.automl.forecasting import \\\n",
    "    utils as automl_forecasting_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52aab31f-8044-43cd-a5c2-40123e859435",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb30cd-26d8-45ac-8847-7162292b8bad",
   "metadata": {},
   "source": [
    "## VPC config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80a10df2-2beb-44cf-b465-d00e01eb3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used.\n",
    "# Fully qualified subnetwork name is in the form of\n",
    "# https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION_NAME/subnetworks/SUBNETWORK_NAME\n",
    "# reference: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications\n",
    "dataflow_subnetwork = None  # @param {type:\"string\"}\n",
    "# Specifies whether Dataflow workers use public IP addresses.\n",
    "dataflow_use_public_ips = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05c355-fc61-444d-8813-1274ccb455c4",
   "metadata": {},
   "source": [
    "# Prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71f15c-697a-41af-940f-5a0a48f10ce6",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0570c6c-96e0-4520-bd60-efea6b2da1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket_name_and_path(uri):\n",
    "    no_prefix_uri = uri[len(\"gs://\") :]\n",
    "    splits = no_prefix_uri.split(\"/\")\n",
    "    return splits[0], \"/\".join(splits[1:])\n",
    "\n",
    "\n",
    "def download_from_gcs(uri):\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    return blob.download_as_string()\n",
    "\n",
    "\n",
    "def write_to_gcs(uri: str, content: str):\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.upload_from_string(content)\n",
    "\n",
    "\n",
    "def generate_auto_transformation(column_names: List[str]) -> List[Dict[str, Any]]:\n",
    "    transformations = []\n",
    "    for column_name in column_names:\n",
    "        transformations.append({\"auto\": {\"column_name\": column_name}})\n",
    "    return transformations\n",
    "\n",
    "\n",
    "def write_auto_transformations(uri: str, column_names: List[str]):\n",
    "    transformations = generate_auto_transformation(column_names)\n",
    "    write_to_gcs(uri, json.dumps(transformations))\n",
    "\n",
    "\n",
    "def get_task_detail(\n",
    "    task_details: List[Dict[str, Any]], task_name: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    for task_detail in task_details:\n",
    "        if task_detail.task_name == task_name:\n",
    "            return task_detail\n",
    "\n",
    "\n",
    "def get_deployed_model_uri(\n",
    "    task_details,\n",
    "):\n",
    "    ensemble_task = get_task_detail(task_details, \"model-upload\")\n",
    "    return ensemble_task.outputs[\"model\"].artifacts[0].uri\n",
    "\n",
    "\n",
    "def get_no_custom_ops_model_uri(task_details):\n",
    "    ensemble_task = get_task_detail(task_details, \"automl-tabular-ensemble\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"model_without_custom_ops\"].artifacts[0].uri\n",
    "    )\n",
    "\n",
    "\n",
    "def get_feature_attributions(\n",
    "    task_details,\n",
    "):\n",
    "    ensemble_task = get_task_detail(task_details, \"model-evaluation-2\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"evaluation_metrics\"]\n",
    "        .artifacts[0]\n",
    "        .metadata[\"explanation_gcs_path\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_evaluation_metrics(\n",
    "    task_details,\n",
    "):\n",
    "    ensemble_task = get_task_detail(task_details, \"model-evaluation\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"evaluation_metrics\"].artifacts[0].uri\n",
    "    )\n",
    "\n",
    "\n",
    "def load_and_print_json(s):\n",
    "    parsed = json.loads(s)\n",
    "    print(json.dumps(parsed, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7e2a0-c61e-4239-81db-87ffff89f0a2",
   "metadata": {},
   "source": [
    "## Define training specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e782f2-94b7-4989-a4ec-1df0e8c528e9",
   "metadata": {},
   "source": [
    "### liquor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac879c87-ff9f-431d-8021-003e40465a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = os.path.join(BUCKET_URI, f\"automl_forecasting_pipeline/run-{uuid.uuid4()}\")\n",
    "optimization_objective = \"minimize-mae\"\n",
    "time_column = \"date\"\n",
    "time_series_identifier_column = \"store_name\"\n",
    "target_column = \"sale_dollars\"\n",
    "data_source_csv_filenames = None\n",
    "data_source_bigquery_table_path = (\n",
    "    \"bq://bigquery-public-data.iowa_liquor_sales_forecasting.2020_sales_train\"\n",
    ")\n",
    "\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "predefined_split_key = None\n",
    "if predefined_split_key:\n",
    "    training_fraction = None\n",
    "    validation_fraction = None\n",
    "    test_fraction = None\n",
    "\n",
    "weight_column = None\n",
    "\n",
    "features = [\n",
    "    time_column,\n",
    "    target_column,\n",
    "    \"city\",\n",
    "    \"zip_code\",\n",
    "    \"county\",\n",
    "]\n",
    "\n",
    "available_at_forecast_columns = \",\".join([time_column])\n",
    "unavailable_at_forecast_columns = \",\".join([target_column])\n",
    "time_series_attribute_columns = \",\".join([\"city\", \"zip_code\", \"county\"])\n",
    "forecast_horizon = 30\n",
    "context_window = 30\n",
    "\n",
    "# transformations = generate_auto_transformation(features)\n",
    "# transform_config_path = os.path.join(root_dir, f\"transform_config_{uuid.uuid4()}.json\")\n",
    "# write_to_gcs(transform_config_path, json.dumps(transformations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0558b-d825-47d6-8e02-dce07bd5b23a",
   "metadata": {},
   "source": [
    "# Set Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2872078a-90b6-4221-a488-b2c540c864a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : forecast-wrkfws-tab-forecast-v1\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'forecast-wrkfws-{PREFIX}'\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff854a56-c5c2-4707-92da-d60478ad040e",
   "metadata": {},
   "source": [
    "# L2L training & customize search space and change training configuration\n",
    "\n",
    "We will create a skip evaluation AutoML Forecasting pipeline with the following customizations:\n",
    "- Limit the hyperparameter search space\n",
    "- Change machine type and tuning / training parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629e733-03ce-4658-99f3-c6abc544341e",
   "metadata": {},
   "source": [
    "### experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d493923d-be59-489f-a0d2-5ea6ca54c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_NAME          : l2l-run-20230802-045547\n",
      "ROOT_DIR          : gs://tab-forecast-v1-hybrid-vertex-bucket/forecast-wrkfws-tab-forecast-v1/l2l-run-20230802-045547\n",
      "LOG_DIR           : gs://tab-forecast-v1-hybrid-vertex-bucket/forecast-wrkfws-tab-forecast-v1/l2l-run-20230802-045547/logs\n",
      "ARTIFACTS_DIR     : gs://tab-forecast-v1-hybrid-vertex-bucket/forecast-wrkfws-tab-forecast-v1/l2l-run-20230802-045547/artifacts\n"
     ]
    }
   ],
   "source": [
    "# new experiment\n",
    "RUN_PREFIX        = \"l2l\"\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'{RUN_PREFIX}-run-{invoke_time}'\n",
    "\n",
    "ROOT_DIR          = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}\"\n",
    "LOG_DIR           = f\"{ROOT_DIR}/logs\"\n",
    "ARTIFACTS_DIR     = f\"{ROOT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345da02-aaa6-4c8d-a8b0-991ddd4642df",
   "metadata": {},
   "source": [
    "### define & submit job config\n",
    "\n",
    "* see `get_learn_to_learn_forecasting_pipeline_and_parameters` [src code](https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/preview/automl/forecasting/utils.py#L177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fdee5ff-bf15-4896-a982-92f32ee3178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_config_path    : gs://tab-forecast-v1-hybrid-vertex-bucket/forecast-wrkfws-tab-forecast-v1/l2l-run-20230802-045547/transform_config.json\n",
      "\n",
      "(\"transformations         : [{'auto': {'column_name': 'date'}}, {'auto': \"\n",
      " \"{'column_name': 'sale_dollars'}}, {'auto': {'column_name': 'city'}}, \"\n",
      " \"{'auto': {'column_name': 'zip_code'}}, {'auto': {'column_name': 'county'}}]\")\n"
     ]
    }
   ],
   "source": [
    "transformations = generate_auto_transformation(features)\n",
    "transform_config_path = os.path.join(ROOT_DIR, f\"transform_config.json\")\n",
    "\n",
    "write_to_gcs(transform_config_path, json.dumps(transformations))\n",
    "\n",
    "print(f\"transform_config_path    : {transform_config_path}\\n\")\n",
    "pprint(f\"transformations         : {transformations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25d48292-7def-46ca-ba73-71a5648eca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auto': [{'column_name': 'date'}, {'column_name': 'sale_dollars'}, {'column_name': 'city'}, {'column_name': 'zip_code'}, {'column_name': 'county'}]}\n"
     ]
    }
   ],
   "source": [
    "transformations_v = {k: [dic[k] for dic in transformations] for k in transformations[0]}\n",
    "transformations_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "323b2442-1058-4e3f-a613-4e70c4ab7da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_id           : l2l-run-20230802-045547\n",
      "template_path    : /home/jupyter/.local/lib/python3.7/site-packages/google_cloud_pipeline_components/preview/automl/forecasting/learn_to_learn_forecasting_pipeline.yaml\n",
      "(\"parameter_values : {'project': 'hybrid-vertex', 'location': 'us-central1', \"\n",
      " \"'root_dir': \"\n",
      " \"'gs://tab-forecast-v1-hybrid-vertex-bucket/forecast-wrkfws-tab-forecast-v1/l2l-run-20230802-045547', \"\n",
      " \"'target_column': 'sale_dollars', 'optimization_objective': 'minimize-mae', \"\n",
      " \"'transformations': {'auto': [{'column_name': 'date'}, {'column_name': \"\n",
      " \"'sale_dollars'}, {'column_name': 'city'}, {'column_name': 'zip_code'}, \"\n",
      " \"{'column_name': 'county'}]}, 'train_budget_milli_node_hours': 250, \"\n",
      " \"'time_column': 'date', 'time_series_identifier_column': 'store_name', \"\n",
      " \"'time_series_attribute_columns': 'city,zip_code,county', \"\n",
      " \"'available_at_forecast_columns': 'date', 'unavailable_at_forecast_columns': \"\n",
      " \"'sale_dollars', 'forecast_horizon': 30, 'context_window': 30, \"\n",
      " \"'num_selected_trials': 5, 'data_source_bigquery_table_path': \"\n",
      " \"'bq://bigquery-public-data.iowa_liquor_sales_forecasting.2020_sales_train', \"\n",
      " \"'training_fraction': 0.8, 'validation_fraction': 0.1, 'test_fraction': 0.1, \"\n",
      " \"'feature_transform_engine_dataflow_machine_type': 'n1-standard-16', \"\n",
      " \"'feature_transform_engine_dataflow_max_num_workers': 10, \"\n",
      " \"'feature_transform_engine_dataflow_disk_size_gb': 40, \"\n",
      " \"'dataflow_use_public_ips': True, \"\n",
      " \"'feature_transform_engine_bigquery_staging_full_dataset_id': '', \"\n",
      " \"'evaluation_batch_predict_machine_type': 'n1-standard-16', \"\n",
      " \"'evaluation_batch_predict_starting_replica_count': 25, \"\n",
      " \"'evaluation_batch_predict_max_replica_count': 25, \"\n",
      " \"'evaluation_dataflow_machine_type': 'n1-standard-16', \"\n",
      " \"'evaluation_dataflow_max_num_workers': 25, \"\n",
      " \"'evaluation_dataflow_disk_size_gb': 50, 'study_spec_parameters_override': \"\n",
      " \"[], 'stage_1_tuner_worker_pool_specs_override': [], \"\n",
      " \"'stage_2_trainer_worker_pool_specs_override': [], \"\n",
      " \"'enable_probabilistic_inference': False, 'run_evaluation': True, \"\n",
      " \"'group_total_weight': 0.0, 'temporal_total_weight': 0.0, \"\n",
      " \"'group_temporal_total_weight': 0.0}\")\n"
     ]
    }
   ],
   "source": [
    "# worker_pool_specs_override = [\n",
    "#     {\"machine_spec\": {\"machine_type\": \"n1-standard-8\"}},  # override for TF chief node\n",
    "#     {},  # override for TF worker node, since it's not used, leave it empty\n",
    "#     {},  # override for TF ps node, since it's not used, leave it empty\n",
    "#     {\n",
    "#         \"machine_spec\": {\n",
    "#             \"machine_type\": \"n1-standard-4\"  # override for TF evaluator node\n",
    "#         }\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials = 5\n",
    "\n",
    "train_budget_milli_node_hours = 250  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_learn_to_learn_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=ROOT_DIR,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations_v, #transform_config_path,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_column=time_series_identifier_column,\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    # stage_1_tuner_worker_pool_specs_override=worker_pool_specs_override,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    # quantile forecast, L2L without probabilistic inference requires `minimize-quantile-loss`\n",
    "    # quantiles=\",\".join(map(lambda x: str(x), [0.25, 0.5, 0.9])),\n",
    ")\n",
    "\n",
    "job_id = f\"{RUN_NAME}\"\n",
    "\n",
    "print(f\"job_id           : {job_id}\")\n",
    "print(f\"template_path    : {template_path}\")\n",
    "pprint(f\"parameter_values : {parameter_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1eb85-8d5b-417d-9bde-dcae9eb02099",
   "metadata": {},
   "source": [
    "### submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "185530a3-dee8-4e23-9358-cecf1b7330ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/l2l-run-20230802-045547\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/l2l-run-20230802-045547')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/l2l-run-20230802-045547?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/l2l-run-20230802-045547 to Experiment: forecast-wrkfws-tab-forecast-v1\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=ROOT_DIR,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# job.run(\n",
    "#     sync=False,\n",
    "#     service_account=SERVICE_ACCOUNT,\n",
    "# )\n",
    "job.submit(\n",
    "    service_account=VERTEX_SA,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9c0b9-1d1d-41b4-a735-2aeb7890083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_task_details = job.gca_resource.job_detail.task_details\n",
    "\n",
    "pprint(pipeline_task_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d0039-aa17-4d1e-bcef-6aec02988e70",
   "metadata": {},
   "source": [
    "### Skip architecture search\n",
    "Instead of doing architecture search everytime, we can reuse the existing architecture search result. This could help:\n",
    "1. reducing the variation of the output model\n",
    "2. reducing training cost\n",
    "\n",
    "The existing architecture search result is stored in the `tuning_result_output` output of the `automl-forecasting-stage-1-tuner` component. We can manually input it or get it programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a50e9c-59ff-4a90-bd39-6c2442d5cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_1_tuner_task = get_task_detail(\n",
    "    pipeline_task_details, \"automl-forecasting-stage-1-tuner\"\n",
    ")\n",
    "\n",
    "stage_1_tuning_result_artifact_uri = (\n",
    "    stage_1_tuner_task.outputs[\"tuning_result_output\"].artifacts[0].uri\n",
    ")\n",
    "\n",
    "upload_model_task = get_task_detail(\n",
    "    pipeline_task_details, \"model-upload-2\"\n",
    ")\n",
    "\n",
    "forecasting_mp_model_artifact = (\n",
    "    upload_model_task.outputs[\"model\"].artifacts[0]\n",
    ")\n",
    "\n",
    "forecasting_mp_model = aiplatform.Model(forecasting_mp_model_artifact.metadata['resourceName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be820e11-b24b-4464-82f3-abf70aa40f86",
   "metadata": {},
   "source": [
    "#### Run the skip architecture search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd18f96-de04-4e47-b472-aa5ac80300da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new experiment\n",
    "RUN_PREFIX        = \"l2l-skip-arch\"\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'{RUN_PREFIX}-run-{invoke_time}'\n",
    "\n",
    "ROOT_DIR          = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}\"\n",
    "LOG_DIR           = f\"{ROOT_DIR}/logs\"\n",
    "ARTIFACTS_DIR     = f\"{ROOT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b48ed-fb4d-43a6-8deb-624b0b8fe9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials = 5\n",
    "\n",
    "train_budget_milli_node_hours = 250  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_learn_to_learn_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=ROOT_DIR,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transform_config_path,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_column=time_series_identifier_column,\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    feature_transform_engine_dataflow_subnetwork=dataflow_subnetwork,\n",
    "    feature_transform_engine_dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    stage_1_tuning_result_artifact_uri=stage_1_tuning_result_artifact_uri,\n",
    ")\n",
    "\n",
    "job_id = f\"{RUN_NAME}\"\n",
    "\n",
    "print(f\"job_id           : {job_id}\")\n",
    "print(f\"template_path    : {template_path}\")\n",
    "pprint(f\"parameter_values : {parameter_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8892c-33a6-43aa-9606-cf069074c8dc",
   "metadata": {},
   "source": [
    "### submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cbf2ac-0595-47ad-b382-2baff558bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=ROOT_DIR,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# job.run(\n",
    "#     sync=False,\n",
    "#     service_account=SERVICE_ACCOUNT,\n",
    "# )\n",
    "job.submit(\n",
    "    service_account=VERTEX_SA,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7caae-0e79-436b-877d-9e6f13e2a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model URI\n",
    "skip_architecture_search_pipeline_task_details = (\n",
    "    job.gca_resource.job_detail.task_details\n",
    ")\n",
    "\n",
    "pprint(skip_architecture_search_pipeline_task_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c25ea3-ab3a-4065-bc19-023cd69ce32d",
   "metadata": {},
   "source": [
    "# TiDE(a.k.a. FastNN) training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671532a7-65a6-46b1-93e5-e260ec60be3d",
   "metadata": {},
   "source": [
    "### experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d617bc-112e-40f1-993b-322b2133d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new experiment\n",
    "RUN_PREFIX        = \"tide\"\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'{RUN_PREFIX}-run-{invoke_time}'\n",
    "\n",
    "ROOT_DIR          = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}\"\n",
    "LOG_DIR           = f\"{ROOT_DIR}/logs\"\n",
    "ARTIFACTS_DIR     = f\"{ROOT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21195a69-1829-426e-844e-37a0c3d1044b",
   "metadata": {},
   "source": [
    "### define & submit job config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51322f09-00c1-41e8-9cf7-709e061b10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = generate_auto_transformation(features)\n",
    "transform_config_path = os.path.join(ROOT_DIR, f\"transform_config.json\")\n",
    "\n",
    "write_to_gcs(transform_config_path, json.dumps(transformations))\n",
    "\n",
    "print(f\"transformations          : {transformations}\")\n",
    "print(f\"transform_config_path    : {transform_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4cb9a7-4f61-48a1-aac5-b13008da41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials = 5\n",
    "\n",
    "train_budget_milli_node_hours = 250  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=root_dir,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transform_config_path,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_column=time_series_identifier_column,\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    feature_transform_engine_dataflow_subnetwork=dataflow_subnetwork,\n",
    "    feature_transform_engine_dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    # enable_probabilistic_inference=True,\n",
    "    # quantile forecast, TiDE without probabilistic inference requires `minimize-quantile-loss`\n",
    "    # quantiles=\",\".join(map(lambda x: str(x), [0.25, 0.5, 0.9])),\n",
    ")\n",
    "\n",
    "job_id = f\"{RUN_NAME}\"\n",
    "\n",
    "print(f\"job_id           : {job_id}\")\n",
    "print(f\"template_path    : {template_path}\")\n",
    "pprint(f\"parameter_values : {parameter_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d7800-f4e2-4880-b200-15b8771d21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=ROOT_DIR,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# job.run(\n",
    "#     sync=False,\n",
    "#     service_account=SERVICE_ACCOUNT,\n",
    "# )\n",
    "job.submit(\n",
    "    service_account=VERTEX_SA,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd40965-8190-4a57-bc78-3d05c3eeaf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model URI\n",
    "tide_pipeline_task_details = (\n",
    "    job.gca_resource.job_detail.task_details\n",
    ")\n",
    "\n",
    "pprint(tide_pipeline_task_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af82e6-de7a-44b5-83df-0498346f4ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "233a9ba3-a45b-4f62-bf4e-5fd7978e1d76",
   "metadata": {},
   "source": [
    "# TODO \n",
    "\n",
    "* probabilistic forecast"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
