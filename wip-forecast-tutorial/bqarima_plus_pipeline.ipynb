{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73d4067-b7b0-4401-8f62-b6567fecea60",
   "metadata": {},
   "source": [
    "## BQ ARIMA+ Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b6d139-5293-41bd-afe6-c6f4fbca23d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "LOCATION: us-central1\n",
      "REGION: us-central1\n",
      "VERTEX_SA: jt-vertex-sa@hybrid-vertex.iam.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "LOCATION = 'us-central1'\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'\n",
    "VERTEX_SA = 'jt-vertex-sa@hybrid-vertex.iam.gserviceaccount.com'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"LOCATION: {LOCATION}\")\n",
    "print(f\"REGION: {REGION}\")\n",
    "print(f\"VERTEX_SA: {VERTEX_SA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4acef695-5266-4d48-a0ad-1f8b83fc10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'forecasting-1'\n",
    "SERIES = 'a-forecast-tourney'\n",
    "\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = SERIES.replace('-','_')\n",
    "BQ_TABLE = EXPERIMENT\n",
    "\n",
    "viz_limit = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e2063c4-e6aa-4dc8-9404-6c9249a7fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "from kfp.v2.dsl import HTML, Artifact, Condition, Input, Output, component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e55f05e-0daf-48a9-8f54-1e3d50720d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID, \n",
    "    location=REGION,\n",
    "    # credentials=credentials\n",
    ")\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637fe37-3cc2-4565-9269-3b462b74ef3b",
   "metadata": {},
   "source": [
    "## pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "348939b4-bfd1-4bcf-b728-5f88f9e296a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "\n",
    "! rm -rf $REPO_DOCKER_PATH_PREFIX\n",
    "! mkdir $REPO_DOCKER_PATH_PREFIX\n",
    "# !mkdir -p ./$REPO_DOCKER_PATH_PREFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247f320-e596-4dfc-9a1b-4b7504bcb289",
   "metadata": {},
   "source": [
    "### create BQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df18381-709a-440d-9128-3f66a4750a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/create_bq_dataset.py\n",
    "\n",
    "# import kfp\n",
    "# from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "# from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "#                         OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-bigquery==3.6.0'],\n",
    ")\n",
    "def create_bq_dataset(\n",
    "    project: str,\n",
    "    new_bq_dataset: str,\n",
    "    bq_location: str,\n",
    "    experiment_name: str,\n",
    "    dataset_tag: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('bq_dataset_name', str),\n",
    "    ('bq_dataset_uri', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    bq_client = bigquery.Client(project=project) # bq_location)\n",
    "    \n",
    "    ds = bigquery.Dataset(f\"{project}.{new_bq_dataset}\")\n",
    "    ds.location = bq_location #REGION\n",
    "    ds.labels = {'dataset_tag': f\"{dataset_tag}\", 'experiment': f'{experiment_name}'}\n",
    "    ds = bq_client.create_dataset(dataset = ds, exists_ok = True)\n",
    "\n",
    "    return (\n",
    "        f'{new_bq_dataset}',\n",
    "        f'bq://{project}:{new_bq_dataset}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7eb69-8012-4738-bf0d-a3b2d59e7132",
   "metadata": {},
   "source": [
    "### prepare forecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef707c1e-0a0f-4132-811d-4e426bef57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/forecast_bqarima.py\n",
    "\n",
    "# import kfp\n",
    "# from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "# from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "#                         OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-bigquery==3.6.0'],\n",
    ")\n",
    "def forecast_bqarima(\n",
    "    project: str,\n",
    "    bq_input: str,\n",
    "    bq_test: str,\n",
    "    bq_horizon: str,\n",
    "    forecast_test_length: int,\n",
    "    forecast_horizon_length: int,\n",
    "    target_column: str,\n",
    "    time_column: str,\n",
    "    series_column: str,\n",
    "    cov_unavailable: list,\n",
    "    cov_available: list,\n",
    "    cov_attribute: list\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('bq_model', str), \n",
    "    ('bq_model_query', str), \n",
    "    ('bq_output', str), \n",
    "    ('platform', str), \n",
    "    ('method', str), \n",
    "    ('scenario', str)\n",
    "]):\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import bigquery\n",
    "    bq = bigquery.Client(project = project)\n",
    "    \n",
    "    # parameters\n",
    "    table = bq_test.split('.')[-1]\n",
    "    bq_model = f\"{bq_test[:-(len(table)+1)]}.arimaplus\"\n",
    "    bq_output = f\"{bq_test[:-(len(table)+1)]}.fitted_forecast_arimaplus\"\n",
    "    platform = 'BigQuery' \n",
    "    method = 'ARIMA_PLUS'\n",
    "    scenario = 'automatic'\n",
    "    \n",
    "    logging.info(f\"table: {table}\")\n",
    "    logging.info(f\"bq_model: {bq_model}\")\n",
    "    logging.info(f\"bq_output: {bq_output}\")\n",
    "    logging.info(f\"platform: {platform}\")\n",
    "    logging.info(f\"method: {method}\")\n",
    "    \n",
    "    # ======================================\n",
    "    # Create Model: ARIMA_PLUS\n",
    "    # ======================================\n",
    "    # data_frequency should be ok as default = auto_frequency\n",
    "    queryARIMA = f\"\"\"\n",
    "        CREATE OR REPLACE MODEL `{bq_model}`\n",
    "        OPTIONS\n",
    "          (model_type = 'ARIMA_PLUS',\n",
    "           time_series_timestamp_col = '{time_column}',\n",
    "           time_series_data_col = '{target_column}',\n",
    "           time_series_id_col = '{series_column}',\n",
    "           auto_arima_max_order = 5,\n",
    "           holiday_region = 'US',\n",
    "           horizon = {forecast_test_length}+{forecast_horizon_length}\n",
    "          ) AS\n",
    "        SELECT {series_column}, {time_column}, {target_column}\n",
    "        FROM `{bq_input}`\n",
    "        WHERE splits in ('TRAIN','VALIDATE')\n",
    "    \"\"\"\n",
    "    job = bq.query(query = queryARIMA)\n",
    "    job.result()\n",
    "    \n",
    "    \n",
    "    # ======================================\n",
    "    # Create Raw Output\n",
    "    # ======================================\n",
    "    query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{bq_output}` AS\n",
    "        WITH\n",
    "            FORECAST AS (\n",
    "                SELECT\n",
    "                    {series_column}, \n",
    "                    EXTRACT(DATE from time_series_timestamp) as {time_column},\n",
    "                    time_series_adjusted_data as yhat,\n",
    "                    prediction_interval_lower_bound as yhat_lower,\n",
    "                    prediction_interval_upper_bound as yhat_upper\n",
    "                FROM ML.EXPLAIN_FORECAST(MODEL `{bq_model}`, STRUCT({forecast_test_length+forecast_horizon_length} AS horizon, 0.95 AS confidence_level))\n",
    "                WHERE time_series_type = 'forecast'\n",
    "            ),\n",
    "            ACTUAL AS (\n",
    "                SELECT {series_column}, {time_column}, {target_column}, splits\n",
    "                FROM `{bq_input}`\n",
    "                WHERE splits = 'TEST'\n",
    "            )\n",
    "        SELECT {series_column}, {time_column}, {target_column}, yhat, yhat_lower, yhat_upper, splits\n",
    "        FROM FORECAST\n",
    "        LEFT OUTER JOIN ACTUAL\n",
    "        USING ({series_column}, {time_column})\n",
    "        ORDER BY {series_column}, {time_column} \n",
    "    \"\"\"\n",
    "    job = bq.query(query = query)\n",
    "    job.result()\n",
    "    \n",
    "    # ======================================\n",
    "    # Insert Output for Tournament (first remove prior run if present)\n",
    "    # ======================================\n",
    "    query = f\"\"\"\n",
    "        DELETE `{bq_test}`\n",
    "        WHERE platform = '{platform}' and method = '{method}' and scenario = '{scenario}'\n",
    "    \"\"\"\n",
    "    job = bq.query(query)\n",
    "    job.result()\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        INSERT INTO `{bq_test}`\n",
    "        SELECT\n",
    "            '{platform}' as platform,\n",
    "            '{method}' as method,\n",
    "            '{scenario}' as scenario,\n",
    "            {series_column},\n",
    "            {time_column},\n",
    "            {target_column},\n",
    "            yhat,\n",
    "            yhat_lower,\n",
    "            yhat_upper\n",
    "        FROM `{bq_output}`\n",
    "        WHERE splits = 'TEST'\n",
    "        ORDER by {series_column}, {time_column}\n",
    "    \"\"\"\n",
    "    job = bq.query(query = query)\n",
    "    job.result()\n",
    "    \n",
    "    # Insert forecast for future horizon\n",
    "    query = f\"\"\"\n",
    "        DELETE `{bq_horizon}`\n",
    "        WHERE platform = '{platform}' and method = '{method}' and scenario = '{scenario}'\n",
    "    \"\"\"\n",
    "    job = bq.query(query)\n",
    "    job.result()\n",
    "    \n",
    "    structvars = ''\n",
    "    for i, v in enumerate(cov_unavailable+cov_available+cov_attribute+[target_column, time_column]):\n",
    "        if i>0: structvars += ', '\n",
    "        structvars += f'CAST(null AS FLOAT64) AS {v}'\n",
    "    query = f\"\"\"\n",
    "        INSERT INTO `{bq_horizon}`\n",
    "        SELECT\n",
    "            '{platform}' as platform,\n",
    "            '{method}' as method,\n",
    "            '{scenario}' as scenario,\n",
    "            {series_column},\n",
    "            {time_column},\n",
    "            {target_column},\n",
    "            yhat,\n",
    "            yhat_lower,\n",
    "            yhat_upper,\n",
    "            STRUCT({structvars}) AS feature_attributions\n",
    "        FROM `{bq_output}`\n",
    "        WHERE splits is NULL\n",
    "        ORDER by {series_column}, {time_column}\n",
    "    \"\"\"\n",
    "    job = bq.query(query = query)\n",
    "    job.result()\n",
    "    \n",
    "    return (\n",
    "        bq_model, \n",
    "        queryARIMA, \n",
    "        bq_output, \n",
    "        platform, \n",
    "        method, \n",
    "        scenario\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912c9f8-89a5-4c19-80e8-e6c5fe8c4e69",
   "metadata": {},
   "source": [
    "### plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ce394fb-01eb-4d3e-bb72-fc158f65ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/get_model_evaluation_metrics.py\n",
    "\n",
    "# import kfp\n",
    "# from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "# from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "#                         OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=[\"jinja2\", \"pandas\", \"matplotlib\"],\n",
    ")\n",
    "def get_model_evaluation_metrics(\n",
    "    metrics_in: Input[Artifact], metrics_out: Output[HTML]\n",
    ") -> NamedTuple(\"Outputs\", [(\"avg_mean_absolute_error\", float)]):\n",
    "    \"\"\"\n",
    "    Get the average mean absolute error from the metrics\n",
    "    Args:\n",
    "        metrics_in: metrics artifact\n",
    "        metrics_out: metrics artifact\n",
    "    Returns:\n",
    "        avg_mean_absolute_error: average mean absolute error\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Helpers\n",
    "    def prettyfier(styler):\n",
    "        \"\"\"\n",
    "        Helper function to prettify the metrics table.\n",
    "        Args:\n",
    "            styler: Styler object\n",
    "        Returns:\n",
    "            Styler object\n",
    "        \"\"\"\n",
    "        caption = {\n",
    "            \"selector\": \"caption\",\n",
    "            \"props\": [\n",
    "                (\"caption-side\", \"top\"),\n",
    "                (\"font-size\", \"150%\"),\n",
    "                (\"font-weight\", \"bold\"),\n",
    "                (\"font-family\", \"arial\"),\n",
    "            ],\n",
    "        }\n",
    "        headers = {\n",
    "            \"selector\": \"th\",\n",
    "            \"props\": [(\"color\", \"black\"), (\"font-family\", \"arial\")],\n",
    "        }\n",
    "        rows = {\n",
    "            \"selector\": \"td\",\n",
    "            \"props\": [(\"text-align\", \"center\"), (\"font-family\", \"arial\")],\n",
    "        }\n",
    "        styler.set_table_styles([caption, headers, rows])\n",
    "        styler.set_caption(\"Forecasting accuracy report \")\n",
    "        styler.hide(axis=\"index\")\n",
    "        styler.format(precision=2)\n",
    "        styler.background_gradient(cmap=\"Blues\")\n",
    "        return styler\n",
    "\n",
    "    def get_column_names(header):\n",
    "        \"\"\"\n",
    "        Helper function to get the column names from the metrics table.\n",
    "        Args:\n",
    "            header: header\n",
    "        Returns:\n",
    "            column_names: column names\n",
    "        \"\"\"\n",
    "        header_clean = header.replace(\"_\", \" \")\n",
    "        header_abbrev = \"\".join([h[0].upper() for h in header_clean.split()])\n",
    "        header_prettied = f\"{header_clean} ({header_abbrev})\"\n",
    "        return header_prettied\n",
    "\n",
    "    # Extract rows and schema from metrics artifact\n",
    "    rows = metrics_in.metadata[\"rows\"]\n",
    "    schema = metrics_in.metadata[\"schema\"]\n",
    "\n",
    "    # Convert into a tabular format\n",
    "    columns = [metrics[\"name\"] for metrics in schema[\"fields\"] if \"name\" in metrics]\n",
    "    records = []\n",
    "    for row in rows:\n",
    "        records.append([dl[\"v\"] for dl in row[\"f\"]])\n",
    "    metrics = (\n",
    "        pd.DataFrame.from_records(records, columns=columns, index=\"product_name\")\n",
    "        .astype(float)\n",
    "        .round(3)\n",
    "    )\n",
    "    metrics = metrics.reset_index()\n",
    "\n",
    "    # Create the HTML artifact for the metrics\n",
    "    pretty_columns = list(\n",
    "        map(\n",
    "            lambda h: get_column_names(h)\n",
    "            if h != columns[0]\n",
    "            else h.replace(\"_\", \" \").capitalize(),\n",
    "            columns,\n",
    "        )\n",
    "    )\n",
    "    pretty_metrics = metrics.copy()\n",
    "    pretty_metrics.columns = pretty_columns\n",
    "    html_metrics = pretty_metrics.style.pipe(prettyfier).to_html()\n",
    "    with open(metrics_out.path, \"w\") as f:\n",
    "        f.write(html_metrics)\n",
    "\n",
    "    # Create metrics dictionary for the model\n",
    "    avg_mean_absolute_error = round(float(metrics.mean_absolute_error.mean()), 0)\n",
    "    component_outputs = NamedTuple(\"Outputs\", [(\"avg_mean_absolute_error\", float)])\n",
    "\n",
    "    return component_outputs(avg_mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f126ff3-bf58-4466-aefd-0f770e9afe80",
   "metadata": {},
   "source": [
    "## train config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f64ccdb9-f633-4f60-9053-adc894bb05ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting arima_uni_cfg.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile arima_uni_cfg.py\n",
    "time_column='date'\n",
    "timeseries_id='timeseries_id'\n",
    "target_column='gross_quantity'\n",
    "COVARIATE_COLUMNS = [\n",
    "        'product_id',\n",
    "        'location_id',\n",
    "        'gross_quantity',\n",
    "        # 'date',\n",
    "        'weekday',\n",
    "        'wday',\n",
    "        'month',\n",
    "        'year',\n",
    "        'event_name_1',\n",
    "        'event_type_1',\n",
    "        'event_name_2',\n",
    "        'event_type_2',\n",
    "        'snap_CA',\n",
    "        'snap_TX',\n",
    "        'snap_WI',\n",
    "        'dept_id',\n",
    "        'cat_id',\n",
    "        'state_id',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3cb93e-dc73-45fe-82ac-e0c4d3b97a7a",
   "metadata": {},
   "source": [
    "# Build pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d61f22-73e9-4005-aa4c-74d0259a5fb3",
   "metadata": {},
   "source": [
    "## Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb20a8fa-e890-4dd4-97b6-16b550352e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: m5_bqarima_pipe\n",
      "RUN_NAME: run-20230307-085431\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EXPERIMENT_PREFIX = 'm5_bqarima_pipe'                     # custom identifier for organizing experiments\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}'\n",
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10321d4-86f4-42c7-acb5-228e6490bbe2",
   "metadata": {},
   "source": [
    "## pipeline vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da8d2ffa-49cd-4bdc-a813-3154d9422973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT_PATH: gs://vertex-forecast-22/m5_bqarima_pipe/v01/pipeline_root\n",
      "PIPELINE_NAME: arima-uni-m5-bqarima-pipe-v01\n"
     ]
    }
   ],
   "source": [
    "VERSION = 'v01'\n",
    "\n",
    "BUCKET_NAME = 'vertex-forecast-22'\n",
    "GCS_BUCKET_URI =f'gs://{BUCKET_NAME}'\n",
    "\n",
    "EXPERIMENT_GCS_DIR = f'gs://{BUCKET_NAME}/{EXPERIMENT_NAME}/{VERSION}'\n",
    "\n",
    "# Stores pipeline executions for each run\n",
    "PIPELINE_ROOT_PATH = f'{EXPERIMENT_GCS_DIR}/pipeline_root'\n",
    "print(f'PIPELINE_ROOT_PATH: {PIPELINE_ROOT_PATH}')\n",
    "\n",
    "PIPELINE_TAG = 'arima_uni'\n",
    "PIPELINE_NAME = f'{PIPELINE_TAG}-{EXPERIMENT_NAME}-{VERSION}'.replace('_', '-')\n",
    "print(f\"PIPELINE_NAME: {PIPELINE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cd31e39-b014-44a7-96f0-2291978b5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, NamedTuple, Optional\n",
    "\n",
    "# kfp\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from google_cloud_pipeline_components.v1.bigquery import (\n",
    "    BigqueryCreateModelJobOp, BigqueryEvaluateModelJobOp,\n",
    "    BigqueryExplainForecastModelJobOp, BigqueryForecastModelJobOp,\n",
    "    BigqueryMLArimaEvaluateJobOp, BigqueryQueryJobOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44301410-e6a5-4e4e-96a2-9fa807886c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-forecas-repo/wip-forecast-tutorial\n",
      "01-prepare-datasets.ipynb  __pycache__\t     bqarima_plus_pipeline.ipynb  src\n",
      "02-bqarima_plus.ipynb\t   arima_uni_cfg.py  custom_pipeline_spec.json\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e7e24e3-b1f1-4842-9bb9-72005488e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src import create_bq_dataset,get_model_evaluation_metrics\n",
    "\n",
    "import arima_uni_cfg\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=PIPELINE_NAME\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    dataset_tag: str,\n",
    "    bq_location: str,\n",
    "    new_bq_dataset: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    bq_source: str,\n",
    "    forecast_horizon: int,\n",
    "    bq_model_table: str,\n",
    "    bq_evaluate_time_series_configuration: dict,\n",
    "    bq_evaluate_model_configuration: dict,\n",
    "    bq_forecast_configuration: dict,\n",
    "    bq_explain_forecast_configuration: dict,\n",
    "    performance_threshold: float,\n",
    "    \n",
    "):\n",
    "    \n",
    "    # create BQ dataset\n",
    "    create_train_dataset_op = (\n",
    "      create_bq_dataset(\n",
    "          project=project,\n",
    "          new_bq_dataset=new_bq_dataset,\n",
    "          experiment_name=experiment_name,\n",
    "          dataset_tag=dataset_tag,\n",
    "          bq_location=bq_location,\n",
    "      )\n",
    "    )\n",
    "    \n",
    "    # Run an ARIMA PLUS experiment\n",
    "    bq_arima_model_exp_op = (\n",
    "        BigqueryCreateModelJobOp(\n",
    "            query=f\"\"\"\n",
    "        -- create model table\n",
    "        CREATE OR REPLACE MODEL `{project}.{new_bq_dataset}.{bq_model_table}`\n",
    "        OPTIONS(\n",
    "            MODEL_TYPE = \\'ARIMA_PLUS\\',\n",
    "            TIME_SERIES_TIMESTAMP_COL = \\'{arima_uni_cfg.time_column}\\',\n",
    "            TIME_SERIES_DATA_COL = \\'{arima_uni_cfg.target_column}\\',\n",
    "            TIME_SERIES_ID_COL = [\\'{arima_uni_cfg.timeseries_id}\\']\n",
    "        ) AS\n",
    "        SELECT\n",
    "          {arima_uni_cfg.time_column},\n",
    "          {arima_uni_cfg.timeseries_id},\n",
    "          {arima_uni_cfg.target_column}\n",
    "        FROM `{bq_source}`\n",
    "        WHERE split='TRAIN';\n",
    "        \"\"\",\n",
    "            project=project,\n",
    "            location=location,\n",
    "        )\n",
    "        .set_display_name(\"run arima+ model experiment\")\n",
    "        .after(create_train_dataset_op)\n",
    "    )\n",
    "    \n",
    "    # Evaluate ARIMA PLUS time series\n",
    "    _ = (\n",
    "        BigqueryMLArimaEvaluateJobOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            model=bq_arima_model_exp_op.outputs[\"model\"],\n",
    "            show_all_candidate_models=False,\n",
    "            job_configuration_query=bq_evaluate_time_series_configuration,\n",
    "        )\n",
    "        .set_display_name(\"evaluate arima plus time series\")\n",
    "        .after(bq_arima_model_exp_op)\n",
    "    )\n",
    "    \n",
    "    # Evaluate ARIMA Plus model\n",
    "    bq_arima_evaluate_model_op = (\n",
    "        BigqueryEvaluateModelJobOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            model=bq_arima_model_exp_op.outputs[\"model\"],\n",
    "            query_statement=f\"\"\"SELECT * FROM `{bq_source}` WHERE split='TEST'\"\"\",\n",
    "            job_configuration_query=bq_evaluate_model_configuration,\n",
    "        )\n",
    "        .set_display_name(\"evaluate arima plus model\")\n",
    "        .after(bq_arima_model_exp_op)\n",
    "    )\n",
    "    \n",
    "    # Plot model metrics\n",
    "    get_evaluation_model_metrics_op = (\n",
    "        get_model_evaluation_metrics(\n",
    "            bq_arima_evaluate_model_op.outputs[\"evaluation_metrics\"]\n",
    "        )\n",
    "        .after(bq_arima_evaluate_model_op)\n",
    "        .set_display_name(\"plot evaluation metrics\")\n",
    "    )\n",
    "    \n",
    "    # Check the model performance. If ARIMA_PLUS average MAE metric is below to a minimal threshold\n",
    "    with Condition(\n",
    "        get_evaluation_model_metrics_op.outputs[\"avg_mean_absolute_error\"]\n",
    "        < performance_threshold,\n",
    "        name=\"avg. mae good\",\n",
    "    ):\n",
    "        # Train the ARIMA PLUS model\n",
    "        bq_arima_model_op = (\n",
    "            BigqueryCreateModelJobOp(\n",
    "                query=f\"\"\"\n",
    "        -- create model table\n",
    "        CREATE OR REPLACE MODEL `{project}.{new_bq_dataset}.{bq_model_table}`\n",
    "        OPTIONS(\n",
    "        MODEL_TYPE = \\'ARIMA_PLUS\\',\n",
    "        TIME_SERIES_TIMESTAMP_COL = \\'{arima_uni_cfg.time_column}\\',\n",
    "        TIME_SERIES_DATA_COL = \\'{arima_uni_cfg.target_column}\\',\n",
    "        TIME_SERIES_ID_COL = [\\'{arima_uni_cfg.timeseries_id}\\'],\n",
    "        MODEL_REGISTRY = \\'vertex_ai\\',\n",
    "        VERTEX_AI_MODEL_ID = \\'order_demand_forecasting\\',\n",
    "        VERTEX_AI_MODEL_VERSION_ALIASES = [\\'staging\\']\n",
    "        ) AS\n",
    "        SELECT\n",
    "          {arima_uni_cfg.time_column},\n",
    "          {arima_uni_cfg.target_column},\n",
    "          {arima_uni_cfg.timeseries_id},\n",
    "          FROM `{bq_source}`\n",
    "        GROUP BY {arima_uni_cfg.time_column}, {arima_uni_cfg.timeseries_id};\n",
    "        \"\"\",\n",
    "                project=project,\n",
    "                location=location,\n",
    "            )\n",
    "            .set_display_name(\"train arima+ model\")\n",
    "            .after(get_evaluation_model_metrics_op)\n",
    "        )\n",
    "        \n",
    "        # Generate the ARIMA PLUS forecasts\n",
    "        bq_arima_forecast_op = (\n",
    "            BigqueryForecastModelJobOp(\n",
    "                project=project,\n",
    "                location=location,\n",
    "                model=bq_arima_model_op.outputs[\"model\"],\n",
    "                horizon=forecast_horizon,  # 1 hour\n",
    "                confidence_level=0.9,\n",
    "                job_configuration_query=bq_forecast_configuration,\n",
    "            )\n",
    "            .set_display_name(\"generate hourly forecasts\")\n",
    "            .after(get_evaluation_model_metrics_op)\n",
    "        )\n",
    "        \n",
    "        # Generate the ARIMA PLUS forecast explainations\n",
    "        _ = (\n",
    "            BigqueryExplainForecastModelJobOp(\n",
    "                project=project,\n",
    "                location=location,\n",
    "                model=bq_arima_model_op.outputs[\"model\"],\n",
    "                horizon=forecast_horizon,  # 1 hour\n",
    "                confidence_level=0.9,\n",
    "                job_configuration_query=bq_explain_forecast_configuration,\n",
    "            )\n",
    "            .set_display_name(\"explain hourly forecasts\")\n",
    "            .after(bq_arima_forecast_op)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba15662-0f3e-44c4-9ece-ea55daf724b5",
   "metadata": {},
   "source": [
    "## compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8203a7f-b6d0-4e17-bec4-e3b0d9660cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON_SPEC_LOCAL = \"custom_pipeline_spec.json\"\n",
    "\n",
    "! rm -f $PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1ed5a-ed2a-4989-a77f-3c2b90d01387",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c2796ec-d4bb-4d69-8ce2-e4af8628010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TAG = 'm5'\n",
    "BQ_LOCATION='US' # 'us'\n",
    "NEW_BQ_DATASET = f'{EXPERIMENT_NAME}_{VERSION}'\n",
    "BQ_SOURCE='hybrid-vertex.m5_us.combined_small20k_train' # combined_full_train\n",
    "\n",
    "FORECAST_HORIZON=14\n",
    "\n",
    "\n",
    "BQ_MODEL_TABLE_PREFIX = \"orders_forecast_arima\"\n",
    "BQ_MODEL_TABLE = f\"{BQ_MODEL_TABLE_PREFIX}_{VERSION}\"\n",
    "\n",
    "BQ_TRAINING_TABLE_PREFIX = \"orders_training\"\n",
    "BQ_TRAINING_TABLE = f\"{BQ_TRAINING_TABLE_PREFIX}_{VERSION}\"\n",
    "BQ_TRAIN_CONFIGURATION = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": NEW_BQ_DATASET,\n",
    "        \"tableId\": BQ_TRAINING_TABLE,\n",
    "    },\n",
    "    \"writeDisposition\": \"WRITE_TRUNCATE\",\n",
    "}\n",
    "\n",
    "BQ_EVALUATE_TS_TABLE_PREFIX = \"orders_arima_time_series_evaluate\"\n",
    "BQ_EVALUATE_TS_TABLE = f\"{BQ_EVALUATE_TS_TABLE_PREFIX}_{VERSION}\"\n",
    "BQ_EVALUATE_TS_CONFIGURATION = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": NEW_BQ_DATASET,\n",
    "        \"tableId\": BQ_EVALUATE_TS_TABLE,\n",
    "    },\n",
    "    \"writeDisposition\": \"WRITE_TRUNCATE\",\n",
    "}\n",
    "\n",
    "BQ_EVALUATE_MODEL_TABLE_PREFIX = \"orders_arima_model_evaluate\"\n",
    "BQ_EVALUATE_MODEL_TABLE = f\"{BQ_EVALUATE_MODEL_TABLE_PREFIX}_{VERSION}\"\n",
    "BQ_EVALUATE_MODEL_CONFIGURATION = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": NEW_BQ_DATASET,\n",
    "        \"tableId\": BQ_EVALUATE_MODEL_TABLE,\n",
    "    },\n",
    "    \"writeDisposition\": \"WRITE_TRUNCATE\",\n",
    "}\n",
    "\n",
    "BQ_FORECAST_TABLE_PREFIX = \"orders_arima_forecast\"\n",
    "BQ_FORECAST_TABLE = f\"{BQ_FORECAST_TABLE_PREFIX}_{VERSION}\"\n",
    "BQ_FORECAST_CONFIGURATION = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": NEW_BQ_DATASET,\n",
    "        \"tableId\": BQ_FORECAST_TABLE,\n",
    "    },\n",
    "    \"writeDisposition\": \"WRITE_TRUNCATE\",\n",
    "}\n",
    "\n",
    "BQ_EXPLAIN_FORECAST_TABLE_PREFIX = \"orders_arima_explain_forecast\"\n",
    "BQ_EXPLAIN_FORECAST_TABLE = f\"{BQ_EXPLAIN_FORECAST_TABLE_PREFIX}_{VERSION}\"\n",
    "BQ_EXPLAIN_FORECAST_CONFIGURATION = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": NEW_BQ_DATASET,\n",
    "        \"tableId\": BQ_EXPLAIN_FORECAST_TABLE,\n",
    "    },\n",
    "    \"writeDisposition\": \"WRITE_TRUNCATE\",\n",
    "}\n",
    "PERF_THRESHOLD = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5c83c48-6b16-49e4-9e41-f8b26f0ae5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINES_FILEPATH: gs://vertex-forecast-22/m5_bqarima_pipe/v01/pipeline_root/pipeline_spec.json\n",
      "Copying file://custom_pipeline_spec.json [Content-Type=application/json]...\n",
      "/ [1 files][ 43.4 KiB/ 43.4 KiB]                                                \n",
      "Operation completed over 1 objects/43.4 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT_PATH}/pipeline_spec.json'\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "# copy pipeline spec to gcs path\n",
    "!gsutil cp $PIPELINE_JSON_SPEC_LOCAL $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcb07728-4afa-4468-adc0-72e93293f29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/arima-uni-m5-bqarima-pipe-v01-20230307085437\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/arima-uni-m5-bqarima-pipe-v01-20230307085437')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/arima-uni-m5-bqarima-pipe-v01-20230307085437?project=934903580331\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/arima-uni-m5-bqarima-pipe-v01-20230307085437 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/arima-uni-m5-bqarima-pipe-v01-20230307085437 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/arima-uni-m5-bqarima-pipe-v01-20230307085437 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/arima-uni-m5-bqarima-pipe-v01-20230307085437 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "# vpc_network_name = 'ucaip-haystack-vpc-network'\n",
    "# SERVICE_ACCOUNT = '934903580331-compute@developer.gserviceaccount.com'\n",
    "# SERVICE_ACCOUNT = 'notebooksa@hybrid-vertex.iam.gserviceaccount.com'\n",
    "\n",
    "\n",
    "job = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINES_FILEPATH,\n",
    "    pipeline_root=f'{PIPELINE_ROOT_PATH}',\n",
    "    failure_policy='fast', # slow | fast\n",
    "    # enable_caching=False,\n",
    "    parameter_values={\n",
    "        # here\n",
    "        'project':PROJECT_ID,\n",
    "        'location':REGION,\n",
    "        'version':VERSION,\n",
    "        'dataset_tag':DATASET_TAG,\n",
    "        'bq_location':BQ_LOCATION,\n",
    "        'new_bq_dataset':NEW_BQ_DATASET,\n",
    "        'experiment_name':EXPERIMENT_NAME,\n",
    "        'experiment_run':RUN_NAME,\n",
    "        'bq_source':BQ_SOURCE,\n",
    "        'forecast_horizon':FORECAST_HORIZON,\n",
    "        'bq_model_table':BQ_MODEL_TABLE,\n",
    "        'bq_evaluate_time_series_configuration':BQ_EVALUATE_TS_CONFIGURATION,\n",
    "        'bq_evaluate_model_configuration':BQ_EVALUATE_MODEL_CONFIGURATION,\n",
    "        'bq_forecast_configuration':BQ_FORECAST_CONFIGURATION,\n",
    "        'bq_explain_forecast_configuration':BQ_EXPLAIN_FORECAST_CONFIGURATION,\n",
    "        'performance_threshold': PERF_THRESHOLD,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "EXPERIMENT_PREFIX = 'm5-bqarima_pipe'                     # custom identifier for organizing experiments\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}'\n",
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "job.run(\n",
    "    sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    "    # network=f'projects/{PROJECT_NUM}/global/networks/{vpc_network_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a18c59-f9ff-47a5-a4ab-f2f187a6a811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
