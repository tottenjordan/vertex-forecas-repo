{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35dc312e-aa9e-4d09-944f-100eb7ee6509",
   "metadata": {},
   "source": [
    "# Demand Forecasting with Vertex Forecast on Tabular Workflows\n",
    "\n",
    "**Objectives**\n",
    "* train with and forecast *Iowa liquor BigQuery public dataset*\n",
    "* Use Tabular Workflows to orchestrate Vertex Forecast pipeline\n",
    "* Track experiments\n",
    "* Run model evalutaions for trained forecast models\n",
    "\n",
    "**TODOs**\n",
    "* `skip architecture search` in a retraining pipeline\n",
    "* upload v2 of a model and its evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fd63e1-a2e6-44d8-a956-ee602430fbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install {USER_FLAG} google-cloud-aiplatform kfp google-cloud-pipeline-components --upgrade\n",
    "# !pip3 install --no-cache-dir {USER_FLAG} PyYAML==5.3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f338bf-70d6-4464-89dd-f87a45d99cce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.4.0\n",
      "google_cloud_pipeline_components version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa145e15-4650-4e2e-a5a8-ea9f71816ef1",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "> use the prefix defined in 00-env-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb269c2c-5697-4f9e-ab5b-2bd9f0868ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_NEW_ASSETS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ae3b82-a0f7-41ed-9e2c-3a0552974d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX = forecast-refresh-v1\n"
     ]
    }
   ],
   "source": [
    "# naming convention for all cloud resources\n",
    "VERSION        = \"v1\"              # TODO\n",
    "PREFIX         = f'forecast-refresh-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX = {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1368cb3-2cdf-4571-9281-c847cead0318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"forecast-refresh-v1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "BUCKET_NAME              = \"forecast-refresh-v1-hybrid-vertex-gcs\"\n",
      "BUCKET_URI               = \"gs://forecast-refresh-v1-hybrid-vertex-gcs\"\n",
      "\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://forecast-refresh-v1-hybrid-vertex-gcs/data\"\n",
      "\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-gcs'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fd1173-e7ec-45d2-ac4f-ef6f35f1e18e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/config/\n"
     ]
    }
   ],
   "source": [
    "# For a list of available model metrics, go here:\n",
    "!gsutil ls $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d40790-c76b-4473-8b94-836131bd8cb4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adde7be5-33b5-4ea0-befe-96c895a42225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tide-twrkflow-eval-v1\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_TAG     = \"tide-twrkflow-eval\"\n",
    "EXPERIMENT_VERSION = \"v1\"\n",
    "\n",
    "EXPERIMENT_NAME = f\"{EXPERIMENT_TAG}-{EXPERIMENT_VERSION}\"\n",
    "\n",
    "print(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4986ccbd-7a47-4a01-a673-3fc0c706192d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import json\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from google.cloud import aiplatform, storage, bigquery\n",
    "\n",
    "# from google_cloud_pipeline_components.types.artifact_types import VertexDataset\n",
    "from google_cloud_pipeline_components.preview.automl.forecasting import \\\n",
    "    utils as automl_forecasting_utils\n",
    "\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "aiplatform.init(\n",
    "    experiment=EXPERIMENT_NAME, \n",
    "    project=PROJECT_ID, \n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decaf69b-37d9-4319-a1a1-fef32de88954",
   "metadata": {},
   "source": [
    "## Create BigQuery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca65b389-d25a-4724-b2ea-c8c7cb3b88c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(DatasetReference('hybrid-vertex', 'tide_twrkflow_eval_v1'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BIGQUERY_DATASET_NAME = EXPERIMENT_NAME.replace(\"-\",\"_\")\n",
    "\n",
    "if CREATE_NEW_ASSETS:\n",
    "    ds = bigquery.Dataset(f\"{PROJECT_ID}.{BIGQUERY_DATASET_NAME}\")\n",
    "    ds.location = BQ_LOCATION\n",
    "    ds = bq_client.create_dataset(dataset = ds, exists_ok = False)\n",
    "    # print(ds.full_dataset_id)\n",
    "else:\n",
    "    ds = bigquery.Dataset(f\"{PROJECT_ID}.{BIGQUERY_DATASET_NAME}\")\n",
    "    \n",
    "ds \n",
    "# ds.dataset_id\n",
    "# ds.full_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45f81e-9371-469e-97a7-6f747454df2e",
   "metadata": {},
   "source": [
    "## prepare train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe6f627-9f46-42c8-8699-1c7b8472e1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR = gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/tide-twrkflow-eval-v1/run-2907_22_44_752082\n"
     ]
    }
   ],
   "source": [
    "# Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used.\n",
    "dataflow_subnetwork           = None \n",
    "\n",
    "# Specifies whether Dataflow workers use public IP addresses.\n",
    "dataflow_use_public_ips       = True\n",
    "\n",
    "NOW                           = datetime.datetime.now().strftime(\"%d %H:%M:%S.%f\").replace(\" \",\"\").replace(\":\",\"_\").replace(\".\",\"_\")\n",
    "ROOT_DIR                      = f\"{BUCKET_URI}/automl_forecasting_pipeline/{EXPERIMENT_NAME}/run-{NOW}\"\n",
    "time_column                   = \"date\"\n",
    "time_series_identifier_column = \"store_name\"\n",
    "target_column                 = \"sale_dollars\"\n",
    "data_source_csv_filenames     = None\n",
    "\n",
    "print(f\"ROOT_DIR = {ROOT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aed5e93-24b2-4161-8666-da43fa78a872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available_at_forecast_columns    = ['date']\n",
      "unavailable_at_forecast_columns  = ['sale_dollars']\n",
      "time_series_attribute_columns    = ['city', 'zip_code', 'county']\n"
     ]
    }
   ],
   "source": [
    "data_source_bigquery_table_path = (\n",
    "    \"bq://bigquery-public-data.iowa_liquor_sales_forecasting.2020_sales_train\"\n",
    ")\n",
    "\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "predefined_split_key = None\n",
    "if predefined_split_key:\n",
    "    training_fraction = None\n",
    "    validation_fraction = None\n",
    "    test_fraction = None\n",
    "\n",
    "weight_column = None\n",
    "\n",
    "features = [\n",
    "    time_column,\n",
    "    target_column,\n",
    "    \"city\",\n",
    "    \"zip_code\",\n",
    "    \"county\",\n",
    "]\n",
    "\n",
    "available_at_forecast_columns = [time_column]\n",
    "unavailable_at_forecast_columns = [target_column]\n",
    "time_series_attribute_columns = [\"city\", \"zip_code\", \"county\"]\n",
    "\n",
    "forecast_horizon = 150\n",
    "context_window = 150\n",
    "\n",
    "print(f\"available_at_forecast_columns    = {available_at_forecast_columns}\")\n",
    "print(f\"unavailable_at_forecast_columns  = {unavailable_at_forecast_columns}\")\n",
    "print(f\"time_series_attribute_columns    = {time_series_attribute_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dff50d5c-1bef-4f95-92ba-a5877d360460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformations       = {'auto': ['date', 'sale_dollars', 'city', 'zip_code', 'county'], 'numeric': [], 'categorical': [], 'text': [], 'timestamp': []}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformations = helpers.generate_auto_transformation(features)\n",
    "transformations = helpers.generate_transformation(auto_column_names=features)\n",
    "\n",
    "# TRANSFORM_CONFIG_PATH = f\"{ROOT_DIR}/transform_config_{NOW}.json\"\n",
    "# TRANSFORM_CONFIG_PATH = \"gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/run-28ec73a7-646e-420b-b883-2aa16ea2e518/transform_config_40ac07bd-c92b-4914-beda-18a382062acd.json\"\n",
    "\n",
    "print(f\"transformations       = {transformations}\\n\")\n",
    "# print(f\"TRANSFORM_CONFIG_PATH = {TRANSFORM_CONFIG_PATH}\")\n",
    "\n",
    "# helpers.write_to_gcs(TRANSFORM_CONFIG_PATH, json.dumps(transformations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ae7f8b-da1e-419c-b50a-17a69db9436e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/config/\n"
     ]
    }
   ],
   "source": [
    "# For a list of available model metrics, go here:\n",
    "!gsutil ls $BUCKET_URI/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca7d3f-7120-4f9c-9a69-eecd0aa3a307",
   "metadata": {},
   "source": [
    "# Vertex Forecast Training\n",
    "\n",
    "**Optimization Objectives** ([docs](https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting-parameters#optimization-objectives))\n",
    "\n",
    "| Objective  | API                      | Use case |\n",
    "| :--------: | :------------:           | :------------------------------------- |\n",
    "| RMSE       | `minimize-rmse`          | Minimize root-mean-squared error (RMSE). Captures more extreme values accurately and is less biased when aggregating predictions.Default value. |\n",
    "| MAE        | `minimize-mae`           | Minimize mean-absolute error (MAE). Views extreme values as outliers with less impact on model. |\n",
    "| RMSLE      | `minimize-rmsle`         | Minimize root-mean-squared log error (RMSLE). Penalizes error on relative size rather than absolute value. Useful when both predicted and actual values can be large. |\n",
    "| RMSPE      | `minimize-rmspe`         | Minimize root-mean-squared percentage error (RMSPE). Captures a large range of values accurately. Similar to RMSE, but relative to target magnitude. Useful when the range of values is large. |\n",
    "| WAPE       | `minimize-wape-mae`      | Minimize the combination of weighted absolute percentage error (WAPE) and mean-absolute-error (MAE). Useful when the actual values are low. |\n",
    "| QUANTILE   | `minimize-quantile-loss` | Minimize the scaled pinball loss of the defined quantiles to quantify uncertainty in estimates. Quantile predictions quantify the uncertainty of predictions. They measure the likelihood of a prediction being within a range. |\n",
    "\n",
    "\n",
    "**TiDE on Vertex Tabluar Workflows**\n",
    "* [src](https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/preview/automl/forecasting/utils.py#L413)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20af59-98bf-4f7d-855e-d7316aa0e512",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "* add `with dsl.ParallelFor(LIST) as cw:` for parallel jobs with diff params (e.g., statmike [example](https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%20Forecasting/Vertex%20AI%20Pipelines%20-%20Forecasting%20Tournament%20with%20Kubeflow%20Pipelines%20(KFP).ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d5c8289-e747-4905-aa05-f2fd0db54eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_ID = tide-tide-twrkflow-eval-v1\n"
     ]
    }
   ],
   "source": [
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials           = 5\n",
    "train_budget_milli_node_hours = 500  # 30 minutes\n",
    "\n",
    "optimization_objective        = \"minimize-wape-mae\" \n",
    "\n",
    "RUN_EVALUATION                = True\n",
    "\n",
    "PROBABILISTIC_INFER           = False\n",
    "# QUANTILES                     = [0.25, 0.5, 0.9] # [0.05, 0.25, 0.50, 0.75, 0.95]\n",
    "\n",
    "JOB_ID                        = f\"tide-{EXPERIMENT_NAME}\"\n",
    "\n",
    "print(f\"JOB_ID = {JOB_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45834c-a982-4ef2-941e-080ee0c358b8",
   "metadata": {},
   "source": [
    "## (1) TiDE - full AutoML train & eval\n",
    "\n",
    "TiDE stands for \"Time series Dense Encoder\", which is a new model type in Vertex Forecasting and has the best training and inference performance while not sacrificing any model quality.\n",
    "\n",
    "For more details, please see https://ai.googleblog.com/2023/04/recent-advances-in-deep-long-horizon.html\n",
    "\n",
    "You will create a skip evaluation AutoML Forecasting pipeline with the following customizations:\n",
    "- Limit the hyperparameter search space\n",
    "- Change machine type and tuning / training parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa5a824e-8e1a-40c3-8473-967e8264a398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/tide-tide-twrkflow-eval-v1\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/tide-tide-twrkflow-eval-v1')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tide-tide-twrkflow-eval-v1?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/tide-tide-twrkflow-eval-v1 to Experiment: tide-twrkflow-eval-v1\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=ROOT_DIR,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    run_evaluation=RUN_EVALUATION,                          # set True to eval on test/valid set\n",
    "    evaluated_examples_bigquery_path=f'bq://{PROJECT_ID}.{BIGQUERY_DATASET_NAME}',\n",
    "    enable_probabilistic_inference=PROBABILISTIC_INFER,\n",
    "    \n",
    "    ### quantile forecast\n",
    "    # quantiles=QUANTILES,\n",
    "    \n",
    "    ### hierarchical forecast\n",
    "    # group_columns=XXXX,\n",
    "    # group_total_weight=XXXX,\n",
    "    # temporal_total_weight=XXXX,\n",
    "    # group_temporal_total_weight=XXXX,\n",
    ")\n",
    "\n",
    "# job_id = \"tide-forecasting-probabilistic-inference-{}\".format(uuid.uuid4())\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=JOB_ID,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=JOB_ID,\n",
    "    pipeline_root=ROOT_DIR,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# job.run(sync=False,experiment=EXPERIMENT_NAME)\n",
    "job.submit(\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    # sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d61abe6-89ed-4ec5-af1b-2e2dc259de40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/prob-infer-forecast-refresh-v1-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa5ff12-cec7-48ef-a76e-d040d2188093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature-transform-engine\n",
      "calculate-training-parameters-2\n",
      "get-predictions-column-2\n",
      "exit-handler-1\n",
      "model-upload-2\n",
      "model-evaluation-import-2\n",
      "string-not-empty\n",
      "get-or-create-model-description-2\n",
      "model-batch-predict-2\n",
      "tide-tide-twrkflow-eval-v1\n",
      "feature-attribution-2\n",
      "condition-2\n",
      "automl-forecasting-ensemble-2\n",
      "condition-4\n",
      "automl-forecasting-stage-1-tuner\n",
      "finalize-eval-quantile-parameters-2\n",
      "automl-tabular-finalizer\n",
      "training-configurator-and-validator\n",
      "get-prediction-image-uri-2\n",
      "model-evaluation-forecasting-2\n",
      "split-materialized-data\n",
      "condition-5\n",
      "model-batch-explanation-2\n",
      "set-optional-inputs\n",
      "table-to-uri-2\n"
     ]
    }
   ],
   "source": [
    "pipeline_task_details = job.task_details\n",
    "\n",
    "for task_deets in pipeline_task_details:\n",
    "    print(task_deets.task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0816dcdf-89e3-450c-9fa8-3e2b49539126",
   "metadata": {},
   "source": [
    "### Get trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f808277-2f71-4cd2-8d81-3a25a33c3a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage_1_tuning_result_artifact_uri: gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/tide-twrkflow-eval-v1/run-2907_22_44_752082/934903580331/tide-tide-twrkflow-eval-v1/automl-forecasting-stage-1-tuner_206416678001573888/tuning_result_output\n",
      "forecasting_mp_model: <google.cloud.aiplatform.models.Model object at 0x7f639b10d7e0> \n",
      "resource name: projects/934903580331/locations/us-central1/models/7688328460153913344\n"
     ]
    }
   ],
   "source": [
    "stage_1_tuner_task = helpers.get_task_detail(\n",
    "    pipeline_task_details, \"automl-forecasting-stage-1-tuner\"\n",
    ")\n",
    "stage_1_tuning_result_artifact_uri = (\n",
    "    stage_1_tuner_task.outputs[\"tuning_result_output\"].artifacts[0].uri\n",
    ")\n",
    "print(f\"stage_1_tuning_result_artifact_uri: {stage_1_tuning_result_artifact_uri}\")\n",
    "\n",
    "# get uploaded model\n",
    "upload_model_task = helpers.get_task_detail(\n",
    "    pipeline_task_details, \"model-upload-2\"\n",
    ")\n",
    "\n",
    "forecasting_mp_model_artifact = (\n",
    "    upload_model_task.outputs[\"model\"].artifacts[0]\n",
    ")\n",
    "\n",
    "forecasting_mp_model = aiplatform.Model(forecasting_mp_model_artifact.metadata['resourceName'])\n",
    "print(f\"forecasting_mp_model: {forecasting_mp_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d93595-a1cb-4d14-9c8e-749de2eeed07",
   "metadata": {},
   "source": [
    "### Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81e85a50-7c05-44f2-a6e4-650d9c74edf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createTime': '2023-12-29T08:52:40.959126Z',\n",
      " 'displayName': 'Vertex Forecasting pipeline',\n",
      " 'metadata': {'evaluation_dataset_path': ['bq://hybrid-vertex.vertex_feature_transform_engine_staging_us.vertex_ai_fte_split_output_test_staging_id83e65c1dc39241eca34c763520380490'],\n",
      "              'evaluation_dataset_type': 'bigquery',\n",
      "              'pipeline_job_id': '8418345082247184384',\n",
      "              'pipeline_job_resource_name': 'projects/934903580331/locations/us-central1/pipelineJobs/tide-tide-twrkflow-eval-v1'},\n",
      " 'metrics': {'meanAbsoluteError': 4118.861,\n",
      "             'meanAbsolutePercentageError': 390.12265,\n",
      "             'rSquared': 0.55656505,\n",
      "             'rootMeanSquaredError': 9204.42,\n",
      "             'rootMeanSquaredLogError': 0.9453542,\n",
      "             'rootMeanSquaredPercentageError': 5200.477,\n",
      "             'weightedAbsolutePercentageError': 48.703373},\n",
      " 'metricsSchemaUri': 'gs://google-cloud-aiplatform/schema/modelevaluation/forecasting_metrics_1.0.0.yaml',\n",
      " 'modelExplanation': {'meanAttributions': [{'featureAttributions': {'city': 104.93280171834074,\n",
      "                                                                    'county': 60.114672924658564,\n",
      "                                                                    'date': 604.0033073745092,\n",
      "                                                                    'sale_dollars': 4886.854262410241,\n",
      "                                                                    'zip_code': 10.897589732644093}}]},\n",
      " 'name': 'projects/934903580331/locations/us-central1/models/7688328460153913344@1/evaluations/4012530462180865187'}\n"
     ]
    }
   ],
   "source": [
    "if RUN_EVALUATION:\n",
    "    forecast_EVALS = forecasting_mp_model.list_model_evaluations()\n",
    "\n",
    "    for model_evaluation in forecast_EVALS:\n",
    "        pprint(model_evaluation.to_dict())\n",
    "        \n",
    "else:\n",
    "    print(f\"Model evaluations were set to: {RUN_EVALUATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbea4232-0aa8-4056-8671-0911e9bec5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's evaluation metrics from training:\n",
      "\n",
      "metric: rSquared, value: 0.55656505\n",
      "\n",
      "metric: rootMeanSquaredError, value: 9204.42\n",
      "\n",
      "metric: meanAbsoluteError, value: 4118.861\n",
      "\n",
      "metric: weightedAbsolutePercentageError, value: 48.703373\n",
      "\n",
      "metric: rootMeanSquaredPercentageError, value: 5200.477\n",
      "\n",
      "metric: meanAbsolutePercentageError, value: 390.12265\n",
      "\n",
      "metric: rootMeanSquaredLogError, value: 0.9453542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if RUN_EVALUATION:\n",
    "    # Get evaluations\n",
    "    model_evaluations = forecasting_mp_model.list_model_evaluations()\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    for evaluation in model_evaluations:\n",
    "        evaluation = evaluation.to_dict()\n",
    "        print(\"Model's evaluation metrics from training:\\n\")\n",
    "        metrics = evaluation[\"metrics\"]\n",
    "        for metric in metrics.keys():\n",
    "            print(f\"metric: {metric}, value: {metrics[metric]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e9890-c0bd-42fd-b1d0-a7134fed090e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (Optional) Log pipeline to Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06d0d110-852f-4c2c-8c21-05697f6a3e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def log_pipeline_job_sample(\n",
    "#     experiment_name: str,\n",
    "#     run_name: str,\n",
    "#     pipeline_job: aiplatform.PipelineJob,\n",
    "#     project: str,\n",
    "#     location: str,\n",
    "# ):\n",
    "#     aiplatform.init(experiment=experiment_name, project=project, location=location)\n",
    "\n",
    "#     aiplatform.start_run(run=run_name, resume=True)\n",
    "\n",
    "#     aiplatform.log(pipeline_job=pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1b3b3-bfb8-472a-96d9-f071640280f5",
   "metadata": {},
   "source": [
    "## (2) TiDE - skip architecture search\n",
    "\n",
    "Instead of doing architecture search everytime, we can reuse the existing architecture search result. This could help:\n",
    "1. reducing the variation of the output model\n",
    "2. reducing training cost\n",
    "\n",
    "The existing architecture search result is stored in the `tuning_result_output` output of the `automl-forecasting-stage-1-tuner` component. You can manually input it or get it programmatically.\n",
    "\n",
    "**New Parameter**\n",
    "* `stage_1_tuning_result_artifact_uri` (str): - (Optional) URI of the hyperparameter tuning result from a previous pipeline run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03138e49-5579-44ec-947f-23e8bd0e3df2",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "1. First test passing just experiement name in pipeline job:\n",
    "\n",
    "```\n",
    "job.submit(\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")\n",
    "```\n",
    "\n",
    "2. Check experiments & model eval compare\n",
    "3. Specify `EXPERIMENT_RUN_NAME` is output in (2) not right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a39f117e-ae1f-4a0d-b9cf-bef34759b70c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_ID: tide-skip-arch-tide-twrkflow-eval-v1-v1\n",
      "<google.cloud.aiplatform.models.Model object at 0x7f639b10d7e0> \n",
      "resource name: projects/934903580331/locations/us-central1/models/7688328460153913344\n",
      "ROOT_DIR: gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/tide-twrkflow-eval-v1/run-2909_01_42_271952\n"
     ]
    }
   ],
   "source": [
    "JOB_ID   = f\"tide-skip-arch-{EXPERIMENT_NAME}-v1\"\n",
    "\n",
    "NOW      = datetime.datetime.now().strftime(\"%d %H:%M:%S.%f\").replace(\" \",\"\").replace(\":\",\"_\").replace(\".\",\"_\")\n",
    "ROOT_DIR = f\"{BUCKET_URI}/automl_forecasting_pipeline/{EXPERIMENT_NAME}/run-{NOW}\"\n",
    "\n",
    "print(f\"JOB_ID: {JOB_ID}\")\n",
    "print(f\"ROOT_DIR: {ROOT_DIR}\")\n",
    "\n",
    "print(forecasting_mp_model)\n",
    "print(stage_1_tuning_result_artifact_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa79df78-efbd-4e4c-b412-35e246ee6b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/tide-skip-arch-tide-twrkflow-eval-v1-v1\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/tide-skip-arch-tide-twrkflow-eval-v1-v1')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tide-skip-arch-tide-twrkflow-eval-v1-v1?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/tide-skip-arch-tide-twrkflow-eval-v1-v1 to Experiment: tide-twrkflow-eval-v1\n"
     ]
    }
   ],
   "source": [
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials = 5\n",
    "\n",
    "train_budget_milli_node_hours = 250.0  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=ROOT_DIR,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    stage_1_tuning_result_artifact_uri=stage_1_tuning_result_artifact_uri,\n",
    "    run_evaluation=RUN_EVALUATION,\n",
    "    evaluated_examples_bigquery_path=f'bq://{PROJECT_ID}.{BIGQUERY_DATASET_NAME}',\n",
    "    enable_probabilistic_inference=PROBABILISTIC_INFER,\n",
    ")\n",
    "\n",
    "# job_id = \"tide-forecasting-skip-architecture-search-{}\".format(uuid.uuid4())\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=JOB_ID,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=JOB_ID,\n",
    "    pipeline_root=ROOT_DIR,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# job.run(sync=False,experiment=EXPERIMENT_NAME)\n",
    "job.submit(\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    # sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebf090b6-6674-4cac-b1cb-7f0f45bfd8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/tide-twrkflow-eval-v1/run-2907_22_44_752082/934903580331/tide-tide-twrkflow-eval-v1/automl-forecasting-stage-1-tuner_206416678001573888/tuning_result_output'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_1_tuning_result_artifact_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fff9ed2-1ea5-433c-a20f-d3c5b0bdd7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl-tabular-finalizer\n",
      "automl-forecasting-ensemble\n",
      "model-evaluation-forecasting\n",
      "tide-skip-arch-tide-twrkflow-eval-v1-v1\n",
      "get-or-create-model-description\n",
      "feature-transform-engine\n",
      "finalize-eval-quantile-parameters\n",
      "set-optional-inputs\n",
      "calculate-training-parameters\n",
      "table-to-uri\n",
      "condition-2\n",
      "importer\n",
      "string-not-empty\n",
      "model-batch-explanation\n",
      "training-configurator-and-validator\n",
      "model-batch-predict\n",
      "model-upload\n",
      "condition-3\n",
      "condition-4\n",
      "feature-attribution\n",
      "exit-handler-1\n",
      "automl-forecasting-stage-2-tuner\n",
      "model-evaluation-import\n",
      "get-prediction-image-uri\n",
      "split-materialized-data\n",
      "get-predictions-column\n"
     ]
    }
   ],
   "source": [
    "skip_arch_search_pipeline_task_details = job.task_details\n",
    "\n",
    "for task_deets in skip_arch_search_pipeline_task_details:\n",
    "    print(task_deets.task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84540b6f-a2fd-4589-b409-10e93f63fc8a",
   "metadata": {},
   "source": [
    "### Get trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5210e0f-a5eb-40f1-8d46-bc018c99dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tuning stage task\n",
    "stage_2_tuner_task = helpers.get_task_detail(\n",
    "    skip_arch_search_pipeline_task_details, \"automl-forecasting-stage-2-tuner\"\n",
    ")\n",
    "stage_2_tuning_result_artifact_uri = stage_2_tuner_task.outputs[\"tuning_result_output\"].artifacts[0].uri\n",
    "print(f\"stage-2 result URI     : \\n{stage_2_tuning_result_artifact_uri}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13470121-cd4a-4fe3-ae56-5b5002d1b5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage-2 result URI     : \n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/tide-twrkflow-eval-v1/run-2909_01_42_271952/934903580331/tide-skip-arch-tide-twrkflow-eval-v1-v1/automl-forecasting-stage-2-tuner_5386119199431065600/tuning_result_output\n",
      "\n",
      "forecasting_mp_model_v2 : \n",
      "<google.cloud.aiplatform.models.Model object at 0x7f639b1c0b20> \n",
      "resource name: projects/934903580331/locations/us-central1/models/3016406796710445056\n"
     ]
    }
   ],
   "source": [
    "# get uploaded model\n",
    "upload_model_task_v2 = helpers.get_task_detail(\n",
    "    skip_arch_search_pipeline_task_details, \"model-upload\"\n",
    ")\n",
    "forecasting_mp_model_v2_artifact = upload_model_task_v2.outputs[\"model\"].artifacts[0]\n",
    "\n",
    "forecasting_mp_model_v2 = aiplatform.Model(forecasting_mp_model_v2_artifact.metadata['resourceName'])\n",
    "print(f\"forecasting_mp_model_v2 : \\n{forecasting_mp_model_v2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "91eb65ae-00d3-4253-a1a6-e64c8b0e43cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task_id: -6674520602667122688\n",
       "task_name: \"tide-skip-arch-tide-twrkflow-eval-v1-v1\"\n",
       "create_time {\n",
       "  seconds: 1703840910\n",
       "  nanos: 506927000\n",
       "}\n",
       "start_time {\n",
       "  seconds: 1703840911\n",
       "  nanos: 239967000\n",
       "}\n",
       "end_time {\n",
       "  seconds: 1703845862\n",
       "  nanos: 413734000\n",
       "}\n",
       "executor_detail {\n",
       "}\n",
       "state: SUCCEEDED\n",
       "execution {\n",
       "  name: \"projects/934903580331/locations/us-central1/metadataStores/default/executions/15616573050056497927\"\n",
       "  display_name: \"tide-skip-arch-tide-twrkflow-eval-v1-v1\"\n",
       "  state: COMPLETE\n",
       "  etag: \"1703845862230\"\n",
       "  create_time {\n",
       "    seconds: 1703840910\n",
       "    nanos: 962000000\n",
       "  }\n",
       "  update_time {\n",
       "    seconds: 1703845862\n",
       "    nanos: 230000000\n",
       "  }\n",
       "  schema_title: \"system.Run\"\n",
       "  schema_version: \"0.0.1\"\n",
       "  metadata {\n",
       "    fields {\n",
       "      key: \"input:available_at_forecast_columns\"\n",
       "      value {\n",
       "        list_value {\n",
       "          values {\n",
       "            string_value: \"date\"\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:context_window\"\n",
       "      value {\n",
       "        number_value: 150.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:data_source_bigquery_table_path\"\n",
       "      value {\n",
       "        string_value: \"bq://bigquery-public-data.iowa_liquor_sales_forecasting.2020_sales_train\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:data_source_csv_filenames\"\n",
       "      value {\n",
       "        string_value: \"\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:dataflow_service_account\"\n",
       "      value {\n",
       "        string_value: \"\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:dataflow_subnetwork\"\n",
       "      value {\n",
       "        string_value: \"\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:dataflow_use_public_ips\"\n",
       "      value {\n",
       "        bool_value: true\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:enable_probabilistic_inference\"\n",
       "      value {\n",
       "        bool_value: false\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:encryption_spec_key_name\"\n",
       "      value {\n",
       "        string_value: \"\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluated_examples_bigquery_path\"\n",
       "      value {\n",
       "        string_value: \"bq://hybrid-vertex.tide_twrkflow_eval_v1\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluation_batch_explain_machine_type\"\n",
       "      value {\n",
       "        string_value: \"n1-highmem-8\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluation_batch_explain_max_replica_count\"\n",
       "      value {\n",
       "        number_value: 22.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluation_batch_explain_starting_replica_count\"\n",
       "      value {\n",
       "        number_value: 22.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluation_batch_predict_machine_type\"\n",
       "      value {\n",
       "        string_value: \"n1-standard-16\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluation_batch_predict_max_replica_count\"\n",
       "      value {\n",
       "        number_value: 25.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluation_batch_predict_starting_replica_count\"\n",
       "      value {\n",
       "        number_value: 25.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluation_dataflow_disk_size_gb\"\n",
       "      value {\n",
       "        number_value: 50.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluation_dataflow_machine_type\"\n",
       "      value {\n",
       "        string_value: \"n1-standard-16\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluation_dataflow_max_num_workers\"\n",
       "      value {\n",
       "        number_value: 25.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:evaluation_dataflow_starting_num_workers\"\n",
       "      value {\n",
       "        number_value: 22.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:fast_testing\"\n",
       "      value {\n",
       "        bool_value: false\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:feature_transform_engine_bigquery_staging_full_dataset_id\"\n",
       "      value {\n",
       "        string_value: \"\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:feature_transform_engine_dataflow_disk_size_gb\"\n",
       "      value {\n",
       "        number_value: 40.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:feature_transform_engine_dataflow_machine_type\"\n",
       "      value {\n",
       "        string_value: \"n1-standard-16\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:feature_transform_engine_dataflow_max_num_workers\"\n",
       "      value {\n",
       "        number_value: 10.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:forecast_horizon\"\n",
       "      value {\n",
       "        number_value: 150.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:group_columns\"\n",
       "      value {\n",
       "        null_value: NULL_VALUE\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:group_temporal_total_weight\"\n",
       "      value {\n",
       "        number_value: 0.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:group_total_weight\"\n",
       "      value {\n",
       "        number_value: 0.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:holiday_regions\"\n",
       "      value {\n",
       "        null_value: NULL_VALUE\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:location\"\n",
       "      value {\n",
       "        string_value: \"us-central1\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:model_description\"\n",
       "      value {\n",
       "        string_value: \"\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:model_display_name\"\n",
       "      value {\n",
       "        string_value: \"automl-forecasting-model-upload-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:num_selected_trials\"\n",
       "      value {\n",
       "        number_value: 5.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:optimization_objective\"\n",
       "      value {\n",
       "        string_value: \"minimize-wape-mae\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:predefined_split_key\"\n",
       "      value {\n",
       "        string_value: \"\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:project\"\n",
       "      value {\n",
       "        string_value: \"hybrid-vertex\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:quantiles\"\n",
       "      value {\n",
       "        null_value: NULL_VALUE\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:root_dir\"\n",
       "      value {\n",
       "        string_value: \"gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/tide-twrkflow-eval-v1/run-2909_01_42_271952\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:run_evaluation\"\n",
       "      value {\n",
       "        bool_value: true\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:stage_1_num_parallel_trials\"\n",
       "      value {\n",
       "        number_value: 35.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:stage_1_tuner_worker_pool_specs_override\"\n",
       "      value {\n",
       "        list_value {\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:stage_1_tuning_result_artifact_uri\"\n",
       "      value {\n",
       "        string_value: \"gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/tide-twrkflow-eval-v1/run-2907_22_44_752082/934903580331/tide-tide-twrkflow-eval-v1/automl-forecasting-stage-1-tuner_206416678001573888/tuning_result_output\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:stage_2_num_parallel_trials\"\n",
       "      value {\n",
       "        number_value: 35.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:stage_2_trainer_worker_pool_specs_override\"\n",
       "      value {\n",
       "        list_value {\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:study_spec_parameters_override\"\n",
       "      value {\n",
       "        list_value {\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:target_column\"\n",
       "      value {\n",
       "        string_value: \"sale_dollars\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:temporal_total_weight\"\n",
       "      value {\n",
       "        number_value: 0.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:test_fraction\"\n",
       "      value {\n",
       "        number_value: 0.1\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:time_column\"\n",
       "      value {\n",
       "        string_value: \"date\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:time_series_attribute_columns\"\n",
       "      value {\n",
       "        list_value {\n",
       "          values {\n",
       "            string_value: \"city\"\n",
       "          }\n",
       "          values {\n",
       "            string_value: \"zip_code\"\n",
       "          }\n",
       "          values {\n",
       "            string_value: \"county\"\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:time_series_identifier_columns\"\n",
       "      value {\n",
       "        list_value {\n",
       "          values {\n",
       "            string_value: \"store_name\"\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:timestamp_split_key\"\n",
       "      value {\n",
       "        string_value: \"\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:train_budget_milli_node_hours\"\n",
       "      value {\n",
       "        number_value: 250.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:training_fraction\"\n",
       "      value {\n",
       "        number_value: 0.8\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:transformations\"\n",
       "      value {\n",
       "        struct_value {\n",
       "          fields {\n",
       "            key: \"auto\"\n",
       "            value {\n",
       "              list_value {\n",
       "                values {\n",
       "                  string_value: \"date\"\n",
       "                }\n",
       "                values {\n",
       "                  string_value: \"sale_dollars\"\n",
       "                }\n",
       "                values {\n",
       "                  string_value: \"city\"\n",
       "                }\n",
       "                values {\n",
       "                  string_value: \"zip_code\"\n",
       "                }\n",
       "                values {\n",
       "                  string_value: \"county\"\n",
       "                }\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "          fields {\n",
       "            key: \"categorical\"\n",
       "            value {\n",
       "              list_value {\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "          fields {\n",
       "            key: \"numeric\"\n",
       "            value {\n",
       "              list_value {\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "          fields {\n",
       "            key: \"text\"\n",
       "            value {\n",
       "              list_value {\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "          fields {\n",
       "            key: \"timestamp\"\n",
       "            value {\n",
       "              list_value {\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:unavailable_at_forecast_columns\"\n",
       "      value {\n",
       "        list_value {\n",
       "          values {\n",
       "            string_value: \"sale_dollars\"\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:validation_fraction\"\n",
       "      value {\n",
       "        number_value: 0.1\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:weight_column\"\n",
       "      value {\n",
       "        string_value: \"\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:window_max_count\"\n",
       "      value {\n",
       "        number_value: 0.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:window_predefined_column\"\n",
       "      value {\n",
       "        string_value: \"\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"input:window_stride_length\"\n",
       "      value {\n",
       "        number_value: 0.0\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"not_presented_inputs\"\n",
       "      value {\n",
       "        list_value {\n",
       "          values {\n",
       "            string_value: \"holiday_regions\"\n",
       "          }\n",
       "          values {\n",
       "            string_value: \"quantiles\"\n",
       "          }\n",
       "          values {\n",
       "            string_value: \"group_columns\"\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"vertex-ai-pipelines-artifact-argument-binding\"\n",
       "      value {\n",
       "        struct_value {\n",
       "          fields {\n",
       "            key: \"output:feature-attribution-feature_attributions\"\n",
       "            value {\n",
       "              list_value {\n",
       "                values {\n",
       "                  string_value: \"projects/934903580331/locations/us-central1/metadataStores/default/artifacts/15670133625162546647\"\n",
       "                }\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "outputs {\n",
       "  key: \"feature-attribution-feature_attributions\"\n",
       "  value {\n",
       "    artifacts {\n",
       "      name: \"projects/934903580331/locations/us-central1/metadataStores/default/artifacts/15670133625162546647\"\n",
       "      display_name: \"feature_attributions\"\n",
       "      uri: \"gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/tide-twrkflow-eval-v1/run-2909_01_42_271952/934903580331/tide-skip-arch-tide-twrkflow-eval-v1-v1/feature-attribution_3865450637751746560/feature_attributions\"\n",
       "      etag: \"1703845862203\"\n",
       "      create_time {\n",
       "        seconds: 1703845508\n",
       "        nanos: 685000000\n",
       "      }\n",
       "      update_time {\n",
       "        seconds: 1703845862\n",
       "        nanos: 203000000\n",
       "      }\n",
       "      state: LIVE\n",
       "      schema_title: \"system.Metrics\"\n",
       "      schema_version: \"0.0.1\"\n",
       "      metadata {\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "pipeline_task_status {\n",
       "  update_time {\n",
       "    seconds: 1703845861\n",
       "    nanos: 981484659\n",
       "  }\n",
       "  state: SUCCEEDED\n",
       "}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get values for stage-2 trials\n",
    "for task_deets in skip_arch_search_pipeline_task_details:\n",
    "    if task_deets.task_name == \"tide-skip-arch-tide-twrkflow-eval-v1-v1\":\n",
    "        # break\n",
    "        stage_2_parallel_trials = task_deets.execution.metadata.get(key=\"input:stage_2_num_parallel_trials\")\n",
    "        stage_2_worker_pool_spec = task_deets.execution.metadata.get(key=\"input:stage_2_trainer_worker_pool_specs_override\")\n",
    "    \n",
    "print(f\"stage_2_parallel_trials  : {stage_2_parallel_trials}\")\n",
    "print(f\"stage_2_worker_pool_spec : {stage_2_worker_pool_spec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3b980-d104-4117-b762-f5a8ba77a88a",
   "metadata": {},
   "source": [
    "### Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c5b49fd-7a9b-4294-965d-1b2c30c2ead6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createTime': '2023-12-29T10:28:31.692046Z',\n",
      " 'displayName': 'Vertex Forecasting pipeline',\n",
      " 'metadata': {'evaluation_dataset_path': ['bq://hybrid-vertex.vertex_feature_transform_engine_staging_us.vertex_ai_fte_split_output_test_staging_id5efc32f2a70e48bbbc24bef6e28b5331'],\n",
      "              'evaluation_dataset_type': 'bigquery',\n",
      "              'pipeline_job_id': '7487788809241755648',\n",
      "              'pipeline_job_resource_name': 'projects/934903580331/locations/us-central1/pipelineJobs/tide-skip-arch-tide-twrkflow-eval-v1-v1'},\n",
      " 'metrics': {'meanAbsoluteError': 4128.132,\n",
      "             'meanAbsolutePercentageError': 405.4942,\n",
      "             'rSquared': 0.5472558,\n",
      "             'rootMeanSquaredError': 9180.089,\n",
      "             'rootMeanSquaredLogError': 0.9531391,\n",
      "             'rootMeanSquaredPercentageError': 5343.9043,\n",
      "             'weightedAbsolutePercentageError': 48.812996},\n",
      " 'metricsSchemaUri': 'gs://google-cloud-aiplatform/schema/modelevaluation/forecasting_metrics_1.0.0.yaml',\n",
      " 'modelExplanation': {'meanAttributions': [{'featureAttributions': {'city': 77.72522828588328,\n",
      "                                                                    'county': 44.59189790698212,\n",
      "                                                                    'date': 422.9841234999592,\n",
      "                                                                    'sale_dollars': 5728.135184951725,\n",
      "                                                                    'zip_code': 13.547329363941682}}]},\n",
      " 'name': 'projects/934903580331/locations/us-central1/models/3016406796710445056@1/evaluations/2429997948310490520'}\n"
     ]
    }
   ],
   "source": [
    "if RUN_EVALUATION:\n",
    "    forecast_EVALS = forecasting_mp_model_v2.list_model_evaluations()\n",
    "\n",
    "    for model_evaluation in forecast_EVALS:\n",
    "        pprint(model_evaluation.to_dict())\n",
    "        \n",
    "else:\n",
    "    print(f\"Model evaluations were set to: {RUN_EVALUATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99ee6208-8f50-4bed-b6ea-7df9996637ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's evaluation metrics from training:\n",
      "\n",
      "metric: rootMeanSquaredPercentageError, value: 5343.9043\n",
      "\n",
      "metric: rootMeanSquaredLogError, value: 0.9531391\n",
      "\n",
      "metric: weightedAbsolutePercentageError, value: 48.812996\n",
      "\n",
      "metric: meanAbsolutePercentageError, value: 405.4942\n",
      "\n",
      "metric: rSquared, value: 0.5472558\n",
      "\n",
      "metric: rootMeanSquaredError, value: 9180.089\n",
      "\n",
      "metric: meanAbsoluteError, value: 4128.132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if RUN_EVALUATION:\n",
    "    # Get evaluations\n",
    "    model_evaluations = forecasting_mp_model_v2.list_model_evaluations()\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    for evaluation in model_evaluations:\n",
    "        evaluation = evaluation.to_dict()\n",
    "        print(\"Model's evaluation metrics from training:\\n\")\n",
    "        metrics = evaluation[\"metrics\"]\n",
    "        for metric in metrics.keys():\n",
    "            print(f\"metric: {metric}, value: {metrics[metric]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df0cf8-0c87-4668-8a60-ad2c24e57294",
   "metadata": {},
   "source": [
    "## (3) TiDE - comparing pipeline runs\n",
    "\n",
    "* see similar [GitHub example](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/comparing_pipeline_runs.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9bb07f8-6501-4bd0-843f-0bc7195dc605",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/tide-tournament-v1/run-2918_35_09_635342\n"
     ]
    }
   ],
   "source": [
    "forecast_horizon = 150\n",
    "context_window   = 150\n",
    "\n",
    "COMPARE_VERSION = \"v1\"\n",
    "EXPERIMENT_NAME = f\"tide-tourny-{COMPARE_VERSION}\"\n",
    "\n",
    "NOW      = datetime.datetime.now().strftime(\"%d %H:%M:%S.%f\").replace(\" \",\"\").replace(\":\",\"_\").replace(\".\",\"_\")\n",
    "ROOT_DIR = f\"{BUCKET_URI}/automl_forecasting_pipeline/{EXPERIMENT_NAME}/run-{NOW}\"\n",
    "\n",
    "print(f\"ROOT_DIR: {ROOT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2f697a57-3eed-4494-a24b-507dda393954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'optimization_objective': 'minimize-mae'}, {'optimization_objective': 'minimize-rmsle'}, {'optimization_objective': 'minimize-rmspe'}, {'optimization_objective': 'minimize-wape-mae'}]\n"
     ]
    }
   ],
   "source": [
    "runs = [\n",
    "    # {\"optimization_objective\": \"minimize-rmse\"},\n",
    "    {\"optimization_objective\": \"minimize-mae\"},\n",
    "    {\"optimization_objective\": \"minimize-rmsle\"},\n",
    "    {\"optimization_objective\": \"minimize-rmspe\"},\n",
    "    {\"optimization_objective\": \"minimize-wape-mae\"},\n",
    "]\n",
    "\n",
    "# runs = [\n",
    "#     {\"forecast_horizon\": 75, \"context_window\": 75},\n",
    "#     {\"forecast_horizon\": 75, \"context_window\": 100},\n",
    "#     {\"forecast_horizon\": 100, \"context_window\": 100},\n",
    "#     {\"forecast_horizon\": 100, \"context_window\": 100},\n",
    "#     {\"forecast_horizon\": 125, \"context_window\": 100},\n",
    "#     {\"forecast_horizon\": 125, \"context_window\": 100},\n",
    "# ]\n",
    "\n",
    "print(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "62ff1cc9-814d-4032-abc5-a1e5f414a0ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-mae-0\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-mae-0')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tide-tournament-v1-minimize-mae-0?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-mae-0 to Experiment: tide-tournament-v1\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-rmsle-1\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-rmsle-1')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tide-tournament-v1-minimize-rmsle-1?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-rmsle-1 to Experiment: tide-tournament-v1\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-rmspe-2\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-rmspe-2')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tide-tournament-v1-minimize-rmspe-2?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-rmspe-2 to Experiment: tide-tournament-v1\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-wape-mae-3\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-wape-mae-3')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tide-tournament-v1-minimize-wape-mae-3?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/tide-tournament-v1-minimize-wape-mae-3 to Experiment: tide-tournament-v1\n"
     ]
    }
   ],
   "source": [
    "for i, run in enumerate(runs):\n",
    "    \n",
    "    (\n",
    "        template_path,\n",
    "        parameter_values,\n",
    "    ) = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        root_dir=ROOT_DIR,\n",
    "        target_column=target_column,\n",
    "        optimization_objective=run['optimization_objective'],\n",
    "        transformations=transformations,\n",
    "        train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "        data_source_csv_filenames=data_source_csv_filenames,\n",
    "        data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "        weight_column=weight_column,\n",
    "        predefined_split_key=predefined_split_key,\n",
    "        training_fraction=training_fraction,\n",
    "        validation_fraction=validation_fraction,\n",
    "        test_fraction=test_fraction,\n",
    "        num_selected_trials=num_selected_trials,\n",
    "        time_column=time_column,\n",
    "        time_series_identifier_columns=[time_series_identifier_column],\n",
    "        time_series_attribute_columns=time_series_attribute_columns,\n",
    "        available_at_forecast_columns=available_at_forecast_columns,\n",
    "        unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        context_window=context_window,\n",
    "        dataflow_subnetwork=dataflow_subnetwork,\n",
    "        dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "        run_evaluation=RUN_EVALUATION,                          # set True to eval on test/valid set\n",
    "        evaluated_examples_bigquery_path=f'bq://{PROJECT_ID}.{BIGQUERY_DATASET_NAME}',\n",
    "        enable_probabilistic_inference=PROBABILISTIC_INFER,\n",
    "\n",
    "        ### quantile forecast\n",
    "        # quantiles=QUANTILES,\n",
    "\n",
    "        ### hierarchical forecast\n",
    "        # group_columns=XXXX,\n",
    "        # group_total_weight=XXXX,\n",
    "        # temporal_total_weight=XXXX,\n",
    "        # group_temporal_total_weight=XXXX,\n",
    "    )\n",
    "    \n",
    "    PIPE_RUN_NAME = f\"{EXPERIMENT_NAME}-{run['optimization_objective']}-{i}\"\n",
    "\n",
    "    job = aiplatform.PipelineJob(\n",
    "        display_name=PIPE_RUN_NAME,\n",
    "        template_path=template_path,\n",
    "        location=REGION,\n",
    "        pipeline_root=ROOT_DIR,\n",
    "        job_id=PIPE_RUN_NAME,\n",
    "        parameter_values=parameter_values,\n",
    "        enable_caching=True,\n",
    "    )\n",
    "    \n",
    "    job.submit(\n",
    "        experiment=EXPERIMENT_NAME,\n",
    "        # sync=False,\n",
    "        service_account=VERTEX_SA,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c166519b-ba95-4cb0-ac6f-59840906d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see state of all pipelineJob\n",
    "vertex_ai.get_experiment_df(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4cd9ae-3cbb-4289-954b-f87401bf6565",
   "metadata": {},
   "source": [
    "## (4) TiDE - challenger vs blessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b85925-793a-4ac5-bab9-53146e1e1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud.aiplatform import gapic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c67341-f40a-4fc9-aa93-2f05f9421b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blessed_eval = gapic.ModelEvaluation(\n",
    "#     display_name=\"eval\",\n",
    "#     metrics_schema_uri=\"gs://google-cloud-aiplatform/schema/modelevaluation/forecasting_metrics_1.0.0.yaml\",\n",
    "#     metrics=metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7720b671-ce58-4fd0-b9c3-c3dc32e638f9",
   "metadata": {},
   "source": [
    "#### IF Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d446b984-68b4-49c9-86e5-742cbe563fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Quantiles        : 3\n",
      "Quantiles             : [0.25, 0.5, 0.9]\n",
      "cleaned_quantile_list : ['25', '50', '90']\n"
     ]
    }
   ],
   "source": [
    "def _get_quantile_strings(quantile_list):\n",
    "    \n",
    "    cleaned_list = []\n",
    "    \n",
    "    for ele in quantile_list:\n",
    "        if str(ele).startswith(\"0.\"):\n",
    "            # cleaned_list.append(str(round(ele, 2)).replace(\"0.\",\"\"))\n",
    "            cleaned_list.append(('{0:.2f}'.format(ele)).replace(\"0.\",\"\"))\n",
    "                \n",
    "        if str(ele).startswith(\".\"):\n",
    "            # cleaned_list.append(str(round(ele, 2)).replace(\".\",\"\"))\n",
    "            cleaned_list.append(('{0:.2f}'.format(ele)).replace(\"0.\",\"\"))\n",
    "    \n",
    "    return cleaned_list\n",
    "\n",
    "cleaned_quantile_list = _get_quantile_strings(QUANTILES)\n",
    "\n",
    "print(f\"# of Quantiles        : {len(QUANTILES)}\")\n",
    "print(f\"Quantiles             : {QUANTILES}\")\n",
    "print(f\"cleaned_quantile_list : {cleaned_quantile_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "49d17b3b-eee5-4995-8c8b-21304e8e81e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      " *EXCEPT(predicted_sale_dollars),\n",
      " predicted_sale_dollars.value AS predicted_sales_mean,\n",
      "  predicted_sale_dollars.quantile_predictions[OFFSET(0)] AS predicted_sale_dollars_p25, predicted_sale_dollars.quantile_predictions[OFFSET(1)] AS predicted_sale_dollars_p50, predicted_sale_dollars.quantile_predictions[OFFSET(2)] AS predicted_sale_dollars_p90,\n",
      "FROM\n",
      " `hybrid-vertex.forecast_refresh_v1.predictions_2023_12_28T16_02_01_117Z_466`\n",
      " LIMIT 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# quantile_string = \"\"\n",
    "\n",
    "# for i in range(0, len(cleaned_quantile_list)):\n",
    "#     quantile_string += f\" predicted_{TARGET_COLUMN}.quantile_predictions[OFFSET({i})] AS predicted_{TARGET_COLUMN}_p{cleaned_quantile_list[i]},\"\n",
    "    \n",
    "# # quantile_string\n",
    "\n",
    "# TARGET_COLUMN = \"sale_dollars\"\n",
    "\n",
    "# query = f\"\"\"\n",
    "# SELECT\n",
    "#  *EXCEPT(predicted_{TARGET_COLUMN}),\n",
    "#  predicted_{TARGET_COLUMN}.value AS predicted_sales_mean,\n",
    "#  {quantile_string}\n",
    "# FROM\n",
    "#  `{cleaned_bq_output_uri}`\n",
    "#  LIMIT 100\n",
    "# \"\"\"\n",
    "# print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9edb0613-cb7e-459c-a68a-827142a73285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      " *EXCEPT(predicted_sale_dollars),\n",
      " predicted_sale_dollars.value AS predicted_sales_mean,\n",
      " predicted_sale_dollars.quantile_predictions[OFFSET(0)] AS predicted_sale_dollars_p25,\n",
      " predicted_sale_dollars.quantile_predictions[OFFSET(1)] AS predicted_sale_dollars_p50,\n",
      " predicted_sale_dollars.quantile_predictions[OFFSET(2)] AS predicted_sale_dollars_p90,\n",
      "FROM\n",
      " `hybrid-vertex.forecast_refresh_v1.predictions_2023_12_28T16_02_01_117Z_466`\n",
      " LIMIT 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TARGET_COLUMN = \"sale_dollars\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    " *EXCEPT(predicted_{TARGET_COLUMN}),\n",
    " predicted_{TARGET_COLUMN}.value AS predicted_sales_mean,\n",
    " predicted_{TARGET_COLUMN}.quantile_predictions[OFFSET(0)] AS predicted_{TARGET_COLUMN}_p25,\n",
    " predicted_{TARGET_COLUMN}.quantile_predictions[OFFSET(1)] AS predicted_{TARGET_COLUMN}_p50,\n",
    " predicted_{TARGET_COLUMN}.quantile_predictions[OFFSET(2)] AS predicted_{TARGET_COLUMN}_p90,\n",
    "FROM\n",
    " `{cleaned_bq_output_uri}`\n",
    " LIMIT 100\n",
    "\"\"\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738007a-9035-470a-b40f-611beedff0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qs_eval['date'] = qs_eval[\"date\"].astype(\"datetime64[ns]\")\n",
    "# qs_eval['predicted_sales_mean'].dtype\n",
    "\n",
    "qs_eval = bq_client.query(query).to_dataframe()\n",
    "\n",
    "qs_eval['date'] = qs_eval[\"date\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2606240-371d-4005-beb8-ba7af9a16a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_on_date</th>\n",
       "      <th>sale_dollars</th>\n",
       "      <th>store_name</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>predicted_sales_mean</th>\n",
       "      <th>predicted_sale_dollars_p25</th>\n",
       "      <th>predicted_sale_dollars_p50</th>\n",
       "      <th>predicted_sale_dollars_p90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Altoona</td>\n",
       "      <td>POLK</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Super Stop Liquor and Wine / Altoona</td>\n",
       "      <td>50009.0</td>\n",
       "      <td>3989.0</td>\n",
       "      <td>2169.0</td>\n",
       "      <td>3994.0</td>\n",
       "      <td>6723.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Altoona</td>\n",
       "      <td>POLK</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Super Stop Liquor and Wine / Altoona</td>\n",
       "      <td>50009.0</td>\n",
       "      <td>3833.0</td>\n",
       "      <td>2431.0</td>\n",
       "      <td>3842.0</td>\n",
       "      <td>7467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Altoona</td>\n",
       "      <td>POLK</td>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Super Stop Liquor and Wine / Altoona</td>\n",
       "      <td>50009.0</td>\n",
       "      <td>4191.5</td>\n",
       "      <td>2522.0</td>\n",
       "      <td>4192.0</td>\n",
       "      <td>7512.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city county       date predicted_on_date  sale_dollars  \\\n",
       "0  Altoona   POLK 2021-04-01        2021-04-01           NaN   \n",
       "1  Altoona   POLK 2021-04-09        2021-04-01           NaN   \n",
       "2  Altoona   POLK 2021-04-16        2021-04-01           NaN   \n",
       "\n",
       "                             store_name zip_code  predicted_sales_mean  \\\n",
       "0  Super Stop Liquor and Wine / Altoona  50009.0                3989.0   \n",
       "1  Super Stop Liquor and Wine / Altoona  50009.0                3833.0   \n",
       "2  Super Stop Liquor and Wine / Altoona  50009.0                4191.5   \n",
       "\n",
       "   predicted_sale_dollars_p25  predicted_sale_dollars_p50  \\\n",
       "0                      2169.0                      3994.0   \n",
       "1                      2431.0                      3842.0   \n",
       "2                      2522.0                      4192.0   \n",
       "\n",
       "   predicted_sale_dollars_p90  \n",
       "0                      6723.0  \n",
       "1                      7467.0  \n",
       "2                      7512.0  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs_eval.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91cbd6d-0839-47bb-bc2d-286de010919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_forecast = aiplatform.Model(\n",
    "#     model_name=BPJ_OUTPUT_DICT['model']\n",
    "# )\n",
    "# my_evaluation_job = trained_forecast.evaluate(\n",
    "#     prediction_type=\"classification\",\n",
    "#     target_field_name=\"type\",\n",
    "#     data_source_uris=[\"gs://sdk-model-eval/my-prediction-data.csv\"],\n",
    "#     staging_bucket=\"gs://my-staging-bucket/eval_pipeline_root\",\n",
    "# )\n",
    "# my_evaluation_job.wait()\n",
    "# my_evaluation = my_evaluation_job.get_model_evaluation()\n",
    "# my_evaluation.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc736d6-e6a0-4228-a063-33242a0ba53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp\n",
    "# from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp\n",
    "\n",
    "# from google_cloud_pipeline_components.v1.model_evaluation import ModelEvaluationForecastingOp\n",
    "\n",
    "# from google_cloud_pipeline_components.preview.model_evaluation import ModelEvaluationFeatureAttributionOp\n",
    "\n",
    "# from google_cloud_pipeline_components._implementation.model_evaluation import (ModelImportEvaluationOp, TargetFieldDataRemoverOp)\n",
    "\n",
    "# preview.model_evaluation.ModelEvaluationFeatureAttributionOp\n",
    "# from google_cloud_pipeline_components.experimental.evaluation import (\n",
    "#     # EvaluationDataSamplerOp, \n",
    "#     # GetVertexModelOp,\n",
    "#     # ModelEvaluationForecastingOp, \n",
    "#     # ModelEvaluationFeatureAttributionOp,\n",
    "#     # ModelImportEvaluationOp, \n",
    "#     # TargetFieldDataRemoverOp\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
