{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18ebbd838e32"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jt-forecast-repo/vertex-forecas-repo\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mThXALJl9Yue"
   },
   "source": [
    "# Forecasting on Vertex pipelines for private preview\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/automl_tabular_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/automl_tabular_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/automl/automl_tabular_on_vertex_pipelines.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcc745968395",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, you will use a few Vertex AI Tabular Workflows pipelines to train AutoML models using different configurations. You will see:\n",
    "- how `get_l2l_forecasting_pipeline_and_parameters` gives you the ability to customize the default AutoML Tabular pipeline\n",
    "- how `get_l2l_forecasting_pipeline_and_parameters` allows you to reduce the training time and cost for an AutoML model by using the tuning results from a previous pipeline run.\n",
    "- how `get_time_series_dense_encoder_forecasting_pipeline_and_parameters` allows you to train FastNN model\n",
    "- how to enable probabilistic inference for forecasting training pipelines\n",
    "- how to perform the batch prediction with the forecasting model trained with Tabular workflow.\n",
    "\n",
    "Learn more about [Tabular Workflow for E2E AutoML](https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/e2e-automl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f887ec5c06c5",
    "tags": []
   },
   "source": [
    "**Objective**\n",
    "\n",
    "In this tutorial, you learn how to create AutoML Forecasting models using [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) downloaded from [Google Cloud Pipeline Components](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction) (GCPC). These pipelines will be Vertex AI Tabular Workflow pipelines which are maintained by Google. These pipelines will showcase different ways to customize the Vertex Tabular training process.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `AutoML Training`\n",
    "- `Vertex AI Pipelines`\n",
    "\n",
    "The steps performed are:\n",
    "\n",
    "- Create a training pipeline with Learn-to-learn algorithm using specified machine type for training.\n",
    "- Create a training pipeline that reuses the architecture search results from the previous pipeline to save time.\n",
    "- Create a training pipeline with TiDE(Time series Dense Encoder) algorithm.\n",
    "- Create a training pipeline with the probabilistic inference enabled.\n",
    "- Perform the batch prediction using the trained model in the above steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eac26958afe8",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "***Dataset***\n",
    "\n",
    "The dataset you will be using is [Liquor](https://www.kaggle.com/datasets/residentmario/iowa-liquor-sales)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "181d4dfbf917",
    "tags": []
   },
   "source": [
    "**Costs**\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* BigQuery\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and [BigQuery](https://cloud.google.com/bigquery), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VPC related config**\n",
    "\n",
    "If you need to use a custom Dataflow subnetwork, you can set it through the `dataflow_subnetwork` parameter. The requirements are:\n",
    "1. `dataflow_subnetwork` must be fully qualified subnetwork name.\n",
    "   [[reference](https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications)]\n",
    "1. The following service accounts must have [Compute Network User role](https://cloud.google.com/compute/docs/access/iam#compute.networkUser) assigned on the specified dataflow subnetwork [[reference](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared)]:\n",
    "    1. Compute Engine default service account: PROJECT_NUMBER-compute@developer.gserviceaccount.com\n",
    "    1. Dataflow service account: service-PROJECT_NUMBER@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
    "\n",
    "If your project has VPC-SC enabled, please make sure:\n",
    "\n",
    "1. The dataflow subnetwork used in VPC-SC is configured properly for Dataflow.\n",
    "   [[reference](https://cloud.google.com/dataflow/docs/guides/routes-firewall)]\n",
    "1. `dataflow_use_public_ips` is set to False.\n",
    "\n",
    "\n",
    "Fully qualified subnetwork name is in the form of:\n",
    ">`https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION_NAME/subnetworks/SUBNETWORK_NAME`\n",
    "\n",
    "Reference: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e85f0288a6df",
    "tags": []
   },
   "source": [
    "**Install additional packages**\n",
    "\n",
    "Install the latest version of the Google Cloud Pipeline Components (GCPC) SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "E7-SzYTR9bo2",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5b402c95-1270-4a31-bfbc-08561489bedf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://automl-tables-build-oss-dependencies/gcpc/2023_03_27_05_17_32/google_cloud_pipeline_components-2.0.0b1.dev0-py2.py3-none-any.whl...\n",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \n",
      "Operation completed over 1 objects/1.2 MiB.                                      \n",
      "\u001b[33m  WARNING: The script wsdump is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tabulate is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-deprecated and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.11.0 which is incompatible.\n",
      "ray 2.4.0 requires grpcio<=1.51.3,>=1.42.0; python_version >= \"3.10\" and sys_platform != \"darwin\", but you have grpcio 1.54.2 which is incompatible.\n",
      "ydata-profiling 4.1.2 requires requests<2.29,>=2.24.0, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !rm -rf ./google_cloud_pipeline_components*.whl\n",
    "# !gsutil cp gs://automl-tables-build-oss-dependencies/gcpc/2023_03_27_05_17_32/google_cloud_pipeline_components-2.0.0b1.dev0-py2.py3-none-any.whl .\n",
    "# !pip install --upgrade --force-reinstall --user ./google_cloud_pipeline_components*.whl -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj5O0S5RTxzY",
    "tags": []
   },
   "source": [
    "#### Restart the kernel\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages.\n",
    "\n",
    "\n",
    "**Note: Once this cell has finished running, continue on. You do not need to re-run any of the cells above.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "> use the prefix defined in 00-env-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX = forecast-refresh-v1\n"
     ]
    }
   ],
   "source": [
    "# naming convention for all cloud resources\n",
    "VERSION        = \"v1\"              # TODO\n",
    "PREFIX         = f'forecast-refresh-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX = {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"forecast-refresh-v1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "BUCKET_NAME              = \"forecast-refresh-v1-hybrid-vertex-gcs\"\n",
      "BUCKET_URI               = \"gs://forecast-refresh-v1-hybrid-vertex-gcs\"\n",
      "\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://forecast-refresh-v1-hybrid-vertex-gcs/data\"\n",
      "\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-gcs'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96ad3d416327",
    "outputId": "0541be5c-ed7f-49c8-818a-02f88225c10c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/config/\n"
     ]
    }
   ],
   "source": [
    "# For a list of available model metrics, go here:\n",
    "!gsutil ls $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbbc3479a1da"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "8G6YmJT1yqkV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# from google_cloud_pipeline_components.types.artifact_types import VertexDataset\n",
    "from google_cloud_pipeline_components.preview.automl.forecasting import \\\n",
    "    utils as automl_forecasting_utils\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# bigquery client\n",
    "bqclient = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    # location=LOCATION\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0423f260423"
   },
   "source": [
    "### Initialize Vertex SDK\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forecast-refresh-v1-v1\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = f\"{PREFIX}-v1\"\n",
    "\n",
    "print(EXPERIMENT_NAME)\n",
    "\n",
    "aiplatform.init(experiment=EXPERIMENT_NAME, project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the tuple of GCS bucket and object URI.\n",
    "def get_bucket_name_and_path(uri):\n",
    "    no_prefix_uri = uri[len(\"gs://\") :]\n",
    "    splits = no_prefix_uri.split(\"/\")\n",
    "    return splits[0], \"/\".join(splits[1:])\n",
    "\n",
    "# Fetch the content from a GCS object URI.\n",
    "\n",
    "def download_from_gcs(uri):\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    return blob.download_as_string()\n",
    "\n",
    "# Upload the string content as a GCS object.\n",
    "\n",
    "def write_to_gcs(uri: str, content: str):\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.upload_from_string(content)\n",
    "\n",
    "\n",
    "def generate_auto_transformation(column_names: List[str]) -> List[Dict[str, Any]]:\n",
    "    transformations = []\n",
    "    for column_name in column_names:\n",
    "        transformations.append({\"auto\": {\"column_name\": column_name}})\n",
    "    return transformations\n",
    "\n",
    "# This is the example to set non-auto transformations.\n",
    "# For more details about the transformations, please check:\n",
    "# https://cloud.google.com/vertex-ai/docs/datasets/data-types-tabular#transformations\n",
    "def generate_transformation(\n",
    "    auto_column_names: Optional[List[str]] = None,\n",
    "    numeric_column_names: Optional[List[str]] = None,\n",
    "    categorical_column_names: Optional[List[str]] = None,\n",
    "    text_column_names: Optional[List[str]] = None,\n",
    "    timestamp_column_names: Optional[List[str]] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    if auto_column_names is None:\n",
    "        auto_column_names = []\n",
    "    if numeric_column_names is None:\n",
    "        numeric_column_names = []\n",
    "    if categorical_column_names is None:\n",
    "        categorical_column_names = []\n",
    "    if text_column_names is None:\n",
    "        text_column_names = []\n",
    "    if timestamp_column_names is None:\n",
    "        timestamp_column_names = []\n",
    "    return {\n",
    "        \"auto\": auto_column_names,\n",
    "        \"numeric\": numeric_column_names,\n",
    "        \"categorical\": categorical_column_names,\n",
    "        \"text\": text_column_names,\n",
    "        \"timestamp\": timestamp_column_names,\n",
    "    }\n",
    "\n",
    "\n",
    "def write_auto_transformations(uri: str, column_names: List[str]):\n",
    "    transformations = generate_auto_transformation(column_names)\n",
    "    write_to_gcs(uri, json.dumps(transformations))\n",
    "\n",
    "# Retrieve the data given a task name.\n",
    "def get_task_detail(\n",
    "    task_details: List[Dict[str, Any]], task_name: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    for task_detail in task_details:\n",
    "        if task_detail.task_name == task_name:\n",
    "            return task_detail\n",
    "\n",
    "# Retrieve the URI of the model.\n",
    "def get_deployed_model_uri(\n",
    "    task_details,\n",
    "):\n",
    "    ensemble_task = get_task_detail(task_details, \"model-upload\")\n",
    "    return ensemble_task.outputs[\"model\"].artifacts[0].uri\n",
    "\n",
    "\n",
    "def get_no_custom_ops_model_uri(task_details):\n",
    "    ensemble_task = get_task_detail(task_details, \"automl-tabular-ensemble\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"model_without_custom_ops\"].artifacts[0].uri\n",
    "    )\n",
    "\n",
    "# Retrieve the feature importance details from GCS.\n",
    "def get_feature_attributions(\n",
    "    task_details,\n",
    "):\n",
    "    ensemble_task = get_task_detail(task_details, \"model-evaluation-2\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"evaluation_metrics\"]\n",
    "        .artifacts[0]\n",
    "        .metadata[\"explanation_gcs_path\"]\n",
    "    )\n",
    "\n",
    "# Retrieve the evaluation metrics from GCS.\n",
    "def get_evaluation_metrics(\n",
    "    task_details,\n",
    "):\n",
    "    ensemble_task = get_task_detail(task_details, \"model-evaluation\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"evaluation_metrics\"].artifacts[0].uri\n",
    "    )\n",
    "\n",
    "# Pretty print the JSON string.\n",
    "def load_and_print_json(s):\n",
    "    parsed = json.loads(s)\n",
    "    print(json.dumps(parsed, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqSmR2Q3Rphx",
    "tags": []
   },
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jjYKwvgxRbTn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used.\n",
    "dataflow_subnetwork = None \n",
    "\n",
    "# Specifies whether Dataflow workers use public IP addresses.\n",
    "dataflow_use_public_ips = True\n",
    "\n",
    "import datetime\n",
    "\n",
    "# NOW = datetime.datetime.now().strftime(\"%d %H:%M:%S.%f\").replace(\" \",\"\").replace(\":\",\"_\").replace(\".\",\"_\")\n",
    "NOW = '2106_14_34_399161' # tmp\n",
    "\n",
    "print(NOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvNFMRmBegZq"
   },
   "source": [
    "### Define training specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eV4JrwB8wAkg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR              = gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/forecast-refresh-v1-v1/run-2106_14_34_399161\n"
     ]
    }
   ],
   "source": [
    "# root_dir = os.path.join(BUCKET_URI, f\"automl_forecasting_pipeline/run-{uuid.uuid4()}\")\n",
    "\n",
    "ROOT_DIR                      = f\"{BUCKET_URI}/automl_forecasting_pipeline/{EXPERIMENT_NAME}/run-{NOW}\"\n",
    "optimization_objective        = \"minimize-mae\"\n",
    "time_column                   = \"date\"\n",
    "time_series_identifier_column = \"store_name\"\n",
    "target_column                 = \"sale_dollars\"\n",
    "data_source_csv_filenames     = None\n",
    "\n",
    "print(f\"ROOT_DIR              = {ROOT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available_at_forecast_columns    = ['date']\n",
      "unavailable_at_forecast_columns  = ['sale_dollars']\n",
      "time_series_attribute_columns    = ['city', 'zip_code', 'county']\n"
     ]
    }
   ],
   "source": [
    "# training data specs\n",
    "\n",
    "data_source_bigquery_table_path = (\n",
    "    \"bq://bigquery-public-data.iowa_liquor_sales_forecasting.2020_sales_train\"\n",
    ")\n",
    "\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "predefined_split_key = None\n",
    "if predefined_split_key:\n",
    "    training_fraction = None\n",
    "    validation_fraction = None\n",
    "    test_fraction = None\n",
    "\n",
    "weight_column = None\n",
    "\n",
    "features = [\n",
    "    time_column,\n",
    "    target_column,\n",
    "    \"city\",\n",
    "    \"zip_code\",\n",
    "    \"county\",\n",
    "]\n",
    "\n",
    "# available_at_forecast_columns = \",\".join([time_column])\n",
    "# unavailable_at_forecast_columns = \",\".join([target_column])\n",
    "# time_series_attribute_columns = \",\".join([\"city\", \"zip_code\", \"county\"])\n",
    "\n",
    "# forecast_horizon = 30\n",
    "# context_window = 30\n",
    "\n",
    "available_at_forecast_columns = [time_column]\n",
    "unavailable_at_forecast_columns = [target_column]\n",
    "time_series_attribute_columns = [\"city\", \"zip_code\", \"county\"]\n",
    "\n",
    "forecast_horizon = 150\n",
    "context_window = 150\n",
    "\n",
    "print(f\"available_at_forecast_columns    = {available_at_forecast_columns}\")\n",
    "print(f\"unavailable_at_forecast_columns  = {unavailable_at_forecast_columns}\")\n",
    "print(f\"time_series_attribute_columns    = {time_series_attribute_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformations       = {'auto': ['date', 'sale_dollars', 'city', 'zip_code', 'county'], 'numeric': [], 'categorical': [], 'text': [], 'timestamp': []}\n",
      "\n",
      "TRANSFORM_CONFIG_PATH = gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/run-28ec73a7-646e-420b-b883-2aa16ea2e518/transform_config_40ac07bd-c92b-4914-beda-18a382062acd.json\n"
     ]
    }
   ],
   "source": [
    "# transformations = generate_auto_transformation(features)\n",
    "transformations = generate_transformation(auto_column_names=features)\n",
    "\n",
    "# TRANSFORM_CONFIG_PATH = f\"{ROOT_DIR}/transform_config_{NOW}.json\"\n",
    "TRANSFORM_CONFIG_PATH = \"gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/run-28ec73a7-646e-420b-b883-2aa16ea2e518/transform_config_40ac07bd-c92b-4914-beda-18a382062acd.json\"\n",
    "\n",
    "print(f\"transformations       = {transformations}\\n\")\n",
    "print(f\"TRANSFORM_CONFIG_PATH = {TRANSFORM_CONFIG_PATH}\")\n",
    "\n",
    "write_to_gcs(TRANSFORM_CONFIG_PATH, json.dumps(transformations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/config/\n"
     ]
    }
   ],
   "source": [
    "# For a list of available model metrics, go here:\n",
    "!gsutil ls $BUCKET_URI/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vertex Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TimeSeriesDataset\n",
      "Create TimeSeriesDataset backing LRO: projects/934903580331/locations/us-central1/datasets/2490918303959089152/operations/1549626330700578816\n",
      "TimeSeriesDataset created. Resource name: projects/934903580331/locations/us-central1/datasets/2490918303959089152\n",
      "To use this TimeSeriesDataset in another session:\n",
      "ds = aiplatform.TimeSeriesDataset('projects/934903580331/locations/us-central1/datasets/2490918303959089152')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'f8e2ba9b-7bc9-4961-9d57-037f83594781'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Vertex managed dataset artifact.\n",
    "vertex_dataset = aiplatform.TimeSeriesDataset.create(\n",
    "    bq_source=data_source_bigquery_table_path\n",
    ")\n",
    "vertex_dataset_artifact_id = vertex_dataset.gca_resource.metadata_artifact.split(\"/\")[-1]\n",
    "vertex_dataset_artifact_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-iXXE14voyR",
    "tags": []
   },
   "source": [
    "# AutoML Forecast with Tabular Workflows\n",
    "\n",
    "> training & customize search space and change training configuration\n",
    "\n",
    "We will create a skip evaluation AutoML Forecasting pipeline with the following customizations:\n",
    "- Limit the hyperparameter search space\n",
    "- Change machine type and tuning / training parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported forecast algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2L - learn-to-learn algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Currently, 4 model types are supported in the APIs/SDK with the utility functions:**\n",
    "\n",
    "* `time_series_dense_encoder` (TiDE): get_time_series_dense_encoder_forecasting_pipeline_and_parameters\n",
    "* `learn_to_learn` (L2L): get_learn_to_learn_forecasting_pipeline_and_parameters\n",
    "* `sequence_to_sequence` (seq2seq): get_sequence_to_sequence_forecasting_pipeline_and_parameters\n",
    "* `temporal_fusion_transformer` (TFT): get_temporal_fusion_transformer_forecasting_pipeline_and_parameters\n",
    "\n",
    "**You can use multiple features at the same time:**\n",
    "1. You can use `TiDE` or `L2L` together with the probabilistic inference enabled.\n",
    "2. With the probabilistic inference enabled, you don't have to set the optimization objective to be `minimize-quantile-loss` in order to train the model for quantile forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sG46cXVueb66",
    "outputId": "179c6b0d-18e7-4f0f-fa62-c169fae8c1e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/l2l-forecasting-forecast-refresh-v1-v1\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/l2l-forecasting-forecast-refresh-v1-v1')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/l2l-forecasting-forecast-refresh-v1-v1?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/l2l-forecasting-forecast-refresh-v1-v1 to Experiment: forecast-refresh-v1-v1\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/l2l-forecasting-d4191e93-9e7c-4ccc-aedd-2729ac2543a7 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"n1-standard-8\"}},  # override for TF chief node\n",
    "    {},  # override for TF worker node, since it's not used, leave it empty\n",
    "    {},  # override for TF ps node, since it's not used, leave it empty\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\"  # override for TF evaluator node\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials = 5\n",
    "\n",
    "train_budget_milli_node_hours = 250  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_learn_to_learn_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=ROOT_DIR,\n",
    "    evaluated_examples_bigquery_path=f\"bq://{PROJECT_ID}.{BIGQUERY_DATASET_NAME}\", # `bq://project.dataset`.\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_column=time_series_identifier_column,\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    stage_1_tuner_worker_pool_specs_override=worker_pool_specs_override,\n",
    "    # feature_transform_engine_dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    # feature_transform_engine_dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    # quantile forecast, L2L without probabilistic inference requires `minimize-quantile-loss`\n",
    "    # quantiles=\",\".join(map(lambda x: str(x), [0.25, 0.5, 0.9])),\n",
    ")\n",
    "\n",
    "job_id = \"l2l-forecasting-{}\".format(EXPERIMENT_NAME)\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=ROOT_DIR,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    "    # Uncomment the following line if you want to use Vertex managed dataset.\n",
    "    # input_artifacts={'vertex_dataset': vertex_dataset_artifact_id},\n",
    ")\n",
    "\n",
    "# job.run(sync=False,experiment=EXPERIMENT_NAME)\n",
    "job.submit(\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    # sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sF8a2RKtRhg",
    "tags": []
   },
   "source": [
    "## L2L - Skip architecture search\n",
    "Instead of doing architecture search everytime, we can reuse the existing architecture search result. This could help:\n",
    "1. reducing the variation of the output model\n",
    "2. reducing training cost\n",
    "\n",
    "The existing architecture search result is stored in the `tuning_result_output` output of the `automl-forecasting-stage-1-tuner` component. We can manually input it or get it programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATE_NEW_ASSETS = True\n",
    "\n",
    "# BIGQUERY_DATASET_NAME = f\"{PREFIX}\".replace(\"-\",\"_\").lower()\n",
    "\n",
    "# if CREATE_NEW_ASSETS:\n",
    "#     ds = bigquery.Dataset(f\"{PROJECT_ID}.{BIGQUERY_DATASET_NAME}\")\n",
    "#     ds.location = BQ_LOCATION\n",
    "#     ds = bqclient.create_dataset(dataset = ds, exists_ok = False)\n",
    "\n",
    "#     print(ds.full_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl-tabular-finalizer\n",
      "feature-transform-engine\n",
      "model-upload-2\n",
      "automl-forecasting-ensemble-2\n",
      "string-not-empty\n",
      "calculate-training-parameters-2\n",
      "condition-2\n",
      "l2l-forecasting-forecast-refresh-v1-v1\n",
      "condition-3\n",
      "get-prediction-image-uri-2\n",
      "training-configurator-and-validator\n",
      "exit-handler-1\n",
      "split-materialized-data\n",
      "automl-forecasting-stage-1-tuner\n",
      "calculate-transformations\n"
     ]
    }
   ],
   "source": [
    "pipeline_task_details = job.task_details\n",
    "\n",
    "for task_deets in pipeline_task_details:\n",
    "    print(task_deets.task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hlPps2Rtpq-",
    "outputId": "b14d62df-b2e0-41bc-a124-6eff3482764c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Model object at 0x7f1cfeac1090> \n",
       "resource name: projects/934903580331/locations/us-central1/models/3409416232942698496"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_1_tuner_task = get_task_detail(\n",
    "    pipeline_task_details, \"automl-forecasting-stage-1-tuner\"\n",
    ")\n",
    "\n",
    "stage_1_tuning_result_artifact_uri = (\n",
    "    stage_1_tuner_task.outputs[\"tuning_result_output\"].artifacts[0].uri\n",
    ")\n",
    "\n",
    "upload_model_task = get_task_detail(\n",
    "    pipeline_task_details, \"model-upload-2\"\n",
    ")\n",
    "\n",
    "forecasting_mp_model_artifact = (\n",
    "    upload_model_task.outputs[\"model\"].artifacts[0]\n",
    ")\n",
    "\n",
    "forecasting_mp_model = aiplatform.Model(forecasting_mp_model_artifact.metadata['resourceName'])\n",
    "forecasting_mp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_ID                 = l2l-skip-a-search-forecast-refresh-v1-v1-v7\n",
      "TRANSFORM_CONFIG_PATH  = gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/run-28ec73a7-646e-420b-b883-2aa16ea2e518/transform_config_40ac07bd-c92b-4914-beda-18a382062acd.json\n"
     ]
    }
   ],
   "source": [
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials = 5\n",
    "\n",
    "train_budget_milli_node_hours = 250  # 15 minutes\n",
    "\n",
    "JOB_ID = f\"l2l-skip-a-search-{EXPERIMENT_NAME}-v7\"\n",
    "\n",
    "print(f\"JOB_ID                 = {JOB_ID}\")\n",
    "print(f\"TRANSFORM_CONFIG_PATH  = {TRANSFORM_CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stage_1_tuning_result_artifact_uri = 'gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/run-28ec73a7-646e-420b-b883-2aa16ea2e518/934903580331/l2l-forecasting-forecast-refresh-v1-v1/automl-forecasting-stage-1-tuner_7439017359651635200/tuning_result_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4x4GA5sMuewX",
    "outputId": "68a26ec6-c299-48f6-ca7b-dc8c3af8d0a0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/l2l-skip-a-search-forecast-refresh-v1-v1-v7\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/l2l-skip-a-search-forecast-refresh-v1-v1-v7')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/l2l-skip-a-search-forecast-refresh-v1-v1-v7?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/l2l-skip-a-search-forecast-refresh-v1-v1-v7 to Experiment: forecast-refresh-v1-v1\n"
     ]
    }
   ],
   "source": [
    "# run pipeline run\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_learn_to_learn_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=ROOT_DIR,\n",
    "    evaluated_examples_bigquery_path=f\"bq://{PROJECT_ID}.{BIGQUERY_DATASET_NAME}\", # `bq://project.dataset`.\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    # time_series_identifier_column=time_series_identifier_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    stage_1_tuning_result_artifact_uri=stage_1_tuning_result_artifact_uri,\n",
    "    # feature_transform_engine_dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    # feature_transform_engine_dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    ")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=JOB_ID,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=JOB_ID,\n",
    "    pipeline_root=ROOT_DIR,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "# job.run(sync=False,experiment=EXPERIMENT_NAME)\n",
    "job.submit(\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    # sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl-forecasting-stage-2-tuner\n",
      "exit-handler-1\n",
      "finalize-eval-quantile-parameters\n",
      "l2l-skip-a-search-forecast-refresh-v1-v1-v7\n",
      "split-materialized-data\n",
      "get-prediction-image-uri\n",
      "model-evaluation-forecasting\n",
      "condition-4\n",
      "condition-3\n",
      "model-batch-explanation\n",
      "training-configurator-and-validator\n",
      "model-upload\n",
      "table-to-uri\n",
      "automl-forecasting-ensemble\n",
      "automl-tabular-finalizer\n",
      "feature-attribution\n",
      "feature-transform-engine\n",
      "get-or-create-model-description\n",
      "model-batch-predict\n",
      "condition-2\n",
      "calculate-training-parameters\n",
      "set-optional-inputs\n",
      "get-predictions-column\n",
      "string-not-empty\n",
      "importer\n",
      "model-evaluation-import\n"
     ]
    }
   ],
   "source": [
    "pipeline_task_details = job.task_details\n",
    "\n",
    "for task_deets in pipeline_task_details:\n",
    "    print(task_deets.task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model URI\n",
    "skip_architecture_search_pipeline_task_details = (\n",
    "    job.gca_resource.job_detail.task_details\n",
    ")\n",
    "skip_architecture_search_pipeline_task_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiftLomOwGda"
   },
   "source": [
    "## TiDE - Time series dense encoder\n",
    "\n",
    "> (aka `FastNN`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_ID = tide-full-forecast-refresh-v1-v1-v2\n"
     ]
    }
   ],
   "source": [
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials = 5\n",
    "\n",
    "train_budget_milli_node_hours = 250  # 15 minutes\n",
    "\n",
    "JOB_ID = f\"tide-full-{EXPERIMENT_NAME}-v2\"\n",
    "\n",
    "print(f\"JOB_ID = {JOB_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSAcS70N1GMN",
    "outputId": "f58edc0f-e490-4702-905e-ab7d6b3098a8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/tide-full-forecast-refresh-v1-v1-v2\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/tide-full-forecast-refresh-v1-v1-v2')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tide-full-forecast-refresh-v1-v1-v2?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/tide-full-forecast-refresh-v1-v1-v2 to Experiment: forecast-refresh-v1-v1\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=ROOT_DIR,\n",
    "    evaluated_examples_bigquery_path=f\"bq://{PROJECT_ID}.{BIGQUERY_DATASET_NAME}\", # `bq://project.dataset`.\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    # time_series_identifier_column=time_series_identifier_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    # feature_transform_engine_dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    # feature_transform_engine_dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    # enable_probabilistic_inference=True,\n",
    "    # quantile forecast, TiDE without probabilistic inference requires `minimize-quantile-loss`\n",
    "    # quantiles=\",\".join(map(lambda x: str(x), [0.25, 0.5, 0.9])),\n",
    ")\n",
    "\n",
    "# job_id = \"tide-forecasting-{}\".format(EXPERIMENT_NAME)\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=JOB_ID,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=JOB_ID,\n",
    "    pipeline_root=ROOT_DIR,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# job.run(sync=False,experiment=EXPERIMENT_NAME)\n",
    "job.submit(\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    # sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalize-eval-quantile-parameters-2\n",
      "exit-handler-1\n",
      "string-not-empty\n",
      "model-evaluation-forecasting-2\n",
      "condition-2\n",
      "condition-5\n",
      "model-batch-explanation-2\n",
      "automl-forecasting-stage-1-tuner\n",
      "table-to-uri-2\n",
      "feature-transform-engine\n",
      "tide-full-forecast-refresh-v1-v1-v2\n",
      "get-prediction-image-uri-2\n",
      "feature-attribution-2\n",
      "model-upload-2\n",
      "automl-tabular-finalizer\n",
      "split-materialized-data\n",
      "model-batch-predict-2\n",
      "calculate-training-parameters-2\n",
      "get-predictions-column-2\n",
      "set-optional-inputs\n",
      "training-configurator-and-validator\n",
      "automl-forecasting-ensemble-2\n",
      "model-evaluation-import-2\n",
      "condition-4\n",
      "get-or-create-model-description-2\n"
     ]
    }
   ],
   "source": [
    "# pipeline_task_details = job.gca_resource.job_detail.task_details\n",
    "\n",
    "pipeline_task_details = job.task_details\n",
    "\n",
    "for task_deets in pipeline_task_details:\n",
    "    print(task_deets.task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TiDE - Skip architecture search\n",
    "\n",
    "> After retrieving the thing result from the stage 1 tuner, you can use it to skip the model architecture search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_ID = tide-skip-search-forecast-refresh-v1-v1\n"
     ]
    }
   ],
   "source": [
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials = 5\n",
    "\n",
    "train_budget_milli_node_hours = 250  # 15 minutes\n",
    "\n",
    "JOB_ID = f\"tide-skip-search-{EXPERIMENT_NAME}\"\n",
    "\n",
    "RUN_EVALUATION = False\n",
    "\n",
    "print(f\"JOB_ID = {JOB_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/forecast-refresh-v1-v1/run-2106_14_34_399161/934903580331/tide-full-forecast-refresh-v1-v1-v2/automl-forecasting-stage-1-tuner_-2508097211070414848/tuning_result_output\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the tuning result output from the previous training pipeline.\n",
    "stage_1_tuner_task = helpers.get_task_detail(\n",
    "    pipeline_task_details, \"automl-forecasting-stage-1-tuner\"\n",
    ")\n",
    "\n",
    "stage_1_tuning_result_artifact_uri = (\n",
    "    stage_1_tuner_task.outputs[\"tuning_result_output\"].artifacts[0].uri\n",
    ")\n",
    "\n",
    "print(stage_1_tuning_result_artifact_uri)\n",
    "train_budget_milli_node_hours = 250.0  # 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/tide-skip-search-forecast-refresh-v1-v1\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/tide-skip-search-forecast-refresh-v1-v1')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tide-skip-search-forecast-refresh-v1-v1?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/tide-skip-search-forecast-refresh-v1-v1 to Experiment: forecast-refresh-v1-v1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=ROOT_DIR,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    # time_series_identifier_column=time_series_identifier_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    # feature_transform_engine_dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    # feature_transform_engine_dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    stage_1_tuning_result_artifact_uri=stage_1_tuning_result_artifact_uri,\n",
    "    run_evaluation=RUN_EVALUATION\n",
    "    # enable_probabilistic_inference=True,\n",
    "    # quantile forecast, TiDE without probabilistic inference requires `minimize-quantile-loss`\n",
    "    # quantiles=\",\".join(map(lambda x: str(x), [0.25, 0.5, 0.9])),\n",
    ")\n",
    "\n",
    "# job_id = \"tide-forecasting-{}\".format(EXPERIMENT_NAME)\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=JOB_ID,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=JOB_ID,\n",
    "    pipeline_root=ROOT_DIR,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# job.run(sync=False,experiment=EXPERIMENT_NAME)\n",
    "job.submit(\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    # sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9tQnPiXwQ1S",
    "tags": []
   },
   "source": [
    "## Probabilistic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9AWKFGV0_UW",
    "outputId": "24eab6fa-84f3-4905-8812-74f60434e113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620?project=294348452381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620?project=294348452381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob run completed. Resource name: projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/294348452381/locations/us-central1/pipelineJobs/l2l-forecasting-probabilistic-inference-bb3f26ad-af03-48e2-b320-d2c89ac8b620\n"
     ]
    }
   ],
   "source": [
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials = 5\n",
    "\n",
    "train_budget_milli_node_hours = 500  # 30 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_learn_to_learn_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=root_dir,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transform_config_path,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_column=time_series_identifier_column,\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    feature_transform_engine_dataflow_subnetwork=dataflow_subnetwork,\n",
    "    feature_transform_engine_dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    enable_probabilistic_inference=True,\n",
    "    # quantile forecast\n",
    "    quantiles=\",\".join(map(lambda x: str(x), [0.25, 0.5, 0.9])),\n",
    ")\n",
    "\n",
    "job_id = \"l2l-forecasting-probabilistic-inference-{}\".format(EXPERIMENT_NAME)\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# job.run(sync=False,experiment=EXPERIMENT_NAME)\n",
    "job.submit(\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    # sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_task_details = job.gca_resource.job_detail.task_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment logging\n",
    "\n",
    "[experiments with pipelines](https://cloud.google.com/vertex-ai/docs/experiments/add-pipelinerun-experiment#associate-pipeline-run-with-experiment-run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pipeline_job_to_experiment_sample(\n",
    "    experiment_name: str,\n",
    "    pipeline_job_display_name: str,\n",
    "    template_path: str,\n",
    "    pipeline_root: str,\n",
    "    parameter_values: Optional[Dict[str, Any]],\n",
    "    project: str,\n",
    "    location: str,\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    pipeline_job = aiplatform.PipelineJob(\n",
    "        display_name=pipeline_job_display_name,\n",
    "        template_path=template_path,\n",
    "        pipeline_root=pipeline_root,\n",
    "        parameter_values=parameter_values,\n",
    "    )\n",
    "\n",
    "    pipeline_job.submit(experiment=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pipeline_job_sample(\n",
    "    experiment_name: str,\n",
    "    run_name: str,\n",
    "    pipeline_job: aiplatform.PipelineJob,\n",
    "    project: str,\n",
    "    location: str,\n",
    "):\n",
    "    aiplatform.init(experiment=experiment_name, project=project, location=location)\n",
    "\n",
    "    aiplatform.start_run(run=run_name, resume=True)\n",
    "\n",
    "    aiplatform.log(pipeline_job=pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L38LI3OOwXkV"
   },
   "source": [
    "##Batch prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5RcLdKnQN-R"
   },
   "source": [
    "### For liquor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfssCxqRg1x-",
    "outputId": "b44a60e6-e33f-4dc0-a466-dd406bf9e650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob created. Resource name: projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this BatchPredictionJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpj = aiplatform.BatchPredictionJob('projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/4296530389217837056?project=294348452381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/4296530389217837056?project=294348452381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob run completed. Resource name: projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob run completed. Resource name: projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.jobs.BatchPredictionJob object at 0x7f8878d0cb80> \n",
      "resource name: projects/294348452381/locations/us-central1/batchPredictionJobs/4296530389217837056\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running Batch prediction for model: {forecasting_mp_model.display_name}\")\n",
    "\n",
    "\n",
    "batch_predict_bq_output_uri_prefix = f\"bq://{PROJECT_ID}\"\n",
    "\n",
    "\n",
    "# Not use this since FTE not support US dataset in us-central1, please copy this\n",
    "# BigQuery table to your own BigQuery dataset that is located in us-central1.\n",
    "# PREDICTION_DATASET_BQ_PATH = (\n",
    "#     \"bq://bigquery-public-data:iowa_liquor_sales_forecasting.2021_sales_predict\"\n",
    "# )\n",
    "\n",
    "PREDICTION_DATASET_BQ_PATH = (\n",
    "    f\"bq://{PROJECT_ID}.iowa_liquor_sales_forecasting_us_central1.2021_sales_predict\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "batch_prediction_job = forecasting_mp_model.batch_predict(\n",
    "    job_display_name=f\"forecasting_iowa_liquor_sales_forecasting_predictions\",\n",
    "    bigquery_source=PREDICTION_DATASET_BQ_PATH,\n",
    "    instances_format=\"bigquery\",\n",
    "    bigquery_destination_prefix=batch_predict_bq_output_uri_prefix,\n",
    "    predictions_format=\"bigquery\",\n",
    "    generate_explanation=False,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "print(batch_prediction_job)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
