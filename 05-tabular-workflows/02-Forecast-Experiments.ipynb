{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eaf3b2b-864f-401f-beaa-bc4a4fc49bf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running Forecast Experiments with Vertex Tabular Workflows\n",
    "\n",
    "**TODO**\n",
    "* Add seperate notebook for evaluating **Algorithms**\n",
    "\n",
    "```\n",
    "* BQ_ARIMA+\n",
    "* Prophet\n",
    "* L2L, TFT, Seq2seq+, TiDE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955c4a2-b41f-4989-b6d7-5b74517c97a9",
   "metadata": {},
   "source": [
    "### Vertex Pipeline console view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade4146-3176-40c8-b218-5bae817064f0",
   "metadata": {},
   "source": [
    "<img src='imgs/tabular_workflow_and_gcpc_overview.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85647b0e-968a-4ec0-b18b-d88c349020a1",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "> use the prefix defined in 00-env-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921fa6ed-4bfe-47d4-9f4c-b53b49f1a8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREATE_NEW_ASSETS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715edf8e-f7e2-4ee7-93e5-8bdf345e3ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX = forecast-refresh-v1\n"
     ]
    }
   ],
   "source": [
    "# naming convention for all cloud resources\n",
    "VERSION        = \"v1\"              # TODO\n",
    "PREFIX         = f'forecast-refresh-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX = {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ff898c-573c-4f92-9140-80968c40e83c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"forecast-refresh-v1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "BUCKET_NAME              = \"forecast-refresh-v1-hybrid-vertex-gcs\"\n",
      "BUCKET_URI               = \"gs://forecast-refresh-v1-hybrid-vertex-gcs\"\n",
      "\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://forecast-refresh-v1-hybrid-vertex-gcs/data\"\n",
      "\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-gcs'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a1ccc2-66d8-42d0-93ec-61cd826a9966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/config/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/tide-cw-eval-v1/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/tide-cw-eval-v2/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/tide-cw-eval-v3/\n"
     ]
    }
   ],
   "source": [
    "# For a list of available model metrics, go here:\n",
    "!gsutil ls $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b312d99-a732-497e-be5a-7db4f78cb0fd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f73970-ce67-4d2a-9dd3-9638a0b8d6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from google.cloud import aiplatform, storage, bigquery\n",
    "\n",
    "# from google_cloud_pipeline_components.types.artifact_types import VertexDataset\n",
    "from google_cloud_pipeline_components.preview.automl.forecasting import \\\n",
    "    utils as automl_forecasting_utils\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a48961-c716-4707-9214-2051b2d8531c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a BigQuery client object.\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "aiplatform.init(\n",
    "    # experiment=EXPERIMENT_NAME, \n",
    "    project=PROJECT_ID, \n",
    "    location=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2001648-8e11-49fc-9de1-5acb6d6533eb",
   "metadata": {},
   "source": [
    "## Training constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0baa0b5d-ef3e-45a2-b956-1bd30622777c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available_at_forecast_columns   = ['date']\n",
      "unavailable_at_forecast_columns = ['sale_dollars']\n",
      "time_series_attribute_columns   = ['city', 'zip_code', 'county']\n",
      "transformations                 = {'auto': ['date', 'sale_dollars', 'city', 'zip_code', 'county'], 'numeric': [], 'categorical': [], 'text': [], 'timestamp': []}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### data sources ###\n",
    "data_source_bigquery_table_path = (\n",
    "    \"bq://bigquery-public-data.iowa_liquor_sales_forecasting.2020_sales_train\"\n",
    ")\n",
    "\n",
    "weight_column = None\n",
    "predefined_split_key = None\n",
    "data_source_csv_filenames = None\n",
    "\n",
    "if predefined_split_key:\n",
    "    training_fraction = None\n",
    "    validation_fraction = None\n",
    "    test_fraction = None\n",
    "    \n",
    "### data splits ###\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "### data transformations ###\n",
    "dataflow_subnetwork           = None # Dataflow's subnetwork name; empty == use default subnetwork\n",
    "dataflow_use_public_ips       = True # Specifies whether Dataflow workers use public IP addresses\n",
    "\n",
    "### features ###\n",
    "time_series_identifier_column = \"store_name\"\n",
    "target_column                 = \"sale_dollars\"\n",
    "time_column                   = \"date\"\n",
    "features = [\n",
    "    time_column,\n",
    "    target_column,\n",
    "    \"city\",\n",
    "    \"zip_code\",\n",
    "    \"county\",\n",
    "]\n",
    "available_at_forecast_columns = [time_column]\n",
    "unavailable_at_forecast_columns = [target_column]\n",
    "time_series_attribute_columns = [\"city\", \"zip_code\", \"county\"]\n",
    "    \n",
    "# feature transforms\n",
    "transformations = helpers.generate_transformation(auto_column_names=features)\n",
    "    \n",
    "print(f\"available_at_forecast_columns   = {available_at_forecast_columns}\")\n",
    "print(f\"unavailable_at_forecast_columns = {unavailable_at_forecast_columns}\")\n",
    "print(f\"time_series_attribute_columns   = {time_series_attribute_columns}\")\n",
    "print(f\"transformations                 = {transformations}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0207b4a7-60bc-410d-aa01-ff316da144e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/automl_forecasting_pipeline/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/config/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/tide-cw-eval-v1/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/tide-cw-eval-v2/\n",
      "gs://forecast-refresh-v1-hybrid-vertex-gcs/tide-cw-eval-v3/\n"
     ]
    }
   ],
   "source": [
    "# For a list of available model metrics, go here:\n",
    "!gsutil ls $BUCKET_URI/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a2070-faca-44fb-aa88-22047ad8a589",
   "metadata": {},
   "source": [
    "# Orchestrate experiment with Vertex Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab9adb-d0d5-4ba8-a834-a2a747491f35",
   "metadata": {},
   "source": [
    "1. For first training iteration, **set context window and the forecast horizon to the same value**, and set your training budget to at least 6 hours\n",
    "\n",
    "2. Train (retrain) the model again, with the **same training budget, but double the size of the context window**, i.e., 2x the size of the forecast horizon\n",
    "\n",
    "3. If evaluation metrics for the second model show substantial improvement, train the model again, **increasing the context window to 5 times the size of the forecast horizon**. \n",
    "> * Consider making a proportional increase to the training budget (if you trained for 10 hours in the first step, increase the training budget to 50 hours).\n",
    "\n",
    "4. Continue increasing the context window until you are no longer seeing improved evaluation metrics or until you are satisfied with the results. Revert back to the lowest value of the context window that produced acceptable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1247685-e4f6-4f48-8b89-8fa9e8a75d3b",
   "metadata": {},
   "source": [
    "## Config Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f61d2a7-44d3-4adb-bf19-bf4a83f3bd20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : tide-cw-eval-v3\n",
      "RUN_NAME          : run-20240109-185241\n",
      "\n",
      "BASE_OUTPUT_DIR   : gs://forecast-refresh-v1-hybrid-vertex-gcs/tide-cw-eval-v3/run-20240109-185241\n",
      "LOG_DIR           : gs://forecast-refresh-v1-hybrid-vertex-gcs/tide-cw-eval-v3/run-20240109-185241/logs\n",
      "ROOT_DIR          : gs://forecast-refresh-v1-hybrid-vertex-gcs/tide-cw-eval-v3/run-20240109-185241/root\n",
      "ARTIFACTS_DIR     : gs://forecast-refresh-v1-hybrid-vertex-gcs/tide-cw-eval-v3/run-20240109-185241/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_VERSION = \"v3\"\n",
    "EXPERIMENT_TAG     = \"tide-cw-eval\"\n",
    "EXPERIMENT_NAME = f\"{EXPERIMENT_TAG}-{EXPERIMENT_VERSION}\"\n",
    "\n",
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "BASE_OUTPUT_DIR   = f'{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5430970-c561-4133-bc2a-0a646d3f5dd7",
   "metadata": {},
   "source": [
    "### Create BigQuery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acfa3d5d-5df0-4694-9e3a-0dd647c1248c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGQUERY_DATASET_NAME   : tide_cw_eval_v3\n"
     ]
    }
   ],
   "source": [
    "BIGQUERY_DATASET_NAME = EXPERIMENT_NAME.replace(\"-\",\"_\")\n",
    "\n",
    "print(f\"BIGQUERY_DATASET_NAME   : {BIGQUERY_DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02a4d4de-919d-4be6-a433-6dadea5bca0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if CREATE_NEW_ASSETS:\n",
    "#     ds = bigquery.Dataset(f\"{PROJECT_ID}.{BIGQUERY_DATASET_NAME}\")\n",
    "#     ds.location = BQ_LOCATION\n",
    "#     ds = bq_client.create_dataset(dataset = ds, exists_ok = True)\n",
    "#     # print(ds.full_dataset_id)\n",
    "# else:\n",
    "#     ds = bigquery.Dataset(f\"{PROJECT_ID}.{BIGQUERY_DATASET_NAME}\")\n",
    "    \n",
    "# ds \n",
    "# ds.dataset_id\n",
    "# ds.full_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55218a-a502-4fb4-8a11-0fa4d88a956a",
   "metadata": {},
   "source": [
    "### Create Vertex Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "071eeb77-22fe-4823-a43d-44ea3d1ba8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if CREATE_NEW_ASSETS:\n",
    "#     # Create a Vertex managed dataset artifact.\n",
    "#     vertex_dataset = aiplatform.TimeSeriesDataset.create(bq_source=data_source_bigquery_table_path)\n",
    "# else:\n",
    "#     vertex_dataset = aiplatform.TimeSeriesDataset('projects/934903580331/locations/us-central1/datasets/1647689642478141440')\n",
    "\n",
    "# vertex_ds_artifact_id = vertex_dataset.gca_resource.metadata_artifact.split(\"/\")[-1]\n",
    "\n",
    "# print(f\"vertex_dataset: {vertex_dataset}\")\n",
    "# print(f\"vertex_ds_artifact_id: {vertex_ds_artifact_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d04fd-a38b-43f2-b1fe-e5edb4adaece",
   "metadata": {},
   "source": [
    "## Create Custom Pipeline Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f722b0-d637-4df0-8171-f7a01d0e0c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "\n",
    "! rm -rf $REPO_DOCKER_PATH_PREFIX\n",
    "! mkdir $REPO_DOCKER_PATH_PREFIX\n",
    "# !mkdir -p ./$REPO_DOCKER_PATH_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7213392d-d333-4cfb-b217-92b42b1ae3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp import components\n",
    "\n",
    "import kfp\n",
    "from typing import NamedTuple, List\n",
    "from kfp.dsl import (\n",
    "    component, \n",
    "    Metrics\n",
    ")\n",
    "from google_cloud_pipeline_components.v1.dataset import TimeSeriesDatasetCreateOp\n",
    "from google_cloud_pipeline_components._implementation.model import GetVertexModelOp\n",
    "from google_cloud_pipeline_components.v1.wait_gcp_resources import \\\n",
    "    WaitGcpResourcesOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be69f83-4761-43f4-9f5c-dd0c63267bb2",
   "metadata": {},
   "source": [
    "### component: args_generate_string\n",
    "\n",
    "> TODO: test this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b3ebe32-2be7-404a-a6d1-d659d20d60dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/args_generate_string.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/args_generate_string.py\n",
    "import kfp\n",
    "from typing import NamedTuple, List, Dict, Any, Union\n",
    "from kfp.dsl import (\n",
    "    component, \n",
    "    Metrics\n",
    ")\n",
    "@component(\n",
    "  base_image='python:3.10',\n",
    ")\n",
    "\n",
    "def args_generate_string(\n",
    "    # cw_values: List[int],\n",
    "    # opt_objective: str,\n",
    "    # experiment_name: str,\n",
    "    experiment_list: List\n",
    ") -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    \n",
    "#     logging.info(f'NUM_EXPERIMENTS: {len(cw_values)}')\n",
    "#     output_list = []\n",
    "    \n",
    "#     for cw in cw_values:\n",
    "#         entry = {\n",
    "#             \"context_window\" : str(cw),\n",
    "#             \"objective\" : opt_objective,\n",
    "#             \"model_display_name\" : f\"{experiment_name}-{str(cw)}\",\n",
    "#         }\n",
    "#         output_list.append(entry)\n",
    "        \n",
    "#     logging.info(f'output_list: {output_list}')\n",
    "    \n",
    "    return json.dumps(\n",
    "        experiment_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed5e1d-ec1f-47b7-b072-794673573bf1",
   "metadata": {},
   "source": [
    "### component: args_generate_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad44d7d2-8477-4200-94e9-a6d40b8ca093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/args_generate_ints.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/args_generate_ints.py\n",
    "import kfp\n",
    "from typing import NamedTuple, List, Dict, Any, Union\n",
    "from kfp.dsl import (\n",
    "    component, \n",
    "    Metrics\n",
    ")\n",
    "@component(\n",
    "  base_image='python:3.10',\n",
    ")\n",
    "\n",
    "def args_generate_ints(\n",
    "    experiment_dict: str,\n",
    ") -> int:\n",
    "\n",
    "    import json\n",
    "    import logging\n",
    "    \n",
    "    entry_dump = json.loads(experiment_dict)\n",
    "    logging.info(f'experiment_dict: {experiment_dict}')\n",
    "    \n",
    "    integer_value_cw = int(entry_dump['context_window'])\n",
    "    \n",
    "    # for model_version, (cw, _, _) in entry_dump.items():\n",
    "    #     print(f\"model_version: {model_version}\")\n",
    "    #     in_cw_value = int(entry_dump[model_version]['context_window'])\n",
    "    \n",
    "    return integer_value_cw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc945d-e1d7-4842-8555-464b526499b5",
   "metadata": {},
   "source": [
    "### component: create_bq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b6c6805-344b-43d9-affe-45e6a5fe4349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/create_bq_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/create_bq_dataset.py\n",
    "import kfp\n",
    "from typing import NamedTuple\n",
    "from kfp.dsl import (\n",
    "    component, \n",
    "    Metrics\n",
    ")\n",
    "@component(\n",
    "  base_image='python:3.10',\n",
    "  packages_to_install=['google-cloud-bigquery==3.14.1'],\n",
    ")\n",
    "def create_bq_dataset(\n",
    "    project: str,\n",
    "    new_bq_dataset: str,\n",
    "    bq_location: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('bq_dataset_name', str),\n",
    "    ('bq_dataset_uri', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    bq_client = bigquery.Client(project=project, location=bq_location) # bq_location)\n",
    "    (\n",
    "      bq_client.query(f'CREATE SCHEMA IF NOT EXISTS `{project}.{new_bq_dataset}`')\n",
    "      .result()\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        f'{new_bq_dataset}',\n",
    "        f'bq://{project}.{new_bq_dataset}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ffd52-dc98-4190-b736-6cc186b8f378",
   "metadata": {},
   "source": [
    "## Define experiment design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d10c20-8b15-4eec-8729-e39d0253edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_EVALUATION                = True\n",
    "PROBABILISTIC_INFER           = False\n",
    "\n",
    "NUM_SELECTED_TRIALS           = 3      # Number of weak models in the final ensemble model\n",
    "TRAIN_BUDGET_MILLI_NODE_HRS   = 250.0  # 15 minutes\n",
    "\n",
    "stage_1_num_parallel_trials   = 35     # Number of parallel trails for stage 1\n",
    "stage_2_num_parallel_trials   = 35     # Number of parallel trails for stage 2\n",
    "\n",
    "forecast_horizon              = 14\n",
    "context_window                = 14\n",
    "\n",
    "optimization_objective        = \"minimize-wape-mae\"\n",
    "\n",
    "CW_VALUES = [\n",
    "    context_window, \n",
    "    int(context_window*2), \n",
    "    int(context_window*4), \n",
    "    int(context_window*6)\n",
    "]\n",
    "\n",
    "print(f\"optimization_objective = {optimization_objective}\")\n",
    "print(f\"forecast_horizon       = {forecast_horizon}\")\n",
    "print(f\"context_window         = {context_window}\")\n",
    "print(f\"CW_VALUES              = {CW_VALUES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73495476-91bd-41d5-991a-a68da366c78b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimization_objective = minimize-wape-mae\n",
      "forecast_horizon       = 14\n",
      "context_window         = 14\n",
      "CW_VALUES              = [14, 28, 56, 84]\n",
      "[{'tide-cw-eval-v3-14': {'context_window': '14',\n",
      "                         'forecast_horizon': 14,\n",
      "                         'model_display_name': 'tide-cw-eval-v3-14',\n",
      "                         'num_trials': 3,\n",
      "                         'objective': 'minimize-wape-mae',\n",
      "                         'train_node_hrs': 250.0}},\n",
      " {'tide-cw-eval-v3-28': {'context_window': '28',\n",
      "                         'forecast_horizon': 14,\n",
      "                         'model_display_name': 'tide-cw-eval-v3-28',\n",
      "                         'num_trials': 3,\n",
      "                         'objective': 'minimize-wape-mae',\n",
      "                         'train_node_hrs': 250.0}},\n",
      " {'tide-cw-eval-v3-56': {'context_window': '56',\n",
      "                         'forecast_horizon': 14,\n",
      "                         'model_display_name': 'tide-cw-eval-v3-56',\n",
      "                         'num_trials': 3,\n",
      "                         'objective': 'minimize-wape-mae',\n",
      "                         'train_node_hrs': 250.0}},\n",
      " {'tide-cw-eval-v3-84': {'context_window': '84',\n",
      "                         'forecast_horizon': 14,\n",
      "                         'model_display_name': 'tide-cw-eval-v3-84',\n",
      "                         'num_trials': 3,\n",
      "                         'objective': 'minimize-wape-mae',\n",
      "                         'train_node_hrs': 250.0}}]\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_LIST = []\n",
    "\n",
    "for cw in CW_VALUES:\n",
    "    entry = {\n",
    "        f\"{EXPERIMENT_NAME}-{str(cw)}\": {\n",
    "            \"context_window\" : str(cw),\n",
    "            \"objective\" : optimization_objective,\n",
    "            \"model_display_name\" : f\"{EXPERIMENT_NAME}-{str(cw)}\",\n",
    "            \"forecast_horizon\" : forecast_horizon,\n",
    "            \"num_trials\" : NUM_SELECTED_TRIALS,\n",
    "            \"train_node_hrs\" : TRAIN_BUDGET_MILLI_NODE_HRS\n",
    "        },\n",
    "    }\n",
    "    EXPERIMENT_LIST.append(entry)\n",
    "\n",
    "pprint(EXPERIMENT_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d480f0-cb67-4d14-b2fd-dadbfe12432d",
   "metadata": {},
   "source": [
    "### Build pipeline\n",
    "\n",
    "* `parallelism=i` == i executions to be scheduled at a time (consider resource quotas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132dcc0-38db-4f8c-96e2-8930e1299c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPE_VERSION = \"v7\"\n",
    "DISPLAY_NAME = f\"{EXPERIMENT_NAME}-{RUN_NAME}-{PIPE_VERSION}\".replace(\"_\",\"-\")\n",
    "\n",
    "print(f\"DISPLAY_NAME: {DISPLAY_NAME}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12be4047-178f-431b-b633-fa2af8c2b5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src import (\n",
    "    create_bq_dataset,\n",
    "    collect_eval_metrics,\n",
    "    # args_generator_op,\n",
    "    args_generate_ints,\n",
    "    args_generate_string\n",
    ")\n",
    "\n",
    "_worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"n1-standard-16\"}},  # override for TF chief node\n",
    "    {},  # override for TF worker node, since it's not used, leave it empty\n",
    "    {},  # override for TF ps node, since it's not used, leave it empty\n",
    "    {\"machine_spec\": {\"machine_type\": \"n1-standard-16\"}},  # override for TF evaluator node\n",
    "]\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"{DISPLAY_NAME}\",\n",
    ")\n",
    "def cw_experiment_pipeline(\n",
    "    project_id: str, \n",
    "    region: str, \n",
    "    bq_location: str,\n",
    "    new_bq_dataset: str,\n",
    "    cwvalues: List[int],\n",
    "    experiment_name: str,\n",
    "    bq_source_uri: str,\n",
    "    optimization_objective: str,\n",
    "    train_budget_milli_node_hours: float,\n",
    "    num_selected_trials: int,\n",
    "    # stage_1_num_parallel_trials: int,\n",
    "    # stage_2_num_parallel_trials: int,\n",
    "):\n",
    "    \n",
    "    import logging\n",
    "    \n",
    "    # create BQ dataset\n",
    "    create_train_dataset_op = (\n",
    "        create_bq_dataset.create_bq_dataset(\n",
    "            project=project_id,\n",
    "            new_bq_dataset=new_bq_dataset,\n",
    "            bq_location=bq_location,\n",
    "        )\n",
    "        .set_display_name(\"Create BQ Dataset\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    time_series_dataset_create_op = (\n",
    "        TimeSeriesDatasetCreateOp(\n",
    "            display_name='train_ds_iowa_liquor',\n",
    "            bq_source=bq_source_uri,\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "        )\n",
    "        .set_display_name(\"Create Managed Dataset\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    args_generate_str_op = (\n",
    "        args_generate_string.args_generate_string(\n",
    "            cw_values=cwvalues,\n",
    "            opt_objective=optimization_objective,\n",
    "            experiment_name=experiment_name,\n",
    "        )\n",
    "        .set_display_name(\"Generate string args\")\n",
    "        .set_caching_options(False)\n",
    "    )\n",
    "\n",
    "    with dsl.ParallelFor(items=args_generate_str_op.output, parallelism=2) as item:\n",
    "        \n",
    "        args_generate_ints_op = (\n",
    "            args_generate_ints.args_generate_ints(\n",
    "                experiment_dict=item\n",
    "            )\n",
    "            .set_display_name(\"Generate integer args\")\n",
    "        )\n",
    "        \n",
    "        # TiDE tabular workflow config\n",
    "        (\n",
    "            pipe_template_path,\n",
    "            pipe_parameter_values,\n",
    "        ) = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "            project=PROJECT_ID,\n",
    "            location=REGION,\n",
    "            root_dir=ROOT_DIR,\n",
    "            model_display_name=item.model_display_name,           # item.model_display_name, | MODEL_DISPLAY_NAME\n",
    "            target_column=target_column,\n",
    "            optimization_objective=item.objective,                # item.objective | optimization_objective\n",
    "            transformations=transformations,\n",
    "            train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "            data_source_csv_filenames=data_source_csv_filenames,\n",
    "            data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "            weight_column=weight_column,\n",
    "            predefined_split_key=predefined_split_key,\n",
    "            training_fraction=training_fraction,\n",
    "            validation_fraction=validation_fraction,\n",
    "            test_fraction=test_fraction,\n",
    "            num_selected_trials=num_selected_trials,\n",
    "            time_column=time_column,\n",
    "            time_series_identifier_columns=[time_series_identifier_column],\n",
    "            time_series_attribute_columns=time_series_attribute_columns,\n",
    "            available_at_forecast_columns=available_at_forecast_columns,\n",
    "            unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "            forecast_horizon=forecast_horizon,\n",
    "            # context_window=item.context_window,                     # item.context_window | cw_item\n",
    "            dataflow_subnetwork=dataflow_subnetwork,\n",
    "            dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "            run_evaluation=RUN_EVALUATION,    \n",
    "            # evaluated_examples_bigquery_path=f'bq://{PROJECT_ID}.{BIGQUERY_DATASET_NAME}',\n",
    "            enable_probabilistic_inference=PROBABILISTIC_INFER,\n",
    "            # holiday_regions=['US','AE'],\n",
    "            # stage_1_tuner_worker_pool_specs_override=_worker_pool_specs_override,\n",
    "            # stage_2_trainer_worker_pool_specs_override=_worker_pool_specs_override,\n",
    "        )\n",
    "        # load pipeline component(s) from YAML\n",
    "        forecasting_pipeline_yaml = components.load_component_from_file(pipe_template_path)\n",
    "        \n",
    "        # TiDE tabular workflow pipeline step(s)\n",
    "        forecast_train_op = (\n",
    "            forecasting_pipeline_yaml(\n",
    "                **pipe_parameter_values,\n",
    "                vertex_dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "                evaluated_examples_bigquery_path=create_train_dataset_op.outputs['bq_dataset_uri'],\n",
    "                # model_display_name=MODEL_DISPLAY_NAME,\n",
    "                # optimization_objective=item.objective,\n",
    "                context_window=args_generate_ints_op.output,  #.outputs['cw_value'],\n",
    "                stage_1_tuner_worker_pool_specs_override=_worker_pool_specs_override,\n",
    "                stage_2_trainer_worker_pool_specs_override=_worker_pool_specs_override,\n",
    "            )\n",
    "            .set_display_name(\"VF Trainer\")\n",
    "            .set_caching_options(True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "694d9127-460c-41e0-942e-3f56e7b7010c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIPELINE_YAML_FILENAME = \"pipeline.yaml\"\n",
    "\n",
    "! rm -f $PIPELINE_YAML_FILENAME\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=cw_experiment_pipeline, \n",
    "    package_path=PIPELINE_YAML_FILENAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc447c17-024f-4635-b957-aaf256d4e17f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://pipeline.yaml [Content-Type=application/octet-stream]...\n",
      "/ [1 files][380.6 KiB/380.6 KiB]                                                \n",
      "Operation completed over 1 objects/380.6 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "PIPELINES_FILEPATH = f\"{BASE_OUTPUT_DIR}/{PIPELINE_YAML_FILENAME}\"\n",
    "\n",
    "!gsutil cp $PIPELINE_YAML_FILENAME $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6353abb9-3c5f-4837-a7ff-6b4f7fe54218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/forecast-cw-experiment-v1-20240109185421\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/forecast-cw-experiment-v1-20240109185421')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/forecast-cw-experiment-v1-20240109185421?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/forecast-cw-experiment-v1-20240109185421 to Experiment: tide-cw-eval-v3\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    location=REGION,\n",
    "    template_path=PIPELINE_YAML_FILENAME,\n",
    "    pipeline_root=ROOT_DIR,\n",
    "    failure_policy='fast',\n",
    "    parameter_values={\n",
    "        'project_id' : PROJECT_ID, \n",
    "        'region' : REGION, \n",
    "        'bq_location' : BQ_LOCATION,\n",
    "        'new_bq_dataset' : BIGQUERY_DATASET_NAME,\n",
    "        'cwvalues' : CW_VALUES,\n",
    "        'experiment_name' : EXPERIMENT_NAME,\n",
    "        'bq_source_uri' : data_source_bigquery_table_path,\n",
    "        'optimization_objective' : optimization_objective,\n",
    "        \"train_budget_milli_node_hours\": TRAIN_BUDGET_MILLI_NODE_HRS,\n",
    "        \"num_selected_trials\": NUM_SELECTED_TRIALS,\n",
    "    },\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "job.submit(\n",
    "    # experiment=EXPERIMENT_NAME,\n",
    "    # sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680eeb9b-df60-4bb0-a303-337721a0e2dd",
   "metadata": {},
   "source": [
    "#### get pipeline task details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c142363d-3597-4de2-94b0-6179f95e0ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# job.task_details\n",
    "\n",
    "# pipeline_task_details = job.task_details\n",
    "\n",
    "# for task_deets in pipeline_task_details:\n",
    "#     print(task_deets.task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6550d0-4d36-4bd1-b7e7-660a5660d9da",
   "metadata": {},
   "source": [
    "# Log Model Evaluations to Vertex AI Experiments\n",
    "\n",
    "**TODO**\n",
    "* for each model in experiment, add ts forecast for a specific slice/product/region/etc from data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d1798fb5-ec9e-4ed5-be3c-a0c59a3a4d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tmp - remove experiment_nme from pipeline job\n",
    "EXPERIMENT_NAME = f\"{EXPERIMENT_NAME}\"\n",
    "\n",
    "aiplatform.init(\n",
    "    experiment=EXPERIMENT_NAME, \n",
    "    project=PROJECT_ID, \n",
    "    location=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778378b9-bc74-4fb0-83aa-9bbfa7c6eb79",
   "metadata": {},
   "source": [
    "## Get trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "658fd465-d0ad-4ab8-9111-bbe54b10d2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of items: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projects/934903580331/locations/us-central1/models/3678295204445552640'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_list_tmp = []\n",
    "\n",
    "for task_deets in pipeline_task_details:\n",
    "    if task_deets.task_name == \"model-upload-2\":\n",
    "        \n",
    "        task_list_tmp.append(task_deets)\n",
    "        # print(task_deets.task_name)\n",
    "        \n",
    "print(f\"Length of items: {len(task_list_tmp)}\")\n",
    "\n",
    "# example\n",
    "task_list_tmp[0].outputs[\"model\"].artifacts[0].metadata['resourceName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a90e14-7fe5-46d8-93ef-f6e878471d56",
   "metadata": {},
   "source": [
    "## Log experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5013325f-4a10-4620-b443-946b1cf4b264",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "Logging for experiment run: tide-cw-eval-v3-84\n",
      "Associating projects/934903580331/locations/us-central1/metadataStores/default/contexts/tide-cw-eval-v3-tmp-jt-tide-cw-eval-v3-84 to Experiment: tide-cw-eval-v3-tmp-jt\n",
      "logging metrics...\n",
      "logging metaparams...\n",
      "\n",
      "i: 1\n",
      "Logging for experiment run: tide-cw-eval-v3-56\n",
      "Associating projects/934903580331/locations/us-central1/metadataStores/default/contexts/tide-cw-eval-v3-tmp-jt-tide-cw-eval-v3-56 to Experiment: tide-cw-eval-v3-tmp-jt\n",
      "logging metrics...\n",
      "logging metaparams...\n",
      "\n",
      "i: 2\n",
      "Logging for experiment run: tide-cw-eval-v3-14\n",
      "Associating projects/934903580331/locations/us-central1/metadataStores/default/contexts/tide-cw-eval-v3-tmp-jt-tide-cw-eval-v3-14 to Experiment: tide-cw-eval-v3-tmp-jt\n",
      "logging metrics...\n",
      "logging metaparams...\n",
      "\n",
      "i: 3\n",
      "Logging for experiment run: tide-cw-eval-v3-28\n",
      "Associating projects/934903580331/locations/us-central1/metadataStores/default/contexts/tide-cw-eval-v3-tmp-jt-tide-cw-eval-v3-28 to Experiment: tide-cw-eval-v3-tmp-jt\n",
      "logging metrics...\n",
      "logging metaparams...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "overall_list = [] \n",
    "log_metrics_dict = {}\n",
    "log_params_dict = {}\n",
    "\n",
    "for i in range(0, len(task_list_tmp)):\n",
    "    \n",
    "    print(f\"i: {i}\")\n",
    "    \n",
    "    # get trained model \n",
    "    model_name = task_list_tmp[i].outputs[\"model\"].artifacts[0].metadata['resourceName']\n",
    "    model = aiplatform.Model(model_name)\n",
    "    trained_display_name = model.to_dict()['displayName']\n",
    "    \n",
    "    # get model eval metrics\n",
    "    model_evaluations = model.list_model_evaluations()\n",
    "    for evaluation in model_evaluations:\n",
    "        metrics_dict = evaluation.to_dict()[\"metrics\"]\n",
    "        \n",
    "    # create metrics_dict to log\n",
    "    log_metrics_dict['rmsle'] = round(metrics_dict['rootMeanSquaredLogError'], 2)\n",
    "    log_metrics_dict['rmse'] = round(metrics_dict['rootMeanSquaredError'], 2)\n",
    "    log_metrics_dict['rmspe'] = round(metrics_dict['rootMeanSquaredPercentageError'], 2)\n",
    "    log_metrics_dict['r2'] = round(metrics_dict['rSquared'], 2)\n",
    "    log_metrics_dict['mape'] = round(metrics_dict['meanAbsolutePercentageError'], 2)\n",
    "    log_metrics_dict['wape'] = round(metrics_dict['weightedAbsolutePercentageError'], 2)\n",
    "    log_metrics_dict['mae'] = round(metrics_dict['meanAbsoluteError'], 2)\n",
    "    \n",
    "    # get experiment params\n",
    "    for exp in EXPERIMENT_LIST:\n",
    "        for k,v in exp.items():\n",
    "            if k == trained_display_name:\n",
    "                log_params_dict['cw']             = int(v['context_window'])\n",
    "                log_params_dict['opt_obj']        = v['objective']\n",
    "                log_params_dict['num_trials']     = int(v['num_trials'])\n",
    "                log_params_dict['train_node_hrs'] = float(v['train_node_hrs'])\n",
    "                log_params_dict['display_name']   = v['model_display_name']\n",
    "    \n",
    "    EXPERIMENT_RUN_NAME = trained_display_name\n",
    "    print(f\"Logging for experiment run: {EXPERIMENT_RUN_NAME}\")\n",
    "    \n",
    "    with aiplatform.start_run(f'{trained_display_name}') as my_run:\n",
    "\n",
    "        print(f\"logging metrics...\")\n",
    "        my_run.log_metrics(log_metrics_dict)\n",
    "\n",
    "        print(f\"logging metaparams...\\n\")\n",
    "        my_run.log_params(log_params_dict)\n",
    "\n",
    "        aiplatform.end_run()\n",
    "        \n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc541116-2e76-46ee-88ed-824c932b4d32",
   "metadata": {},
   "source": [
    "### Inspect model_eval object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "55b139a6-1e37-460d-a2b2-4a7577492a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createTime': '2024-01-04T12:27:50.196401Z',\n",
      " 'displayName': 'Vertex Forecasting pipeline',\n",
      " 'metadata': {'evaluation_dataset_path': ['bq://hybrid-vertex.vertex_feature_transform_engine_staging_us.vertex_ai_fte_split_output_test_staging_idb200b234d5404c0da302ae412489a3bb'],\n",
      "              'evaluation_dataset_type': 'bigquery',\n",
      "              'pipeline_job_id': '333446967210278912',\n",
      "              'pipeline_job_resource_name': 'projects/934903580331/locations/us-central1/pipelineJobs/forecast-cw-experiment-v1-20240104115742'},\n",
      " 'metrics': {'meanAbsoluteError': 4006.6643,\n",
      "             'meanAbsolutePercentageError': 428.84814,\n",
      "             'rSquared': 0.55919313,\n",
      "             'rootMeanSquaredError': 8907.6455,\n",
      "             'rootMeanSquaredLogError': 0.9567208,\n",
      "             'rootMeanSquaredPercentageError': 5700.1587,\n",
      "             'weightedAbsolutePercentageError': 47.937183},\n",
      " 'metricsSchemaUri': 'gs://google-cloud-aiplatform/schema/modelevaluation/forecasting_metrics_1.0.0.yaml',\n",
      " 'modelExplanation': {'meanAttributions': [{'featureAttributions': {'city': 29.351781598855478,\n",
      "                                                                    'county': 16.97163820687296,\n",
      "                                                                    'date': 159.27545151050458,\n",
      "                                                                    'sale_dollars': 5030.197243606408,\n",
      "                                                                    'zip_code': 4.4398418408976354}}]},\n",
      " 'name': 'projects/934903580331/locations/us-central1/models/1572158691208069120@1/evaluations/4175312550286328322'}\n"
     ]
    }
   ],
   "source": [
    "forecast_EVALS = model.list_model_evaluations()\n",
    "\n",
    "for model_evaluation in forecast_EVALS:\n",
    "    pprint(model_evaluation.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e94899-ad70-4152-9947-fd7bd0910291",
   "metadata": {},
   "source": [
    "# Visualize with Data Studio\n",
    "\n",
    "> TODO\n",
    "\n",
    "The code block included in this section dynamically generates a Data Studio link that specifies the template, the location of the forecasts, and the query to generate the chart. The data is populated from the forecasts generated earlier.\n",
    "\n",
    "You can inspect the used template [here](https://datastudio.google.com/c/u/0/reporting/067f70d2-8cd6-4a4c-a099-292acd1053e8). This was created by Google specifically to view forecasting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693ecc2-48eb-477b-ab9f-b53b1fcaacd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _sanitize_bq_uri(bq_uri: str):\n",
    "#     if bq_uri.startswith(\"bq://\"):\n",
    "#         bq_uri = bq_uri[5:]\n",
    "#     return bq_uri.replace(\":\", \".\")\n",
    "\n",
    "\n",
    "# def get_data_studio_link(\n",
    "#     batch_prediction_bq_input_uri: str,\n",
    "#     batch_prediction_bq_output_uri: str,\n",
    "#     time_column: str,\n",
    "#     time_series_identifier_column: str,\n",
    "#     target_column: str,\n",
    "# ):\n",
    "#     \"\"\"Creates a link that fills in the demo Data Studio template.\"\"\"\n",
    "#     batch_prediction_bq_input_uri = _sanitize_bq_uri(batch_prediction_bq_input_uri)\n",
    "#     batch_prediction_bq_output_uri = _sanitize_bq_uri(batch_prediction_bq_output_uri)\n",
    "#     query = f\"\"\"\n",
    "#         SELECT\n",
    "#           CAST(input.{time_column} as DATETIME) timestamp_col,\n",
    "#           CAST(input.{time_series_identifier_column} as STRING) time_series_identifier_col,\n",
    "#           CAST(input.{target_column} as NUMERIC) historical_values,\n",
    "#           CAST(predicted_{target_column}.value as NUMERIC) predicted_values,\n",
    "#         FROM `{batch_prediction_bq_input_uri}` input\n",
    "#         LEFT JOIN `{batch_prediction_bq_output_uri}` output\n",
    "#           ON\n",
    "#             TIMESTAMP(input.{time_column}) = TIMESTAMP(output.{time_column})\n",
    "#             AND CAST(input.{time_series_identifier_column} as STRING) = CAST(\n",
    "#               output.{time_series_identifier_column} as STRING)\n",
    "#     \"\"\"\n",
    "#     params = {\n",
    "#         \"templateId\": \"067f70d2-8cd6-4a4c-a099-292acd1053e8\",\n",
    "#         \"ds0.connector\": \"BIG_QUERY\",\n",
    "#         \"ds0.projectId\": PROJECT_ID,\n",
    "#         \"ds0.billingProjectId\": PROJECT_ID,\n",
    "#         \"ds0.type\": \"CUSTOM_QUERY\",\n",
    "#         \"ds0.sql\": query,\n",
    "#     }\n",
    "#     base_url = \"https://datastudio.google.com/c/u/0/reporting\"\n",
    "#     url_params = urllib.parse.urlencode({\"params\": json.dumps(params)})\n",
    "#     return f\"{base_url}?{url_params}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c333d-b2a2-4c96-bfa2-4878795c8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actuals_table = f\"{dataset_path}.actuals\"\n",
    "# query = f\"\"\"\n",
    "#     CREATE OR REPLACE TABLE `{actuals_table}` AS\n",
    "#     {base_data_query}\n",
    "#     SELECT *\n",
    "#     FROM base_data\n",
    "#     WHERE split != 'TRAIN'\n",
    "# \"\"\"\n",
    "# client.query(query).result()\n",
    "# print(f\"Created {actuals_table}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474da212-9369-4c98-b866-70f1fddf5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Click the link below to view ARIMA predictions:\")\n",
    "# print(\n",
    "#     get_data_studio_link(\n",
    "#         batch_prediction_bq_input_uri=actuals_table,\n",
    "#         batch_prediction_bq_output_uri=f\"{dataset_path}.{pred_table}\",\n",
    "#         time_column=time_column,\n",
    "#         time_series_identifier_column=time_series_identifier_column,\n",
    "#         target_column=target_column,\n",
    "#     )\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
