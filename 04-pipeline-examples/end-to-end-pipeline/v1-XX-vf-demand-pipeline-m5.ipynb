{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDXJsws2W4GS"
   },
   "source": [
    "# Vertex Forecast: Retail Demand Pipelines with M5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU2k2Mv9W2aR"
   },
   "source": [
    "### Retail demand forecasting with Vertex Forecast and Pipelines\n",
    "\n",
    "This notebook demonstrates the use of Vertex Pipelines to orchestrate Vertex Forecast (VF) workflows. It makes use of three tables that define the retail demand schema for this example:\n",
    "\n",
    "**Dataset**\n",
    "* Uses the M5 public dataset\n",
    "* See the [M5 Dataprep Notebook](https://colab.research.google.com/drive/1rjff9GVdjP--go_V9v2hV_heIFMtX3Mc?usp=sharing) for step-by-step instructions for downloading and preparing the M5 dataset\n",
    "\n",
    "**Tables in the dataset**\n",
    "- **activity**: activity table, containing time-variant information, e.g., sales, promo, etc.\n",
    "- **product**: the product table containing metadata of items\n",
    "- **plan table**: the table used to generate forecasts for future periods where actuals are unknown\n",
    "\n",
    "**Modeling approach**\n",
    "* Two models are trained, each with a different optimization objective: `minimize-rmse` & `minimize-mape`\n",
    "* The predictions from both models aere stacked (averaged), to produce the final prediction\n",
    "* In the training job configuration, `additional_experiments` is used to specify two overrides:\n",
    "> * `'forecasting_model_type_override': 'seq2seq'` - instructs VF to only train a sequence-to-sequence model\n",
    "> * `'forecasting_hierarchical_group_column_names':'dept_id, cat_id'` - specifies hierarchical modeling, and which input features to aggregate\n",
    "\n",
    "**Model Pipeline**\n",
    "* This pipeline includes all steps needed to train and evlauate the models, as well as generate a forecast/prediction for the future\n",
    "* It includes examples of custom pipeline components and pre-built components from Vertex AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BZ4E1uhdh5v"
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNXJzYea4PV0"
   },
   "source": [
    "**Tip:** search for `TODO` to find variables/parameters to specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MucqTNkgv5PS",
    "outputId": "fd5f8d74-4377-4ac2-b1d2-444abb5daa8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1'\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Colab stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C34pTnYM13Xt"
   },
   "source": [
    "**If running this notebook in Colab**, authenticate with user account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "r-OQQrsW1yF0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    \n",
    "if 'google.colab' in sys.modules:\n",
    "    USER_FLAG = ''\n",
    "else:\n",
    "    USER_FLAG = '--user'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeJ1lH5z2KIS"
   },
   "source": [
    "#### Install KFP SDK and Vertex Pipelines client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ejzDQo1qFMny"
   },
   "outputs": [],
   "source": [
    "# # New\n",
    "# ! pip install -U google-cloud-storage $USER_FLAG\n",
    "# ! pip install $USER_FLAG kfp google-cloud-pipeline-components --upgrade\n",
    "# # !git clone https://github.com/kubeflow/pipelines.git\n",
    "# # !pip install pipelines/components/google-cloud/.\n",
    "# !pip -q install $USER_FLAG --upgrade google-cloud-aiplatform\n",
    "\n",
    "# # Automatically restart kernel after installs\n",
    "# import IPython\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ACsQkK9lAhoG",
    "outputId": "805e380f-3b50-42fb-ea0f-e60481bed4a9"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# if 'google.colab' in sys.modules:\n",
    "#     PROJECT_ID = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "#     !gcloud config set project {PROJECT_ID}\n",
    "#     from google.colab import auth\n",
    "#     auth.authenticate_user()\n",
    "#     USER_FLAG = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yX2XtuvAQGh"
   },
   "source": [
    "Check the versions of the installed packages\n",
    "* KFP SDK version should be >=1.8.9\n",
    "* GCP components should be >= 0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wtIOwOdAWYQ",
    "outputId": "39fcc344-4962-4b70-807d-3c6b4ea8a2ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.19\n",
      "google_cloud_pipeline_components version: 1.0.41\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnlLTWe0dxbt"
   },
   "source": [
    "## Pipeline Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lsSf-hFBDBP"
   },
   "source": [
    "### Set some variables\n",
    "\n",
    "**Before you run the next cell**, **edit it** to set variables for your project.\n",
    "\n",
    "* For `BUCKET_NAME`, enter the name of a Cloud Storage (GCS) bucket in your project.  Don't include the `gs://` prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9pym-S3NptV"
   },
   "source": [
    "GCP Project Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2ZJX1hONu9O",
    "outputId": "81965b72-689f-449f-f69d-b19ec1492e3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "REGION: us-central1\n",
      "env: GOOGLE_CLOUD_PROJECT=hybrid-vertex\n"
     ]
    }
   ],
   "source": [
    "# GCP Project Configuration:\n",
    "# project where pipeline and vertex jobs are executed\n",
    "\n",
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "REGION = 'us-central1'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"REGION: {REGION}\")\n",
    "\n",
    "assert LOCATION, 'the value for this variable must be set'\n",
    "assert PROJECT_ID, 'the value for this variable must be set'\n",
    "# assert PROJECT_NUMBER, 'the value for this variable must be set'\n",
    "\n",
    "%env GOOGLE_CLOUD_PROJECT={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-jjgdP1OfLm"
   },
   "source": [
    "Required Pipeline Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lATxPx73SNn3"
   },
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0h2HqvT5cw_-"
   },
   "outputs": [],
   "source": [
    "# packages\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image, clear_output\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# google cloud\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental import forecasting as gcc_aip_forecasting\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# kfp\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67J7CzYnW1Lw",
    "outputId": "4e6306a3-d1b2-4d57-b3f0-63c024e78ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertex_ai SDK version: 1.22.1\n",
      "bigquery SDK version: 3.7.0\n"
     ]
    }
   ],
   "source": [
    "print(f'vertex_ai SDK version: {vertex_ai.__version__}')\n",
    "print(f'bigquery SDK version: {bigquery.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SILo2Q23SQ5Z"
   },
   "source": [
    "BQ and pipeline clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup SDK Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID, \n",
    "    # credentials=credentials\n",
    ")\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnKwQdxhdjfT"
   },
   "source": [
    "# Training, Evaluation, and Forecast Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsW59-Xq6NwP",
    "tags": []
   },
   "source": [
    "### Pipeline visualization in Vertex UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxDwdJxtarF5"
   },
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhdmQN7XTe_h"
   },
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-forecas-repo/04-pipeline-examples/end-to-end-pipeline\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "\n",
    "! rm -rf $REPO_DOCKER_PATH_PREFIX\n",
    "! mkdir $REPO_DOCKER_PATH_PREFIX\n",
    "# !mkdir -p ./$REPO_DOCKER_PATH_PREFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO9Z3VLT__uy"
   },
   "source": [
    "### Pipeline Component: **Create BQ dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9AFUbW0UNFB"
   },
   "source": [
    "Creates BQ dataset, if one doesnt exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5cM12L5hADDr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/create_bq_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/create_bq_dataset.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-bigquery==2.34.4'],\n",
    ")\n",
    "def create_bq_dataset(\n",
    "    project: str,\n",
    "    vertex_dataset: str,\n",
    "    new_bq_dataset: str,\n",
    "    bq_location: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('bq_dataset_name', str),\n",
    "    ('bq_dataset_uri', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    bq_client = bigquery.Client(project=project, location='US') # bq_location)\n",
    "    (\n",
    "      bq_client.query(f'CREATE SCHEMA IF NOT EXISTS `{project}.{new_bq_dataset}`')\n",
    "      .result()\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        f'{new_bq_dataset}',\n",
    "        f'bq://{project}:{new_bq_dataset}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS5fsWdNlafn"
   },
   "source": [
    "### Pipeline component: **Input Table Specs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhVS_R0h0sBc"
   },
   "source": [
    "Note\n",
    "* `time_granularity_unit` - eligible unit values are: MINUTE, HOUR, DAY, WEEK, MONTH, YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oKecgPn9nkru"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/create_input_table_specs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/create_input_table_specs.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9'\n",
    ")\n",
    "def create_input_table_specs(\n",
    "    products_table_uri: str,\n",
    "    activities_table_uri: str,\n",
    "    locations_table_uri: str,\n",
    "    time_granularity_unit: str,\n",
    "    time_granularity_quantity: int,\n",
    "    # train_data_bq_source: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('input_table_specs', str),\n",
    "    ('model_feature_columns', str),\n",
    "    # ('train_data_source_bq_uri', str),\n",
    "]):\n",
    "    \n",
    "    import json\n",
    "    import logging\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    # train_data_source_bq_uri = f'bq://{train_data_bq_source}'\n",
    "\n",
    "    products_table_specs = {\n",
    "        'bigquery_uri': products_table_uri,\n",
    "        'table_type': 'FORECASTING_ATTRIBUTE',\n",
    "        'forecasting_attribute_table_metadata': {\n",
    "            'primary_key_column': 'product_id'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    locations_table_specs = {\n",
    "        'bigquery_uri': locations_table_uri,\n",
    "        'table_type': 'FORECASTING_ATTRIBUTE',\n",
    "        'forecasting_attribute_table_metadata': {\n",
    "            'primary_key_column': 'location_id'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    activities_table_specs = {\n",
    "        'bigquery_uri': activities_table_uri,\n",
    "        'table_type': 'FORECASTING_PRIMARY',\n",
    "        'forecasting_primary_table_metadata': {\n",
    "            'time_column': 'date',\n",
    "            'target_column': 'gross_quantity',\n",
    "            'time_series_identifier_columns': ['product_id', 'location_id'],\n",
    "            'unavailable_at_forecast_columns': [],\n",
    "            'time_granularity': {\n",
    "                'unit': time_granularity_unit,\n",
    "                'quantity': time_granularity_quantity,\n",
    "            },\n",
    "            # 'predefined_splits_column': 'ml_use',\n",
    "            # 'predefined_split_column': 'ml_use', # model_override\n",
    "        }\n",
    "    }\n",
    "\n",
    "    model_feature_columns = [\n",
    "        'product_id',\n",
    "        'location_id',\n",
    "        'gross_quantity',\n",
    "        'date',\n",
    "        'weekday',\n",
    "        'wday',\n",
    "        'month',\n",
    "        'year',\n",
    "        'event_name_1',\n",
    "        'event_type_1',\n",
    "        'event_name_2',\n",
    "        'event_type_2',\n",
    "        'snap_CA',\n",
    "        'snap_TX'\n",
    "        'snap_WI',\n",
    "        'dept_id',\n",
    "        'cat_id',\n",
    "        'state_id',\n",
    "    ]\n",
    "\n",
    "    input_table_specs = [\n",
    "    activities_table_specs,\n",
    "    products_table_specs,\n",
    "    locations_table_specs,\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "        json.dumps(input_table_specs),  # input_table_specs\n",
    "        json.dumps(model_feature_columns),  # model_feature_columns\n",
    "        # f'{train_data_source_bq_uri}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZgrcH6DQYZk"
   },
   "source": [
    "### Pipeline Component: Get Eval Dataset uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VhHRQfIBQdaw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/get_eval_dataset_path_uri.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/get_eval_dataset_path_uri.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(base_image='python:3.9')\n",
    "def get_eval_dataset_path_uri(\n",
    "    project: str,\n",
    "    eval_bq_dataset: str,\n",
    "    model_1_table: str,\n",
    "    model_2_table: str,\n",
    ") -> NamedTuple('Outputs',[\n",
    "    ('model_1_bigquery_table_uri', str),\n",
    "    ('model_2_bigquery_table_uri', str),\n",
    "    ('eval_bq_dataset', str),\n",
    "]):\n",
    "    \n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    model_1_table_path_name = f'{project}:{eval_bq_dataset}:eval-{model_1_table}'\n",
    "    model_2_table_path_name = f'{project}:{eval_bq_dataset}:eval-{model_2_table}'\n",
    "\n",
    "    logging.info(model_1_table_path_name)\n",
    "    logging.info(model_2_table_path_name)\n",
    "\n",
    "    return (\n",
    "        f'bq://{model_1_table_path_name}',\n",
    "        f'bq://{model_2_table_path_name}',\n",
    "        f'{eval_bq_dataset}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3YGooBYtmJw"
   },
   "source": [
    "### Pipeline component: combine model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hmzRsr0c-GF4"
   },
   "outputs": [],
   "source": [
    "# from typing import Dict, List, Optional, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KbKX5kHkuHTs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/create_combined_preds_table.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/create_combined_preds_table.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=[\n",
    "      'google-cloud-bigquery==2.34.4', \n",
    "      'google-cloud-aiplatform==1.21.0'\n",
    "  ],\n",
    ")\n",
    "def create_combined_preds_table(\n",
    "  project: str,\n",
    "  dataset: str,\n",
    "  bq_location: str,\n",
    "  model_1_eval_table_uri: str,\n",
    "  model_2_eval_table_uri: str,\n",
    "  model_1_path: Input[Artifact],\n",
    "  model_2_path: Input[Artifact],\n",
    "  override: str = 'False',\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('combined_preds_table_uri', str),\n",
    "    ('eval_bq_dataset', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    override = bool(override)\n",
    "    \n",
    "    bq_client = bigquery.Client(\n",
    "        project=project, \n",
    "        location=\"US\", # bq_location\n",
    "    )\n",
    "    \n",
    "    combined_preds_table_name = f'{project}.{dataset}.combined_preds'\n",
    "\n",
    "    model_1_eval_table_uri=model_1_eval_table_uri\n",
    "    model_2_eval_table_uri=model_2_eval_table_uri\n",
    "\n",
    "    def _sanitize_bq_uri(bq_uri):\n",
    "        if bq_uri.startswith(\"bq://\"):\n",
    "            bq_uri = bq_uri[5:]\n",
    "        return bq_uri.replace(\":\", \".\")\n",
    "\n",
    "    model_1_eval_table_uri = _sanitize_bq_uri(\n",
    "        model_1_eval_table_uri\n",
    "    )\n",
    "\n",
    "    model_2_eval_table_uri = _sanitize_bq_uri(\n",
    "        model_2_eval_table_uri\n",
    "    )\n",
    "\n",
    "    (\n",
    "        bq_client.query(\n",
    "            f\"\"\"\n",
    "            CREATE {'OR REPLACE TABLE' if override else 'TABLE IF NOT EXISTS'}\n",
    "                `{combined_preds_table_name}`\n",
    "            AS (\n",
    "                SELECT * except(row_number) from\n",
    "                (\n",
    "                  SELECT *,ROW_NUMBER() OVER (PARTITION BY datetime,vertex__timeseries__id order by predicted_on_date asc) row_number\n",
    "                  FROM\n",
    "                (\n",
    "                  SELECT\n",
    "                  DATE(table_a.date) as datetime,\n",
    "                  DATE(table_a.predicted_on_date) as predicted_on_date,\n",
    "                  CAST(table_a.gross_quantity as INTEGER) as gross_quantity,\n",
    "                  table_a.vertex__timeseries__id,\n",
    "                  table_a.predicted_gross_quantity.value as predicted_gross_quantity_a,\n",
    "                  table_b.predicted_gross_quantity.value as predicted_gross_quantity_b\n",
    "                  FROM\n",
    "                  `{model_1_eval_table_uri}` AS table_a\n",
    "                  INNER JOIN `{model_2_eval_table_uri}` AS table_b\n",
    "                  ON DATE(table_a.date) = DATE(table_b.date)\n",
    "                  and table_a.vertex__timeseries__id = table_b.vertex__timeseries__id\n",
    "                  and DATE(table_a.predicted_on_date) = DATE(table_b.predicted_on_date)\n",
    "                ) a\n",
    "              )m\n",
    "              where row_number = 1\n",
    "            );\n",
    "          \"\"\"\n",
    "        )\n",
    "        .result()\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        f'bq://{combined_preds_table_name}',\n",
    "        f'{dataset}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline ComponentL create final pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lOqPuFR66DWz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/create_final_pred_table.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/create_final_pred_table.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=[\n",
    "      'google-cloud-bigquery==2.34.4'\n",
    "  ],\n",
    ")\n",
    "def create_final_pred_table(\n",
    "    project: str,\n",
    "    dataset: str,\n",
    "    bq_location: str,\n",
    "    combined_preds_table_uri: str,\n",
    "    override: str = 'False',\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('final_preds_table_uri', str),\n",
    "]):\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    override = bool(override)\n",
    "    \n",
    "    bq_client = bigquery.Client(\n",
    "        project=project, \n",
    "        location=\"US\", # bq_location\n",
    "    )\n",
    "    \n",
    "    final_preds_table_name = f'{project}.{dataset}.final_preds'\n",
    "    \n",
    "    (\n",
    "        bq_client.query(\n",
    "            f\"\"\"\n",
    "            CREATE {'OR REPLACE TABLE' if override else 'TABLE IF NOT EXISTS'}\n",
    "                `{final_preds_table_name}`\n",
    "            AS (\n",
    "              SELECT\n",
    "                  datetime,\n",
    "                  vertex__timeseries__id,\n",
    "                  gross_quantity as gross_quantity_actual,\n",
    "                  ROUND(a.predicted_gross_quantity_a, 2) as model_a_pred,\n",
    "                  ROUND(a.predicted_gross_quantity_b, 2) as model_b_pred,\n",
    "                  ROUND((a.predicted_gross_quantity_a + a.predicted_gross_quantity_b)/2, 2) AS Final_Pred,\n",
    "                  ROUND(ABS(gross_quantity - ((a.predicted_gross_quantity_a + a.predicted_gross_quantity_b)/2)), 2) as Final_Pred_error,\n",
    "              FROM\n",
    "                `{combined_preds_table_uri[5:]}` AS a);\n",
    "            \"\"\"\n",
    "        )\n",
    "        .result()\n",
    "    )\n",
    "\n",
    "    return (\n",
    "      f'bq://{final_preds_table_name}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21o5eMdw6ynk"
   },
   "source": [
    "### Pipeline component: Create Forecast input table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "l6UcST27qLf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/create_forecast_input_table_specs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/create_forecast_input_table_specs.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0'\n",
    "    ],\n",
    ")\n",
    "def create_forecast_input_table_specs(\n",
    "  project: str,\n",
    "  forecast_products_table_uri: str,\n",
    "  forecast_activities_table_uri: str,\n",
    "  forecast_locations_table_uri: str,\n",
    "  forecast_plan_table_uri: str,\n",
    "  time_granularity_unit: str,\n",
    "  time_granularity_quantity: int,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('forecast_input_table_specs', str)\n",
    "]):\n",
    "    import json\n",
    "    import os\n",
    "    import logging\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    forecast_input_table_specs = [\n",
    "        {\n",
    "            'bigquery_uri': forecast_plan_table_uri,\n",
    "            'table_type': 'FORECASTING_PLAN',\n",
    "        },\n",
    "        {\n",
    "            'bigquery_uri': forecast_activities_table_uri,\n",
    "            'table_type': 'FORECASTING_PRIMARY',\n",
    "            'forecasting_primary_table_metadata': {\n",
    "                'time_column': 'date',\n",
    "                'target_column': 'gross_quantity',\n",
    "                'time_series_identifier_columns': ['product_id', 'location_id'],\n",
    "                'unavailable_at_forecast_columns': [],\n",
    "                'time_granularity': {\n",
    "                    'unit': time_granularity_unit,\n",
    "                    'quantity': time_granularity_quantity,\n",
    "                },\n",
    "                # 'predefined_splits_column': 'ml_use',\n",
    "                # 'predefined_split_column': 'ml_use', # model_override\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'bigquery_uri': forecast_products_table_uri,\n",
    "            'table_type': 'FORECASTING_ATTRIBUTE',\n",
    "            'forecasting_attribute_table_metadata': {\n",
    "                'primary_key_column': 'product_id'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'bigquery_uri': forecast_locations_table_uri,\n",
    "            'table_type': 'FORECASTING_ATTRIBUTE',\n",
    "            'forecasting_attribute_table_metadata': {\n",
    "                'primary_key_column': 'location_id'\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    logging.info(f\"forecast_input_table_specs: {forecast_input_table_specs}\")\n",
    "\n",
    "    return (\n",
    "        json.dumps(forecast_input_table_specs),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "832J2SZErXBb"
   },
   "source": [
    "### Pipeline Component: Get predict table path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jgo7ln-prWom"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/get_predict_table_path.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/get_predict_table_path.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9'\n",
    ")\n",
    "def get_predict_table_path(\n",
    "    predict_processed_table: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('preprocess_bq_uri', str)\n",
    "]):\n",
    "    \n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    logging.info(f\"predict_processed_table: {predict_processed_table}\")\n",
    "    \n",
    "    preprocess_bq_uri = (\n",
    "        json.loads(predict_processed_table)\n",
    "        ['processed_bigquery_table_uri']\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"preprocess_bq_uri: {preprocess_bq_uri}\")\n",
    "    \n",
    "    return (\n",
    "        f'{preprocess_bq_uri}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Component: get model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/get_model_path.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/get_model_path.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0'\n",
    "    ],\n",
    ")\n",
    "def get_model_path(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model: Input[Artifact],\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('model_resource_path', str)\n",
    "]):\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    model_resource_path = model.metadata[\"resourceName\"]\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "    \n",
    "    return (\n",
    "        model_resource_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0gd6pDSrbf_"
   },
   "source": [
    "### Pipeline Component: model_batch_prediction_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ea4OQhysrdaR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/model_batch_prediction_job.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/model_batch_prediction_job.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0'\n",
    "    ],\n",
    ")\n",
    "def model_batch_prediction_job(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    eval_bq_dataset: str,\n",
    "    bigquery_source: str,\n",
    "    model_name: str,\n",
    "    model_1_path: Input[Artifact],\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('batch_predict_output_bq_uri', str),\n",
    "    ('batch_predict_job_dict', dict)\n",
    "]):\n",
    "\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    model_resource_path = model_1_path.metadata[\"resourceName\"]\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "    model = vertex_ai.Model(model_name=model_resource_path)\n",
    "    logging.info(\"Model dict:\", model.to_dict())\n",
    "\n",
    "    batch_predict_job = model.batch_predict(\n",
    "        bigquery_source=bigquery_source,\n",
    "        instances_format=\"bigquery\",\n",
    "        bigquery_destination_prefix=f'bq://{project}.{eval_bq_dataset}',\n",
    "        predictions_format=\"bigquery\",\n",
    "        job_display_name=f'bpj-{model_name}',\n",
    "    )\n",
    "\n",
    "    batch_predict_bq_output_uri = \"{}.{}\".format(\n",
    "      batch_predict_job.output_info.bigquery_output_dataset,\n",
    "      batch_predict_job.output_info.bigquery_output_table\n",
    "    )\n",
    "\n",
    "    logging.info(batch_predict_job.to_dict())\n",
    "    \n",
    "    return (\n",
    "        batch_predict_bq_output_uri,\n",
    "        batch_predict_job.to_dict(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaRSq5rwrkrJ"
   },
   "source": [
    "#### TODO: remove duplicate batch prediction component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "N922fVCHrnh2"
   },
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/model_2_predict_job_v2.py\n",
    "\n",
    "# import kfp\n",
    "# from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "# from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "#                         OutputPath, component, Metrics)\n",
    "\n",
    "# @kfp.v2.dsl.component(\n",
    "#   base_image='python:3.9',\n",
    "#   packages_to_install=['google-cloud-aiplatform==1.21.0'],\n",
    "# )\n",
    "# def model_2_predict_job_v2(\n",
    "#     project: str,\n",
    "#     location: str,\n",
    "#     eval_bq_dataset: str,\n",
    "#     bigquery_source: str,\n",
    "#     model_2_path: Input[Artifact],\n",
    "# ) -> NamedTuple('Outputs', [\n",
    "#                             ('batch_predict_output_bq_uri', str),\n",
    "#                             ('batch_predict_job_dict', dict)]):\n",
    "\n",
    "#   from google.cloud import aiplatform as vertex_ai\n",
    "#   # from google_cloud_pipeline_components.types import artifact_types\n",
    "#   import json\n",
    "#   import logging\n",
    "\n",
    "#   vertex_ai.init(\n",
    "#       project=project,\n",
    "#       location=location,\n",
    "#   )\n",
    "#   # print(\"model_2_path:\", model_2_path)\n",
    "#   # model_aip_uri=model_2_path[\"uri\"]\n",
    "#   # print(\"model_2_path[49:]:\", model_2_path[49:])\n",
    "#   # model_aip_uri=model_2_path[49:]\n",
    "#   # print(\"model_aip_uri_2:\", model_aip_uri)\n",
    "\n",
    "#   # model = artifact_types.VertexModel(uri='xxx')\n",
    "\n",
    "#   model_resource_path = model_2_path.metadata[\"resourceName\"]\n",
    "#   logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "#   model = vertex_ai.Model(model_name=model_resource_path)\n",
    "#   print(\"Model dict:\", model.to_dict())\n",
    "\n",
    "#   batch_predict_job = model.batch_predict(\n",
    "#       bigquery_source=bigquery_source,\n",
    "#       instances_format=\"bigquery\",\n",
    "#       bigquery_destination_prefix=f'bq://{project}.{eval_bq_dataset}',\n",
    "#       predictions_format=\"bigquery\",\n",
    "#       job_display_name='batch-predict-job-2',\n",
    "#   )\n",
    "\n",
    "#   batch_predict_bq_output_uri = \"{}.{}\".format(\n",
    "#       batch_predict_job.output_info.bigquery_output_dataset,\n",
    "#       batch_predict_job.output_info.bigquery_output_table)\n",
    "\n",
    "#   # if batch_predict_bq_output_uri.startswith(\"bq://\"):\n",
    "#   #   batch_predict_bq_output_uri = batch_predict_bq_output_uri[5:]\n",
    "\n",
    "#   # batch_predict_bq_output_uri.replace(\":\", \".\")\n",
    "\n",
    "#   print(batch_predict_job.to_dict())\n",
    "#   return (batch_predict_bq_output_uri,\n",
    "#           batch_predict_job.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOrmelEVr47j"
   },
   "source": [
    "### Pipeline Component: Combine Plan Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "cdD9x3D1r7O6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/create_combined_preds_forecast_table.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/create_combined_preds_forecast_table.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-bigquery==2.34.4'\n",
    "  ],\n",
    ")\n",
    "def create_combined_preds_forecast_table(\n",
    "  project: str,\n",
    "  dataset: str,\n",
    "  model_1_pred_table_uri: str,\n",
    "  model_2_pred_table_uri: str,\n",
    "  override: str = 'False',\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('combined_preds_forecast_table_uri', str)\n",
    "]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    override = bool(override)\n",
    "    bq_client = bigquery.Client(project=project)\n",
    "    combined_preds_forecast_table_name = f'{project}.{dataset}.combined_preds_forecast'\n",
    "    (\n",
    "        bq_client.query(\n",
    "            f\"\"\"\n",
    "            CREATE {'OR REPLACE TABLE' if override else 'TABLE IF NOT EXISTS'}\n",
    "                `{combined_preds_forecast_table_name}`\n",
    "            AS (\n",
    "              SELECT\n",
    "                table_a.date as date,\n",
    "                table_a.vertex__timeseries__id,\n",
    "                ROUND(table_a.predicted_gross_quantity.value,2) as predicted_gross_quantity_a,\n",
    "                ROUND(table_b.predicted_gross_quantity.value, 2) as predicted_gross_quantity_b,\n",
    "                ROUND((table_a.predicted_gross_quantity.value + table_b.predicted_gross_quantity.value)/2, 2) AS Final_Pred\n",
    "              FROM\n",
    "                `{model_1_pred_table_uri[5:]}` AS table_a\n",
    "              INNER JOIN `{model_2_pred_table_uri[5:]}` AS table_b\n",
    "              ON table_a.date = table_b.date\n",
    "              AND table_a.vertex__timeseries__id = table_b.vertex__timeseries__id\n",
    "            );\n",
    "          \"\"\"\n",
    "        )\n",
    "        .result()\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        f'bq://{combined_preds_forecast_table_name}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline component: args_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/args_generator_op_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/args_generator_op_1.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0'\n",
    "    ],\n",
    ")\n",
    "def args_generator_op_1(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    rmse_model: Input[Artifact],\n",
    "    rmse_display_name: str,\n",
    "    mape_model: Input[Artifact],\n",
    "    mape_display_name: str,\n",
    "# ) -> str:\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('model_list', list)\n",
    "]):\n",
    "\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    rmse_model_resource_path = rmse_model.metadata[\"resourceName\"]\n",
    "    logging.info(f\"rmse_model_resource_path: {rmse_model_resource_path}\")\n",
    "    \n",
    "    mape_model_resource_path = mape_model.metadata[\"resourceName\"]\n",
    "    logging.info(f\"mape_model_resource_path: {mape_model_resource_path}\")\n",
    "    \n",
    "    model_list = [rmse_model_resource_path, mape_model_resource_path]\n",
    "    logging.info(f\"model_list: {model_list}\")\n",
    "    \n",
    "    test_list = [\n",
    "        {\n",
    "            \"model\": rmse_model_resource_path, \n",
    "            \"objective\": \"rmse\", \n",
    "        },\n",
    "        {\n",
    "            \"model\": mape_model_resource_path, \n",
    "            \"objective\": \"mape\", \n",
    "        }\n",
    "    ]\n",
    "    logging.info(f\"test_list: {test_list}\")\n",
    "\n",
    "    return (\n",
    "        model_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHYdmNCe1_4k"
   },
   "source": [
    "# Build, Compile, & Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLUmvGbJ5nnu"
   },
   "source": [
    "**Docs**\n",
    "\n",
    "`AutoMLForecastingTrainingJobRunOp()`\n",
    "\n",
    "* [Pipeline Component Docs](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.1.7/google_cloud_pipeline_components.aiplatform.html)\n",
    "* [Source Code](https://github.com/googleapis/python-aiplatform/blob/e7bf0d83d8bb0849a9bce886c958d13f5cbe5fab/google/cloud/aiplatform/training_jobs.py) search for `class AutoMLForecastingTrainingJob`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v19'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: m5-e2e-v19\n",
      "RUN_NAME: run-20230802-024010\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_PREFIX = 'm5-e2e'                     # custom identifier for organizing experiments\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{VERSION}'\n",
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline vars\n",
    "\n",
    "* [AutoML Forecast Job - src code](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/training_jobs.py#L1789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "KEVECqX8VRvV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT_PATH: gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root\n",
      "PIPELINE_NAME: train-m5-e2e-v19-v19\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = 'vertex-forecast-22'\n",
    "GCS_BUCKET_URI =f'gs://{BUCKET_NAME}'\n",
    "\n",
    "EXPERIMENT_GCS_DIR = f'gs://{BUCKET_NAME}/{EXPERIMENT_NAME}/{VERSION}'\n",
    "\n",
    "# Stores pipeline executions for each run\n",
    "PIPELINE_ROOT_PATH = f'{EXPERIMENT_GCS_DIR}/pipeline_root'\n",
    "print(f'PIPELINE_ROOT_PATH: {PIPELINE_ROOT_PATH}')\n",
    "\n",
    "PIPELINE_TAG = 'train'\n",
    "PIPELINE_NAME = f'{PIPELINE_TAG}-{EXPERIMENT_NAME}-{VERSION}'.replace('_', '-')\n",
    "print(f\"PIPELINE_NAME: {PIPELINE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-forecas-repo/04-pipeline-examples/end-to-end-pipeline\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0w-NnE-X2CNq"
   },
   "outputs": [],
   "source": [
    "from src import (\n",
    "    create_bq_dataset,create_input_table_specs, get_eval_dataset_path_uri,\n",
    "    create_combined_preds_table, create_forecast_input_table_specs, get_predict_table_path,\n",
    "    model_batch_prediction_job, create_combined_preds_forecast_table, get_model_path,\n",
    "    create_final_pred_table, args_generator_op_1\n",
    ")\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=PIPELINE_NAME\n",
    ")\n",
    "def pipeline(\n",
    "    vertex_project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    data_source_dataset: str,\n",
    "    eval_destination_dataset: str,\n",
    "    preprocess_dataset_us: str,\n",
    "    locations_table_uri: str,\n",
    "    forecast_locations_table_uri: str,\n",
    "    products_override: str,\n",
    "    products_table_uri: str,\n",
    "    forecast_products_table_uri: str,\n",
    "    activities_override: str,\n",
    "    activities_table_uri: str,\n",
    "    forecast_activities_table_uri: str,\n",
    "    # activities_expected_historical_last_date: str,\n",
    "    forecast_plan_table_uri: str,\n",
    "    time_granularity_unit: str,\n",
    "    time_granularity_quantity: int,\n",
    "    context_window: int,\n",
    "    forecast_horizon: int,\n",
    "    override: str,\n",
    "    gcs_root_dir: str,\n",
    "    batch_predict_explanation_data_sample_size: int,\n",
    "    target_column: str,\n",
    "    generate_explanation: bool,\n",
    "    model_evaluation_flag: str,\n",
    "    forecast_plan_flag: str,\n",
    "    combine_preds_flag: str,\n",
    "    rmse_eval_flag: str,\n",
    "    mape_eval_flag: str,\n",
    "    eval_bq_processed: str,\n",
    "    batch_predict_machine_type: str = \"n1-standard-4\",\n",
    "    batch_predict_instances_format: str = 'bigquery',\n",
    "    batch_predict_predictions_format: str = 'bigquery',\n",
    "    budget_milli_node_hours: int = 1600,\n",
    "):\n",
    "    \n",
    "    # create BQ dataset\n",
    "    create_train_dataset_op = (\n",
    "      create_bq_dataset.create_bq_dataset(\n",
    "          project=vertex_project,\n",
    "          vertex_dataset=data_source_dataset,\n",
    "          new_bq_dataset=eval_destination_dataset,\n",
    "          bq_location=location\n",
    "      )\n",
    "    )\n",
    "\n",
    "    # ======================================\n",
    "    # prep train jobs\n",
    "    # ======================================\n",
    "\n",
    "    create_input_table_specs_op = (\n",
    "        create_input_table_specs.create_input_table_specs(\n",
    "            products_table_uri=products_table_uri,\n",
    "            activities_table_uri=activities_table_uri,\n",
    "            locations_table_uri=locations_table_uri,\n",
    "            time_granularity_unit=time_granularity_unit,\n",
    "            time_granularity_quantity=time_granularity_quantity,\n",
    "            # train_data_bq_source=train_data_bq_source,\n",
    "        )\n",
    "        .after(create_train_dataset_op)\n",
    "    )\n",
    "\n",
    "    forecasting_validation_op = (\n",
    "        gcc_aip_forecasting.ForecastingValidationOp(\n",
    "            input_tables=str(create_input_table_specs_op.outputs['input_table_specs']),\n",
    "            validation_theme='FORECASTING_TRAINING',\n",
    "      )\n",
    "    )\n",
    "\n",
    "    forecasting_preprocessing_op = (\n",
    "      gcc_aip_forecasting.ForecastingPreprocessingOp(\n",
    "          project=vertex_project,\n",
    "          input_tables=str(create_input_table_specs_op.outputs['input_table_specs']),\n",
    "          preprocessing_bigquery_dataset=data_source_dataset,\n",
    "      )\n",
    "      .after(forecasting_validation_op)\n",
    "    )\n",
    "\n",
    "    prepare_data_for_train_op = (\n",
    "      gcc_aip_forecasting.ForecastingPrepareDataForTrainOp(\n",
    "          input_tables=(\n",
    "              str(create_input_table_specs_op.outputs['input_table_specs'])\n",
    "          ),\n",
    "          preprocess_metadata=(\n",
    "              forecasting_preprocessing_op.outputs['preprocess_metadata']\n",
    "          ),\n",
    "          model_feature_columns=(\n",
    "              str(create_input_table_specs_op.outputs['model_feature_columns'])\n",
    "          )\n",
    "      )\n",
    "    )\n",
    "\n",
    "    time_series_dataset_create_op = (\n",
    "      gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "          display_name=f'train_ds_full_m5_{VERSION}',\n",
    "          bq_source=prepare_data_for_train_op.outputs['preprocess_bq_uri'],\n",
    "          project=vertex_project,\n",
    "          location=location,\n",
    "      )\n",
    "    )\n",
    "\n",
    "    mape_model_version = f'{VERSION}-seq2seq-mape'\n",
    "    rmse_model_version = f'{VERSION}-fast_nn-rmse' \n",
    "\n",
    "    get_eval_dataset_path_uri_op = (\n",
    "      get_eval_dataset_path_uri.get_eval_dataset_path_uri(\n",
    "          project=vertex_project,\n",
    "          eval_bq_dataset=create_train_dataset_op.outputs['bq_dataset_name'],\n",
    "          model_1_table=mape_model_version,\n",
    "          model_2_table=rmse_model_version,\n",
    "      )\n",
    "    )\n",
    "\n",
    "    # ======================================\n",
    "    # train jobs\n",
    "    # ======================================\n",
    "\n",
    "    # mape_model_op = gcc_aip_forecasting.ForecastingTrainingWithExperimentsOp(\n",
    "    mape_model_op = (\n",
    "      gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "          display_name=f'train-{mape_model_version}',\n",
    "          model_display_name=mape_model_version,\n",
    "          model_labels={'model_override' : 'se2seq'}, # model_override : se2seq-hier, tft\n",
    "          # model_labels={'model_type' : 'l2l'},\n",
    "          dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "          context_window=context_window,\n",
    "          forecast_horizon=forecast_horizon,\n",
    "          budget_milli_node_hours=budget_milli_node_hours,\n",
    "          project=vertex_project,\n",
    "          location=location,\n",
    "          export_evaluated_data_items=True,\n",
    "          export_evaluated_data_items_bigquery_destination_uri=get_eval_dataset_path_uri_op.outputs['model_1_bigquery_table_uri'], # must be format:``bq://<project_id>:<dataset_id>:<table>``\n",
    "          export_evaluated_data_items_override_destination=True,\n",
    "          target_column=prepare_data_for_train_op.outputs['target_column'],\n",
    "          time_column=prepare_data_for_train_op.outputs['time_column'],\n",
    "          time_series_identifier_column=prepare_data_for_train_op.outputs['time_series_identifier_column'],\n",
    "          time_series_attribute_columns=prepare_data_for_train_op.outputs['time_series_attribute_columns'],\n",
    "          unavailable_at_forecast_columns=prepare_data_for_train_op.outputs['unavailable_at_forecast_columns'],\n",
    "          available_at_forecast_columns=prepare_data_for_train_op.outputs['available_at_forecast_columns'],\n",
    "          data_granularity_unit=prepare_data_for_train_op.outputs['data_granularity_unit'],\n",
    "          data_granularity_count=prepare_data_for_train_op.outputs['data_granularity_count'],\n",
    "          predefined_split_column_name= '', # prepare_data_for_train_op.outputs['predefined_split_column'],\n",
    "          column_transformations=prepare_data_for_train_op.outputs['column_transformations'],\n",
    "          weight_column=prepare_data_for_train_op.outputs['weight_column'],\n",
    "          optimization_objective='minimize-mape',\n",
    "          # hierarchy_group_columns='dept_id, cat_id'\n",
    "          # hierarchy_group_total_weight: Optional[float] = None,\n",
    "          # hierarchy_temporal_total_weight: Optional[float] = None,\n",
    "          # hierarchy_group_temporal_total_weight: Optional[float] = None,\n",
    "          additional_experiments={\n",
    "              'forecasting_model_type_override': 'seq2seq', # fast_nn | seq2seq\n",
    "              # 'forecasting_hierarchical_group_column_names':'dept_id, cat_id'\n",
    "          },\n",
    "      )\n",
    "    )\n",
    "\n",
    "    # rmse_model_op = gcc_aip_forecasting.ForecastingTrainingWithExperimentsOp(\n",
    "    rmse_model_op = (\n",
    "      gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "          display_name=f'train-{rmse_model_version}',\n",
    "          model_display_name=rmse_model_version,\n",
    "          model_labels={'model_override' : 'fast_nn'}, # model_override : se2seq-hier, tft\n",
    "          # model_labels={'model_type' : 'l2l'},\n",
    "          dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "          context_window=context_window,\n",
    "          forecast_horizon=forecast_horizon,\n",
    "          budget_milli_node_hours=budget_milli_node_hours,\n",
    "          project=vertex_project,\n",
    "          location=location,\n",
    "          export_evaluated_data_items=True,\n",
    "          export_evaluated_data_items_bigquery_destination_uri=get_eval_dataset_path_uri_op.outputs['model_2_bigquery_table_uri'],\n",
    "          export_evaluated_data_items_override_destination=True,\n",
    "          target_column=prepare_data_for_train_op.outputs['target_column'],\n",
    "          time_column=prepare_data_for_train_op.outputs['time_column'],\n",
    "          time_series_identifier_column=prepare_data_for_train_op.outputs['time_series_identifier_column'],\n",
    "          time_series_attribute_columns=prepare_data_for_train_op.outputs['time_series_attribute_columns'],\n",
    "          unavailable_at_forecast_columns=prepare_data_for_train_op.outputs['unavailable_at_forecast_columns'],\n",
    "          available_at_forecast_columns=prepare_data_for_train_op.outputs['available_at_forecast_columns'],\n",
    "          data_granularity_unit=prepare_data_for_train_op.outputs['data_granularity_unit'],\n",
    "          data_granularity_count=prepare_data_for_train_op.outputs['data_granularity_count'],\n",
    "          predefined_split_column_name= '', # prepare_data_for_train_op.outputs['predefined_split_column'],\n",
    "          column_transformations=prepare_data_for_train_op.outputs['column_transformations'],\n",
    "          weight_column=prepare_data_for_train_op.outputs['weight_column'],\n",
    "          # hierarchy_group_columns='dept_id, cat_id'\n",
    "          # hierarchy_group_total_weight: Optional[float] = None,\n",
    "          # hierarchy_temporal_total_weight: Optional[float] = None,\n",
    "          # hierarchy_group_temporal_total_weight: Optional[float] = None,\n",
    "          optimization_objective='minimize-rmse',\n",
    "          additional_experiments={\n",
    "              'forecasting_model_type_override': 'fast_nn', # fast_nn | seq2seq\n",
    "              # 'forecasting_hierarchical_group_column_names':'dept_id, cat_id'\n",
    "          },\n",
    "      )\n",
    "    )\n",
    "    \n",
    "    # ======================================\n",
    "    # combine model predictions\n",
    "    # ======================================\n",
    "    with kfp.v2.dsl.Condition(combine_preds_flag == \"True\", name=\"combine-preds\"):\n",
    "        \n",
    "        create_combined_preds_table_op = (\n",
    "          create_combined_preds_table.create_combined_preds_table(\n",
    "              project=vertex_project,\n",
    "              dataset=get_eval_dataset_path_uri_op.outputs['eval_bq_dataset'],\n",
    "              bq_location=location,\n",
    "              model_1_eval_table_uri=get_eval_dataset_path_uri_op.outputs['model_1_bigquery_table_uri'],\n",
    "              model_2_eval_table_uri=get_eval_dataset_path_uri_op.outputs['model_2_bigquery_table_uri'],\n",
    "              model_1_path=mape_model_op.outputs['model'],\n",
    "              model_2_path=rmse_model_op.outputs['model'],\n",
    "          )\n",
    "        )\n",
    "\n",
    "        create_final_preds_table_op = (\n",
    "          create_final_pred_table.create_final_pred_table(\n",
    "              project=vertex_project,\n",
    "              dataset=create_combined_preds_table_op.outputs['eval_bq_dataset'],\n",
    "              bq_location=location,\n",
    "              combined_preds_table_uri=create_combined_preds_table_op.outputs['combined_preds_table_uri'],\n",
    "              override=override,\n",
    "          )\n",
    "        )\n",
    "    \n",
    "    # ======================================\n",
    "    # Model Eval Workflow\n",
    "    # ======================================\n",
    "    with kfp.v2.dsl.Condition(model_evaluation_flag == \"True\", name=\"evaluate-models\"):\n",
    "        \n",
    "        from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp\n",
    "        from google_cloud_pipeline_components.experimental.evaluation import (\n",
    "            EvaluationDataSamplerOp, GetVertexModelOp,\n",
    "            ModelEvaluationForecastingOp, ModelEvaluationFeatureAttributionOp,\n",
    "            ModelImportEvaluationOp, TargetFieldDataRemoverOp)\n",
    "\n",
    "        # Run Data-sampling task\n",
    "        data_sampler_task = (\n",
    "            EvaluationDataSamplerOp(\n",
    "                project=vertex_project,\n",
    "                location=location,\n",
    "                root_dir=gcs_root_dir,\n",
    "                bigquery_source_uri=eval_bq_processed, #activities_table_uri,\n",
    "                instances_format=batch_predict_instances_format,\n",
    "                sample_size=batch_predict_explanation_data_sample_size,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Run Target field-removal task\n",
    "        target_remover_task = (\n",
    "            TargetFieldDataRemoverOp(\n",
    "                project=vertex_project,\n",
    "                location=location,\n",
    "                root_dir=gcs_root_dir,\n",
    "                bigquery_source_uri=data_sampler_task.outputs[\"bigquery_output_table\"],\n",
    "                instances_format=batch_predict_instances_format,\n",
    "                target_field_name=target_column,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        args_generator = (\n",
    "            args_generator_op_1.args_generator_op_1(\n",
    "                project=vertex_project,\n",
    "                location=location,\n",
    "                rmse_model=rmse_model_op.outputs['model'],\n",
    "                rmse_display_name=rmse_model_version,\n",
    "                mape_model=mape_model_op.outputs['model'],\n",
    "                mape_display_name=mape_model_version,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # test_list = [\n",
    "        #     {\n",
    "        #         \"model\": rmse_model_op.outputs['model'], \n",
    "        #         \"objective\": \"rmse\", \n",
    "        #         \"display_name\": rmse_model_version,\n",
    "        #     },\n",
    "        #     {\n",
    "        #         \"model\": mape_model_op.outputs['model'],\n",
    "        #         \"objective\": \"mape\", \n",
    "        #         \"display_name\": mape_model_version,\n",
    "        #     }\n",
    "        # ]\n",
    "        # Evaluation for-loop\n",
    "        # with kfp.v2.dsl.ParallelFor(args_generator.output) as item:\n",
    "            # item.set_display_name(str(args_generator.output))\n",
    "            \n",
    "        with kfp.v2.dsl.Condition(rmse_eval_flag == \"True\", name=\"eval-rmse\"):\n",
    "            \n",
    "            # here\n",
    "            rmse_create_model_eval_ds_op = (\n",
    "                create_bq_dataset.create_bq_dataset(\n",
    "                    project=vertex_project,\n",
    "                    vertex_dataset=data_source_dataset,\n",
    "                    new_bq_dataset=f'{eval_destination_dataset}_rmse',\n",
    "                    bq_location=location\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Run Batch Explanations\n",
    "            rmse_batch_explain_task = (\n",
    "                ModelBatchPredictOp(\n",
    "                    project=vertex_project,\n",
    "                    location=location,\n",
    "                    model=rmse_model_op.outputs['model'],\n",
    "                    job_display_name=f\"bpj-eval-rmse\",\n",
    "                    bigquery_source_input_uri=target_remover_task.outputs[\"bigquery_output_table\"],\n",
    "                    instances_format=batch_predict_instances_format,\n",
    "                    predictions_format=batch_predict_predictions_format,\n",
    "                    bigquery_destination_output_uri=f'bq://{vertex_project}', \n",
    "                    machine_type=batch_predict_machine_type,\n",
    "                    # Set the explanation parameters\n",
    "                    generate_explanation=generate_explanation,\n",
    "                    # explanation_parameters=batch_predict_explanation_parameters,\n",
    "                    # explanation_metadata=batch_predict_explanation_metadata,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Run evaluation based on prediction type and feature attribution component.\n",
    "            # After, import the model evaluations to the Vertex model.\n",
    "            rmse_eval_task = (\n",
    "                ModelEvaluationForecastingOp(\n",
    "                    project=vertex_project,\n",
    "                    location=location,\n",
    "                    root_dir=f\"{gcs_root_dir}/rmse-staging\",\n",
    "                    target_field_name=target_column,\n",
    "                    predictions_bigquery_source=rmse_batch_explain_task.outputs[\"bigquery_output_table\"],\n",
    "                    predictions_format=batch_predict_predictions_format,\n",
    "                    model=rmse_model_op.outputs['model'],\n",
    "                    # prediction_score_column=\"prediction.scores\",\n",
    "                    forecasting_type=\"point\",\n",
    "                    ground_truth_bigquery_source=data_sampler_task.outputs[\"bigquery_output_table\"],\n",
    "                    ground_truth_format=batch_predict_instances_format,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Import the evaluation results to the model resource\n",
    "            rmse_model_import_task = (\n",
    "                ModelImportEvaluationOp(\n",
    "                    problem_type=\"forecasting\",\n",
    "                    forecasting_metrics=rmse_eval_task.outputs[\"evaluation_metrics\"],\n",
    "                    # feature_attributions=feature_attribution_task.outputs[\"feature_attributions\"],\n",
    "                    model=rmse_model_op.outputs['model'],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # here\n",
    "\n",
    "        with kfp.v2.dsl.Condition(mape_eval_flag == \"True\", name=\"eval-mape\"):\n",
    "            \n",
    "            # here\n",
    "            # get_model_task = GetVertexModelOp(model_resource_name=item.model)\n",
    "\n",
    "            mape_create_model_eval_ds_op = (\n",
    "                create_bq_dataset.create_bq_dataset(\n",
    "                    project=vertex_project,\n",
    "                    vertex_dataset=data_source_dataset,\n",
    "                    new_bq_dataset=f'{eval_destination_dataset}_mape',\n",
    "                    bq_location=location\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Run Batch Explanations\n",
    "            mape_batch_explain_task = (\n",
    "                ModelBatchPredictOp(\n",
    "                    project=vertex_project,\n",
    "                    location=location,\n",
    "                    model=mape_model_op.outputs['model'],\n",
    "                    job_display_name=f\"bpj-eval-mape\",\n",
    "                    bigquery_source_input_uri=target_remover_task.outputs[\"bigquery_output_table\"],\n",
    "                    instances_format=batch_predict_instances_format,\n",
    "                    predictions_format=batch_predict_predictions_format,\n",
    "                    bigquery_destination_output_uri=f'bq://{vertex_project}', # create_model_eval_ds_op.outputs['bq_dataset_uri'], # f\"bq://{PROJECT_ID}.{table_ref.dataset_id}\", # create_model_eval_ds_op.outputs['bq_dataset_uri'],\n",
    "                    machine_type=batch_predict_machine_type,\n",
    "                    # Set the explanation parameters\n",
    "                    generate_explanation=generate_explanation,\n",
    "                    # explanation_parameters=batch_predict_explanation_parameters,\n",
    "                    # explanation_metadata=batch_predict_explanation_metadata,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Run evaluation based on prediction type and feature attribution component.\n",
    "            # After, import the model evaluations to the Vertex model.\n",
    "            mape_eval_task = (\n",
    "                ModelEvaluationForecastingOp(\n",
    "                    project=vertex_project,\n",
    "                    location=location,\n",
    "                    root_dir=f\"{gcs_root_dir}/mape-staging\",\n",
    "                    target_field_name=target_column,\n",
    "                    predictions_bigquery_source=mape_batch_explain_task.outputs[\"bigquery_output_table\"],\n",
    "                    predictions_format='bigquery', # batch_predict_predictions_format,\n",
    "                    # predictions_gcs_source = mape_batch_explain_task.outputs['gcs_output_directory'],\n",
    "                    # predictions_format='csv',\n",
    "                    # predictions_format=batch_predict_predictions_format,\n",
    "                    model=mape_model_op.outputs['model'],\n",
    "                    # prediction_score_column=\"prediction.scores\",\n",
    "                    forecasting_type=\"point\",\n",
    "                    ground_truth_bigquery_source=data_sampler_task.outputs[\"bigquery_output_table\"],\n",
    "                    ground_truth_format=batch_predict_instances_format,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Import the evaluation results to the model resource\n",
    "            mape_model_import_task = (\n",
    "                ModelImportEvaluationOp(\n",
    "                    problem_type=\"forecasting\",\n",
    "                    forecasting_metrics=mape_eval_task.outputs[\"evaluation_metrics\"],\n",
    "                    # feature_attributions=feature_attribution_task.outputs[\"feature_attributions\"],\n",
    "                    model=mape_model_op.outputs['model'],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        \n",
    "    # ======================================\n",
    "    # Plan Forecast Workflow\n",
    "    # ======================================\n",
    "    with kfp.v2.dsl.Condition(forecast_plan_flag == \"True\", name=\"forecast-plan\"):\n",
    "\n",
    "        create_forecast_dataset_op = (\n",
    "          create_bq_dataset.create_bq_dataset(\n",
    "              project=vertex_project,\n",
    "              vertex_dataset=data_source_dataset,\n",
    "              new_bq_dataset=preprocess_dataset_us,\n",
    "              bq_location=location\n",
    "          )\n",
    "        )\n",
    "\n",
    "        create_forecast_input_table_specs_op = (\n",
    "          create_forecast_input_table_specs.create_forecast_input_table_specs(\n",
    "              project=vertex_project,\n",
    "              forecast_products_table_uri=forecast_products_table_uri,\n",
    "              forecast_activities_table_uri=forecast_activities_table_uri,\n",
    "              forecast_locations_table_uri=forecast_locations_table_uri,\n",
    "              forecast_plan_table_uri=forecast_plan_table_uri,\n",
    "              time_granularity_unit=time_granularity_unit,\n",
    "              time_granularity_quantity=time_granularity_quantity,\n",
    "          )\n",
    "          .after(create_forecast_dataset_op)\n",
    "        )\n",
    "\n",
    "        forecast_validation_op = (\n",
    "          gcc_aip_forecasting.ForecastingValidationOp(\n",
    "              input_tables=str(create_forecast_input_table_specs_op.outputs['forecast_input_table_specs']),\n",
    "              validation_theme='FORECASTING_PREDICTION',\n",
    "          )\n",
    "        )\n",
    "\n",
    "        forecast_preprocess_op = (\n",
    "          gcc_aip_forecasting.ForecastingPreprocessingOp(\n",
    "              project=vertex_project,\n",
    "              input_tables=str(create_forecast_input_table_specs_op.outputs['forecast_input_table_specs']),\n",
    "              preprocessing_bigquery_dataset=create_forecast_dataset_op.outputs['bq_dataset_name'], # TODO: Table needs to be in 'US'\n",
    "          )\n",
    "          .after(forecast_validation_op)\n",
    "        )\n",
    "\n",
    "        get_predict_table_path_op = (\n",
    "          get_predict_table_path.get_predict_table_path(\n",
    "              predict_processed_table=str(forecast_preprocess_op.outputs['preprocess_metadata']),\n",
    "          )\n",
    "        )\n",
    "\n",
    "\n",
    "        model_1_predict_job_op = (\n",
    "          model_batch_prediction_job.model_batch_prediction_job(\n",
    "              project=vertex_project,\n",
    "              location=location,\n",
    "              eval_bq_dataset=eval_destination_dataset,\n",
    "              bigquery_source=get_predict_table_path_op.outputs['preprocess_bq_uri'],\n",
    "              model_name=mape_model_version,\n",
    "              model_1_path=mape_model_op.outputs['model'],\n",
    "          )\n",
    "        )\n",
    "\n",
    "        model_2_predict_job_op = (\n",
    "          model_batch_prediction_job.model_batch_prediction_job(\n",
    "              project=vertex_project,\n",
    "              location=location,\n",
    "              eval_bq_dataset=eval_destination_dataset,\n",
    "              bigquery_source=get_predict_table_path_op.outputs['preprocess_bq_uri'],\n",
    "              model_name=rmse_model_version,\n",
    "              model_1_path=rmse_model_op.outputs['model'],\n",
    "          )\n",
    "        )\n",
    "\n",
    "        create_combined_preds_forecast_table_op = (\n",
    "          create_combined_preds_forecast_table.create_combined_preds_forecast_table(\n",
    "              project=vertex_project,\n",
    "              dataset=eval_destination_dataset,\n",
    "              model_1_pred_table_uri=model_1_predict_job_op.outputs['batch_predict_output_bq_uri'],\n",
    "              model_2_pred_table_uri=model_2_predict_job_op.outputs['batch_predict_output_bq_uri'],\n",
    "              override=override,\n",
    "          )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAS-bBfhj_q9"
   },
   "source": [
    "Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8baYwrspkB3U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_JSON_SPEC_LOCAL = \"custom_pipeline_spec.json\"\n",
    "\n",
    "! rm -f $PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINES_FILEPATH: gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipeline_spec.json\n",
      "Copying file://custom_pipeline_spec.json [Content-Type=application/json]...\n",
      "/ [1 files][205.9 KiB/205.9 KiB]                                                \n",
      "Operation completed over 1 objects/205.9 KiB.                                    \n",
      "Copying file://src/args_generator_op_1.py [Content-Type=text/x-python]...\n",
      "Copying file://src/create_bq_dataset.py [Content-Type=text/x-python]...         \n",
      "Copying file://src/create_combined_preds_forecast_table.py [Content-Type=text/x-python]...\n",
      "Copying file://src/create_combined_preds_table.py [Content-Type=text/x-python]...\n",
      "Copying file://src/create_final_pred_table.py [Content-Type=text/x-python]...   \n",
      "Copying file://src/create_forecast_input_table_specs.py [Content-Type=text/x-python]...\n",
      "Copying file://src/create_input_table_specs.py [Content-Type=text/x-python]...  \n",
      "Copying file://src/get_eval_dataset_path_uri.py [Content-Type=text/x-python]... \n",
      "Copying file://src/get_model_path.py [Content-Type=text/x-python]...            \n",
      "Copying file://src/get_predict_table_path.py [Content-Type=text/x-python]...    \n",
      "Copying file://src/model_batch_prediction_job.py [Content-Type=text/x-python]...\n",
      "/ [11/11 files][ 17.4 KiB/ 17.4 KiB] 100% Done                                  \n",
      "Operation completed over 11 objects/17.4 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT_PATH}/pipeline_spec.json'\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "# copy pipeline spec to gcs path\n",
    "!gsutil cp $PIPELINE_JSON_SPEC_LOCAL $PIPELINES_FILEPATH\n",
    "\n",
    "# copy src code to gcs path\n",
    "! gsutil -m cp -r $REPO_DOCKER_PATH_PREFIX/*.py $PIPELINE_ROOT_PATH/pipe-src\n",
    "\n",
    "# ! gsutil ls $PIPELINE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/args_generator_op_1.py\n",
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/create_bq_dataset.py\n",
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/create_combined_preds_forecast_table.py\n",
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/create_combined_preds_table.py\n",
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/create_final_pred_table.py\n",
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/create_forecast_input_table_specs.py\n",
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/create_input_table_specs.py\n",
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/get_eval_dataset_path_uri.py\n",
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/get_model_path.py\n",
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/get_predict_table_path.py\n",
      "gs://vertex-forecast-22/m5-e2e-v19/v19/pipeline_root/pipe-src/model_batch_prediction_job.py\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls $PIPELINE_ROOT_PATH/pipe-src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxzuIrmmkS6S"
   },
   "source": [
    "Specify pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3kHYmxRXfUzd",
    "outputId": "1f09a735-b6f7-4193-e633-55fda0e50c9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v19'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Ba4Ch5_FkWtr"
   },
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# project vars (if not set)\n",
    "# ==========================\n",
    "assert LOCATION, 'the value for this variable must be set'\n",
    "assert PROJECT_ID, 'the value for this variable must be set'\n",
    "\n",
    "# ==========================\n",
    "# BigQuery params\n",
    "# ==========================\n",
    "OVERRIDE = 'True'                                                  # replace BQ eval tables?\n",
    "\n",
    "# BQ dataset for source data source\n",
    "DATA_SOURCE_DATASET = 'm5_us'\n",
    "\n",
    "# BQ dataset for eval tables\n",
    "EVAL_DESTINATION_DATASET = f'{VERSION}_eval'\n",
    "PREPROCESS_DATASET_US = 'm5_preprocessing_us'                      # TODO: create this in BQ region \"US\"; dont need to create for different pipeline runs\n",
    "EVAL_BQ_PROCESSED = 'bq://hybrid-vertex.m5_us.preprocess_2023_02_01T05_13_15_432Z'\n",
    "\n",
    "# training BQ tables\n",
    "PRODUCTS_TABLE = 'hybrid-vertex.m5_us.products_2'                  # TODO:\n",
    "LOCATIONS_TABLE = 'hybrid-vertex.m5_us.locations_2'                # TODO:\n",
    "# ACTIVITIES_TABLE = 'hybrid-vertex.m5_us.activity_all_jt_2'\n",
    "ACTIVITIES_TABLE = 'hybrid-vertex.m5_us.activity_small20k'         # TODO:\n",
    "# BATCH_PREDICT_TABLE = 'hybrid-vertex.m5_comp.test_split'         # TODO:\n",
    "\n",
    "assert PRODUCTS_TABLE, 'the value for this variable must be set'\n",
    "assert LOCATIONS_TABLE, 'the value for this variable must be set'\n",
    "assert ACTIVITIES_TABLE, 'the value for this variable must be set'\n",
    "# assert BATCH_PREDICT_TABLE, 'the value for this variable must be set'\n",
    "\n",
    "# forecast BQ tables\n",
    "FORECAST_PRODUCTS_TABLE = 'hybrid-vertex.m5_us.products_2'\n",
    "FORECAST_PLAN_TABLE = 'hybrid-vertex.m5_us.plan_table_all_jt_2'\n",
    "FORECAST_ACTIVITIES_TABLE = 'hybrid-vertex.m5_us.activity_all_jt_2'\n",
    "FORECAST_LOCATIONS_TABLE = 'hybrid-vertex.m5_us.locations_2'\n",
    "\n",
    "assert FORECAST_PRODUCTS_TABLE, 'the value for this variable must be set'\n",
    "assert FORECAST_PLAN_TABLE, 'the value for this variable must be set'\n",
    "assert FORECAST_ACTIVITIES_TABLE, 'the value for this variable must be set'\n",
    "assert FORECAST_LOCATIONS_TABLE, 'the value for this variable must be set'\n",
    "\n",
    "# ==========================\n",
    "# Vertex Forecast Config\n",
    "# ==========================\n",
    "HISTORY_WINDOW_n = 7                                             # TODO: {type: 'integer'} # context_window\n",
    "FORECAST_HORIZON = 7                                             # TODO: {type: 'integer'}\n",
    "BUDGET_MILLI_NODE_HOURS = 1600  # 16000\n",
    "\n",
    "# Max date in the activities/sales table\n",
    "# ACTIVITIES_EXPECTED_HISTORICAL_LAST_DATE = 'xxxx' #  {type: 'string'}\n",
    "# PREDICTED_ON_DATETIME = 'xxx' # not used for POC, but likely needed for production\n",
    "\n",
    "assert HISTORY_WINDOW_n, 'the value for this variable must be set'\n",
    "assert FORECAST_HORIZON, 'the value for this variable must be set'\n",
    "# assert ACTIVITIES_EXPECTED_HISTORICAL_LAST_DATE, 'the value for this variable must be set'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp1PeELskIkD"
   },
   "source": [
    "Submit pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "uZj3xDoTZObz"
   },
   "outputs": [],
   "source": [
    "overwrite = True # True creates new pipeline instance for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "MR0Zr-a1kFrJ",
    "outputId": "1a3646b7-fd68-4130-91c7-8b45883856ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-m5-e2e-v19-v19-20230802024031?project=934903580331\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-e2e-v19-v19-20230802024031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "vpc_network_name = 'ucaip-haystack-vpc-network'\n",
    "# SERVICE_ACCOUNT = '934903580331-compute@developer.gserviceaccount.com'\n",
    "SERVICE_ACCOUNT = 'notebooksa@hybrid-vertex.iam.gserviceaccount.com'\n",
    "\n",
    "job = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINES_FILEPATH,\n",
    "    pipeline_root=f'{PIPELINE_ROOT_PATH}',\n",
    "    failure_policy='fast', # slow | fast\n",
    "    # enable_caching=False,\n",
    "    parameter_values={\n",
    "        'vertex_project': PROJECT_ID,\n",
    "        'location': LOCATION,\n",
    "        'version': VERSION,\n",
    "        'data_source_dataset': DATA_SOURCE_DATASET,\n",
    "        'eval_destination_dataset': EVAL_DESTINATION_DATASET,\n",
    "        'preprocess_dataset_us': PREPROCESS_DATASET_US,\n",
    "        'products_override': 'False',\n",
    "        'products_table_uri': f'bq://{PRODUCTS_TABLE}',\n",
    "        'forecast_products_table_uri': f'bq://{FORECAST_PRODUCTS_TABLE}',\n",
    "        'activities_override': 'False',\n",
    "        'activities_table_uri': f'bq://{ACTIVITIES_TABLE}',\n",
    "        'forecast_activities_table_uri': f'bq://{FORECAST_ACTIVITIES_TABLE}',\n",
    "        # 'activities_expected_historical_last_date': ACTIVITIES_EXPECTED_HISTORICAL_LAST_DATE,\n",
    "        # 'predicted_on_date': PREDICTED_ON_DATETIME,\n",
    "        'forecast_plan_table_uri': f'bq://{FORECAST_PLAN_TABLE}',\n",
    "        'locations_table_uri': f'bq://{LOCATIONS_TABLE}',\n",
    "        'forecast_locations_table_uri': f'bq://{FORECAST_LOCATIONS_TABLE}',\n",
    "        'time_granularity_unit': 'DAY',\n",
    "        'time_granularity_quantity': 1,\n",
    "        'context_window': HISTORY_WINDOW_n,\n",
    "        'forecast_horizon': FORECAST_HORIZON,\n",
    "        'override': OVERRIDE,\n",
    "        'budget_milli_node_hours': BUDGET_MILLI_NODE_HOURS,\n",
    "        'gcs_root_dir': EXPERIMENT_GCS_DIR,\n",
    "        \"batch_predict_explanation_data_sample_size\": 1000,\n",
    "        'target_column': \"gross_quantity\",\n",
    "        \"generate_explanation\": False, # bool\n",
    "        \"batch_predict_machine_type\": \"n1-standard-4\",\n",
    "        \"batch_predict_instances_format\": 'bigquery',\n",
    "        \"batch_predict_predictions_format\": 'bigquery',\n",
    "        \"combine_preds_flag\": \"True\",\n",
    "        \"model_evaluation_flag\": \"True\",\n",
    "        \"forecast_plan_flag\": \"False\",\n",
    "        \"rmse_eval_flag\": \"True\",\n",
    "        \"mape_eval_flag\": \"True\",\n",
    "        \"eval_bq_processed\": EVAL_BQ_PROCESSED,\n",
    "    },\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=False,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    # network=f'projects/{PROJECT_NUM}/global/networks/{vpc_network_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k00P4rBLmR-p"
   },
   "source": [
    "# TODO\n",
    "\n",
    "> code snippets for model eval enhancements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_list = [\n",
    "#     {\n",
    "#         \"model\": \"model_1\", \n",
    "#         \"objective\": \"rmse\", \n",
    "#         \"display_name\": \"model_1_display_name\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"model\": \"model_2\", \n",
    "#         \"objective\": \"mape\", \n",
    "#         \"display_name\": \"model_2_display_name\"\n",
    "#     }\n",
    "# ]\n",
    "# test_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_list_printed = [\n",
    "#     {\n",
    "#         'model': 'projects/934903580331/locations/us-central1/models/5372954884150657024', \n",
    "#         'objective': 'rmse', \n",
    "#         'display_name': 'v11-seq2seq-rmse'\n",
    "#     }, \n",
    "#     {\n",
    "#         'model': 'projects/934903580331/locations/us-central1/models/4143472185878511616', \n",
    "#         'objective': 'mape', \n",
    "#         'display_name': 'v11-seq2seq-mape'\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Eval XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # ========================================================================\n",
    "#         # Conditional: generate XAI == True\n",
    "#         # ========================================================================\n",
    "#         with kfp.v2.dsl.Condition(generate_explanation == True, name=\"generate XAI\"):\n",
    "\n",
    "#             # Get Feature Attributions\n",
    "#             feature_attribution_task = (\n",
    "#                 ModelEvaluationFeatureAttributionOp(\n",
    "#                     project=vertex_project,\n",
    "#                     location=location,\n",
    "#                     root_dir=f\"{gcs_root_dir}/{item.objective}-staging\",\n",
    "#                     predictions_format=batch_predict_predictions_format,\n",
    "#                     predictions_bigquery_source=batch_explain_task.outputs[\"bigquery_output_table\"],\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#             # Import the evaluation results to the model resource\n",
    "#             xai_model_import_task = (\n",
    "#                 ModelImportEvaluationOp(\n",
    "#                     problem_type=\"forecasting\",\n",
    "#                     forecasting_metrics=eval_task.outputs[\"evaluation_metrics\"],\n",
    "#                     feature_attributions=feature_attribution_task.outputs[\"feature_attributions\"],\n",
    "#                     model=get_model_task.outputs[\"model\"],\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#         # # ========================================================================\n",
    "#         # # Conditional: generate XAI == False \n",
    "#         # # ========================================================================\n",
    "#         with kfp.v2.dsl.Condition(generate_explanation == False, name=\"dont generate XAI\"):\n",
    "\n",
    "#             # Import the evaluation results to the model resource\n",
    "#             model_import_task = (\n",
    "#                 ModelImportEvaluationOp(\n",
    "#                     problem_type=\"forecasting\",\n",
    "#                     forecasting_metrics=eval_task.outputs[\"evaluation_metrics\"],\n",
    "#                     # feature_attributions=feature_attribution_task.outputs[\"feature_attributions\"],\n",
    "#                     model=get_model_task.outputs[\"model\"],\n",
    "#                 )\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # # get model paths\n",
    "        # get_mape_model_path = (\n",
    "        #     get_model_path.get_model_path(\n",
    "        #         project=vertex_project,\n",
    "        #         location=location,\n",
    "        #         model=mape_model_op.outputs['model'],\n",
    "        #     )\n",
    "        # )\n",
    "        # get_rmse_model_path = (\n",
    "        #     get_model_path.get_model_path(\n",
    "        #         project=vertex_project,\n",
    "        #         location=location,\n",
    "        #         model=rmse_model_op.outputs['model'],\n",
    "        #     )\n",
    "        # )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
