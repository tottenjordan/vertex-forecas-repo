{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c741d35b-264d-46ef-8d0a-b2d4a9863564",
   "metadata": {},
   "source": [
    "# Vertex Forecast - workflow orchestration with Vertex Pipelines\n",
    "\n",
    "This notebook demonstrate the basic building blocks of using Vertex Pipelines to orchestrate Vertex Forecast modeling workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01ffbbb-d388-4d3a-9bdd-2f09c6f8331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "LOCATION: us-central1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "LOCATION = 'us-central1'\n",
    "BQ_LOCATION='US'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"LOCATION: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "784eb769-6ffd-4bd7-b77b-602e469c10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# google cloud\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental import forecasting as gcc_aip_forecasting\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# kfp\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, \n",
    "    Input, InputPath, \n",
    "    Model, Output,\n",
    "    OutputPath, component, \n",
    "    Metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d356e8fa-d8d5-44b6-8102-5724063f4fda",
   "metadata": {},
   "source": [
    "### check package versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0797f330-4ccf-4159-b04c-3db52d2475a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp version: 1.8.19\n",
      "google_cloud_pipeline_components version: 1.0.41\n",
      "vertex_ai SDK version: 1.22.1\n",
      "bigquery SDK version: 3.7.0\n"
     ]
    }
   ],
   "source": [
    "print(f'kfp version: {kfp.__version__}')\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "print(f'vertex_ai SDK version: {vertex_ai.__version__}')\n",
    "print(f'bigquery SDK version: {bigquery.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee3d806-c64c-4e54-87bb-f114d7f9b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID, \n",
    "    location=BQ_LOCATION\n",
    ")\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90916b7-7804-4a11-810c-e10b4339d9c2",
   "metadata": {},
   "source": [
    "## custom pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f82b8-10d5-49d9-a467-7de33205d962",
   "metadata": {},
   "source": [
    "### create BigQuery dataset\n",
    "\n",
    "> place to put eval + forecast output\n",
    "\n",
    "**TODO**\n",
    "* explain this custom compnent e.g., NamedTuple output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5af5b62-4b01-401e-a645-74846c1e45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-bigquery==3.7.0'],\n",
    ")\n",
    "def create_bq_dataset(\n",
    "    project: str,\n",
    "    # vertex_dataset: str,\n",
    "    new_bq_dataset: str,\n",
    "    bq_location: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('bq_dataset_name', str),\n",
    "    ('bq_dataset_uri', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    bq_client = bigquery.Client(project=project, location='US') # bq_location)\n",
    "    (\n",
    "      bq_client.query(f'CREATE SCHEMA IF NOT EXISTS `{project}.{new_bq_dataset}`')\n",
    "      .result()\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        f'{new_bq_dataset}',\n",
    "        f'bq://{project}:{new_bq_dataset}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4eab5-6625-40cd-a0e9-30c5552fb9a8",
   "metadata": {},
   "source": [
    "### create input table specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d3ff804-b12e-42a5-a560-f477cd603399",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.22.1'\n",
    "    ],\n",
    ")\n",
    "def create_input_table_specs(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    column_spec_gcs_bucket_name: str,\n",
    "    column_spec_gcs_blob_path: str,\n",
    "    bq_source_train_uri: str,\n",
    "    data_granularity_unit: str,\n",
    "    data_granularity_count: int,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('input_table_specs', str),\n",
    "    ('model_feature_columns', str),\n",
    "    # ('column_specs', str),\n",
    "    # ('unavailable_at_forecast_columns', str),\n",
    "    # ('available_at_forecast_columns', str),\n",
    "    # ('time_series_identifier_column', str),\n",
    "    # ('time_column', str),\n",
    "    # ('target_column', str),\n",
    "    # ('predefined_split_column_name', str),\n",
    "]):\n",
    "    \n",
    "    # here\n",
    "    import os\n",
    "    import json\n",
    "    import pickle as pkl\n",
    "    from pprint import pprint\n",
    "    import logging\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "    # google cloud\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "    \n",
    "    # set clients\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    # check vars in the logs\n",
    "    logging.info(f\"column_spec_gcs_bucket_name: {column_spec_gcs_bucket_name}\")\n",
    "    logging.info(f\"column_spec_gcs_blob_path: {column_spec_gcs_blob_path}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # helper function for downloading gcs blob\n",
    "    # ===================================================\n",
    "    def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # storage_client = storage.Client(project=project_number)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_gcs_obj)\n",
    "        blob.download_to_filename(local_filename)\n",
    "        \n",
    "        filehandler = open(f'{local_filename}', 'rb')\n",
    "        loaded_dict = pkl.load(filehandler)\n",
    "        filehandler.close()\n",
    "        \n",
    "        logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "        \n",
    "        return loaded_dict\n",
    "    \n",
    "    # ===================================================\n",
    "    # load pickled column specs\n",
    "    # ===================================================\n",
    "    \n",
    "    # candidate features\n",
    "    LOCAL_COL_FILE = 'lodaded_column_specs.pkl'\n",
    "    logging.info(f\"LOCAL_COL_FILE: {LOCAL_COL_FILE}\")\n",
    "    \n",
    "    loaded_column_spec_dict = download_blob(\n",
    "        column_spec_gcs_bucket_name,\n",
    "        column_spec_gcs_blob_path,\n",
    "        LOCAL_COL_FILE\n",
    "    )\n",
    "    \n",
    "    COL_TRANSFORMS = loaded_column_spec_dict['column_specs']\n",
    "    UNAVAILABLE_AT_FORECAST_COLS = loaded_column_spec_dict['unavailable_at_forecast_columns']\n",
    "    AVAILABLE_AT_FORECAST_COLS = loaded_column_spec_dict['available_at_forecast_columns']\n",
    "    SERIES_COLUMN = loaded_column_spec_dict['time_series_identifier_column']\n",
    "    TIME_COLUMN = loaded_column_spec_dict['time_column']\n",
    "    TARGET_COLUMN = loaded_column_spec_dict['target_column']\n",
    "    PREDEFINED_SPLIT_COL = loaded_column_spec_dict['predefined_split_column_name']\n",
    "    \n",
    "    model_feature_columns = [\n",
    "        PREDEFINED_SPLIT_COL,\n",
    "        SERIES_COLUMN,\n",
    "        TARGET_COLUMN,\n",
    "        TIME_COLUMN\n",
    "    ]\n",
    "    for col in UNAVAILABLE_AT_FORECAST_COLS:\n",
    "        model_feature_columns.append(col)\n",
    "\n",
    "    for col in AVAILABLE_AT_FORECAST_COLS:\n",
    "        model_feature_columns.append(col)\n",
    "        \n",
    "    logging.info(f\"model_feature_columns: {model_feature_columns}\")\n",
    "    # ===================================================\n",
    "    # train table specs\n",
    "    # ===================================================\n",
    "    activities_table_specs = {\n",
    "        'bigquery_uri': bq_source_train_uri,\n",
    "        'table_type': 'FORECASTING_PRIMARY',\n",
    "        'forecasting_primary_table_metadata': {\n",
    "            'time_column': TIME_COLUMN,\n",
    "            'target_column': TARGET_COLUMN,\n",
    "            'time_series_identifier_column': SERIES_COLUMN,\n",
    "            'unavailable_at_forecast_columns': UNAVAILABLE_AT_FORECAST_COLS,\n",
    "            'time_granularity': {\n",
    "                'unit': data_granularity_unit,\n",
    "                'quantity': data_granularity_count,\n",
    "            },\n",
    "            'predefined_splits_column':PREDEFINED_SPLIT_COL,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    return (\n",
    "        json.dumps(activities_table_specs),  # input_table_specs\n",
    "        json.dumps(model_feature_columns),  # model_feature_columns\n",
    "\n",
    "        # json.dumps(COL_TRANSFORMS), # json array\n",
    "        # json.dumps(UNAVAILABLE_AT_FORECAST_COLS), # json array\n",
    "        # json.dumps(AVAILABLE_AT_FORECAST_COLS), # json array\n",
    "        # f'{SERIES_COLUMN}', # string\n",
    "        # f'{TIME_COLUMN}', # string\n",
    "        # f'{TARGET_COLUMN}', # string\n",
    "        # f'{PREDEFINED_SPLIT_COL}', # string\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fc22c-734b-4675-8b5b-010015d39012",
   "metadata": {},
   "source": [
    "see `AutoMLForecastingTrainingJobRunOp` input data types [here](https://github.com/kubeflow/pipelines/blob/google-cloud-pipeline-components-1.0.41/components/google-cloud/google_cloud_pipeline_components/aiplatform/automl_training_job/automl_forecasting_training_job/component.yaml)\n",
    "\n",
    "see `ForecastingValidationOp` input [here](https://github.com/kubeflow/pipelines/blob/google-cloud-pipeline-components-1.0.41/components/google-cloud/google_cloud_pipeline_components/experimental/forecasting/validate/component.yaml)\n",
    "\n",
    "see`ForecastingPrepareDataForTrainOp` input [here](https://github.com/kubeflow/pipelines/blob/google-cloud-pipeline-components-1.0.41/components/google-cloud/google_cloud_pipeline_components/experimental/forecasting/prepare_data_for_train/component.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815062c4-0bab-4f80-8fb0-2fbae0817c65",
   "metadata": {},
   "source": [
    "### interpret AutoML metrics\n",
    "\n",
    "Since we don't have a pre-built-component to access these metrics programmatically, we can use the Vertex AI GAPIC (Google API Compiler), which auto-generates low-level gRPC interfaces to the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4174138d-0fa5-4062-988e-d35daa9eb925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://google-cloud-aiplatform/schema/modelevaluation/classification_metrics_1.0.0.yaml\n",
      "gs://google-cloud-aiplatform/schema/modelevaluation/forecasting_metrics_1.0.0.yaml\n",
      "gs://google-cloud-aiplatform/schema/modelevaluation/image_object_detection_metrics_1.0.0.yaml\n",
      "gs://google-cloud-aiplatform/schema/modelevaluation/regression_metrics_1.0.0.yaml\n",
      "gs://google-cloud-aiplatform/schema/modelevaluation/text_extraction_metrics_1.0.0.yaml\n",
      "gs://google-cloud-aiplatform/schema/modelevaluation/text_sentiment_metrics_1.0.0.yaml\n",
      "gs://google-cloud-aiplatform/schema/modelevaluation/video_action_recognition_metrics_1.0.0.yaml\n",
      "gs://google-cloud-aiplatform/schema/modelevaluation/video_object_tracking_metrics_1.0.0.yaml\n"
     ]
    }
   ],
   "source": [
    "# For a list of available model metrics, go here:\n",
    "!gsutil ls gs://google-cloud-aiplatform/schema/modelevaluation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1ee7540-bede-47ea-845f-026f70d409d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform==1.22.1\",\n",
    "    ],\n",
    ")\n",
    "def interpret_automl_evaluation_metrics(\n",
    "    region: str, \n",
    "    model: Input[Artifact], \n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    \"\"\"'\n",
    "    For a list of available forecast metrics, go here: \n",
    "        gs://google-cloud-aiplatform/schema/modelevaluation/forecasting_metrics_1.0.0.yaml\n",
    "\n",
    "    More information on available metrics for different types of models: \n",
    "        https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-automl\n",
    "    \"\"\"\n",
    "\n",
    "    import google.cloud.aiplatform.gapic as gapic\n",
    "\n",
    "    # Get a reference to the Model Service client\n",
    "    client_options = {\"api_endpoint\": f\"{region}-aiplatform.googleapis.com\"}\n",
    "\n",
    "    model_service_client = gapic.ModelServiceClient(client_options=client_options)\n",
    "\n",
    "    model_resource_name = model.metadata[\"resourceName\"]\n",
    "\n",
    "    model_evaluations = model_service_client.list_model_evaluations(\n",
    "        parent=model_resource_name\n",
    "    )\n",
    "    model_evaluation = list(model_evaluations)[0]\n",
    "\n",
    "    # metrics_dict = {k: [v] for k, v in dict(model_evaluation.metrics).items()}\n",
    "\n",
    "    available_metrics = [\n",
    "        'rootMeanSquaredLogError',\n",
    "        'rSquared',\n",
    "        'meanAbsoluteError',\n",
    "        'meanAbsolutePercentageError',\n",
    "        'rootMeanSquaredError',\n",
    "        'rootMeanSquaredPercentageError',\n",
    "        'weightedAbsolutePercentageError',\n",
    "    ]\n",
    "    output = dict()\n",
    "    for x in available_metrics:\n",
    "        val = model_evaluation.metrics.get(x)\n",
    "        output[x] = val\n",
    "        metrics.log_metric(str(x), float(val))\n",
    "\n",
    "    metrics.log_metric(\"framework\", \"AutoML\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f3619-3ef9-453d-a0e3-15c751c52a11",
   "metadata": {},
   "source": [
    "### get eval dataset uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74f8f619-89af-480e-b162-a28973a0ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(base_image='python:3.9')\n",
    "def get_eval_dataset_path_uri(\n",
    "    project: str,\n",
    "    eval_bq_dataset: str,\n",
    "    model_name: str,\n",
    ") -> NamedTuple('Outputs',[\n",
    "    ('model_eval_bigquery_table_uri', str),\n",
    "    ('eval_bq_dataset', str),\n",
    "]):\n",
    "    \n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    model_1_table_path_name = f'{project}:{eval_bq_dataset}:eval_{model_name}'\n",
    "\n",
    "    logging.info(model_1_table_path_name)\n",
    "\n",
    "    return (\n",
    "        f'bq://{model_1_table_path_name}',\n",
    "        f'{eval_bq_dataset}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ab75c-c924-49f7-8f0c-a05ef5127577",
   "metadata": {},
   "source": [
    "## Build pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82d736-83a1-469c-878b-003300ba3f96",
   "metadata": {},
   "source": [
    "### define vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "501651a1-5ca5-4175-a826-d04041a7ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v1'\n",
    "PIPELINE_TAG = 'train'\n",
    "BUCKET_NAME = 'vertex-forecast-22'\n",
    "EXPERIMENT_PREFIX = 'm5-simple-pipe' # custom identifier for organizing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a6f401c-e980-4535-aebe-c00fe59db21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: m5-simple-pipe-v1\n",
      "RUN_NAME: run-20230329-180343\n",
      "EXPERIMENT_GCS_DIR: gs://vertex-forecast-22/m5-simple-pipe-v1/run-20230329-180343\n",
      "PIPELINE_ROOT_PATH: gs://vertex-forecast-22/m5-simple-pipe-v1/run-20230329-180343/pipeline_root\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{VERSION}'\n",
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "GCS_BUCKET_URI =f'gs://{BUCKET_NAME}'\n",
    "EXPERIMENT_GCS_DIR = f'gs://{BUCKET_NAME}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "\n",
    "# Stores pipeline executions for each run\n",
    "PIPELINE_ROOT_PATH = f'{EXPERIMENT_GCS_DIR}/pipeline_root'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")\n",
    "print(f'EXPERIMENT_GCS_DIR: {EXPERIMENT_GCS_DIR}')\n",
    "print(f'PIPELINE_ROOT_PATH: {PIPELINE_ROOT_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63310bad-ea20-4acc-bb22-4fbf32b68f49",
   "metadata": {},
   "source": [
    "### define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c961d894-a6d4-41f3-8ffc-189f306f5caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_NAME: train-m5-simple-pipe-v1\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME = f'{PIPELINE_TAG}-{EXPERIMENT_NAME}'.replace('_', '-')\n",
    "print(f\"PIPELINE_NAME: {PIPELINE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b997f34-6f80-4c00-980c-d4d8c7e7af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.pipeline(\n",
    "  name=PIPELINE_NAME\n",
    ")\n",
    "def pipeline(\n",
    "    vertex_project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    new_bq_dataset: str,\n",
    "    bq_location: str,\n",
    "    bq_source_train_uri: str,\n",
    "    column_spec_gcs_bucket_name: str,\n",
    "    column_spec_gcs_blob_path:str,\n",
    "    context_window: int,\n",
    "    forecast_horizon: int,\n",
    "    budget_milli_node_hours: int,\n",
    "    data_granularity_unit: str,\n",
    "    data_granularity_count: int,\n",
    "    optimization_objective: str,\n",
    "    model_name: str,\n",
    "):\n",
    "    \n",
    "    # cretae BQ dataset\n",
    "    create_bq_dataset_op = (\n",
    "        create_bq_dataset(\n",
    "            project=vertex_project,\n",
    "            new_bq_dataset=new_bq_dataset,\n",
    "            bq_location=bq_location\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ======================================\n",
    "    # prep data for forecast train job\n",
    "    # ======================================\n",
    "    \n",
    "    # get column specs\n",
    "    create_input_table_specs_op = (\n",
    "        create_input_table_specs(\n",
    "            project=vertex_project,\n",
    "            location=location,\n",
    "            bq_source_train_uri=bq_source_train_uri,\n",
    "            column_spec_gcs_bucket_name=column_spec_gcs_bucket_name,\n",
    "            column_spec_gcs_blob_path=column_spec_gcs_blob_path,\n",
    "            data_granularity_unit=data_granularity_unit,\n",
    "            data_granularity_count=data_granularity_count,\n",
    "        )\n",
    "        .after(create_bq_dataset_op)\n",
    "    )\n",
    "    # prebuilt component for forecast data validation\n",
    "    forecasting_validation_op = (\n",
    "        gcc_aip_forecasting.ForecastingValidationOp(\n",
    "            input_tables=str(create_input_table_specs_op.outputs['input_table_specs']),\n",
    "            validation_theme='FORECASTING_TRAINING',\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # prebuilt component for forecast data preprocessing\n",
    "    forecasting_preprocessing_op = (\n",
    "      gcc_aip_forecasting.ForecastingPreprocessingOp(\n",
    "          project=vertex_project,\n",
    "          input_tables=str(create_input_table_specs_op.outputs['input_table_specs']),\n",
    "          preprocessing_bigquery_dataset=create_bq_dataset_op.outputs['bq_dataset_name'],\n",
    "          location=bq_location\n",
    "      )\n",
    "    )\n",
    "    \n",
    "    # prebuilt component for forecast data prepping\n",
    "    prepare_data_for_train_op = (\n",
    "      gcc_aip_forecasting.ForecastingPrepareDataForTrainOp(\n",
    "          input_tables=(\n",
    "              str(create_input_table_specs_op.outputs['input_table_specs'])\n",
    "          ),\n",
    "          preprocess_metadata=(\n",
    "              forecasting_preprocessing_op.outputs['preprocess_metadata']\n",
    "          ),\n",
    "          model_feature_columns=(\n",
    "              str(create_input_table_specs_op.outputs['model_feature_columns'])\n",
    "          )\n",
    "      )\n",
    "    )\n",
    "    \n",
    "    # pre-built pipeline component for creating Managed dataset (lineage tracking)\n",
    "    time_series_dataset_create_op = (\n",
    "      gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "          display_name=f'train_m5_pipe_{version}',\n",
    "          bq_source=prepare_data_for_train_op.outputs['preprocess_bq_uri'],\n",
    "          project=vertex_project,\n",
    "          location=location,\n",
    "      )\n",
    "    )\n",
    "    \n",
    "    # get eval BQ destination\n",
    "    get_eval_dataset_path_uri_op = (\n",
    "        get_eval_dataset_path_uri(\n",
    "            project=vertex_project,\n",
    "            eval_bq_dataset=create_bq_dataset_op.outputs['bq_dataset_name'],\n",
    "            model_name=model_name,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ======================================\n",
    "    # model training with Vertex Forecast\n",
    "    # ======================================\n",
    "    \n",
    "    # vertex forecast automl train job\n",
    "    model_training_op = (\n",
    "      gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "          project=vertex_project,\n",
    "          location=location,\n",
    "          display_name=f'train-{model_name}-simple-pipe-{version}',\n",
    "          model_display_name=f'{model_name}-simple-pipe-{version}',\n",
    "          dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "          optimization_objective=optimization_objective,\n",
    "          context_window=context_window,\n",
    "          forecast_horizon=forecast_horizon,\n",
    "          budget_milli_node_hours=budget_milli_node_hours,\n",
    "          export_evaluated_data_items=True,\n",
    "          export_evaluated_data_items_override_destination=True,\n",
    "          export_evaluated_data_items_bigquery_destination_uri=get_eval_dataset_path_uri_op.outputs['model_eval_bigquery_table_uri'],\n",
    "          target_column=prepare_data_for_train_op.outputs['target_column'],\n",
    "          time_column=prepare_data_for_train_op.outputs['time_column'],\n",
    "          time_series_identifier_column=prepare_data_for_train_op.outputs['time_series_identifier_column'],\n",
    "          # time_series_attribute_columns=prepare_data_for_train_op.outputs['time_series_attribute_columns'],\n",
    "          unavailable_at_forecast_columns=prepare_data_for_train_op.outputs['unavailable_at_forecast_columns'],\n",
    "          available_at_forecast_columns=prepare_data_for_train_op.outputs['available_at_forecast_columns'],\n",
    "          predefined_split_column_name=prepare_data_for_train_op.outputs['predefined_split_column'],\n",
    "          #\n",
    "          data_granularity_unit=data_granularity_unit,\n",
    "          data_granularity_count=data_granularity_count,\n",
    "          holiday_regions=['GLOBAL', 'NA', 'US'],\n",
    "      )\n",
    "    )\n",
    "    \n",
    "    # get trained model\n",
    "    automl_model = model_training_op.outputs[\"model\"]\n",
    "    \n",
    "    # ======================================\n",
    "    # Vertex Forecast model evaluation\n",
    "    # ======================================\n",
    "    \n",
    "    # Analyzes evaluation AutoML metrics using a custom component.\n",
    "    automl_eval_op = (\n",
    "        interpret_automl_evaluation_metrics(\n",
    "            region=location,\n",
    "            model=automl_model\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    automl_eval_metrics = automl_eval_op.outputs[\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29619be6-a5b5-4240-b33f-d72413f2d2b6",
   "metadata": {},
   "source": [
    "## compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "399d05da-75a5-4f4c-9d80-91e4327680b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON_SPEC_LOCAL = \"custom_pipeline_spec.json\"\n",
    "\n",
    "! rm -f $PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83bcb537-d6bc-4ff0-b915-5680f387877a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINES_FILEPATH: gs://vertex-forecast-22/m5-simple-pipe-v1/run-20230329-180343/pipeline_root/pipeline_spec.json\n",
      "Copying file://custom_pipeline_spec.json [Content-Type=application/json]...\n",
      "/ [1 files][ 51.7 KiB/ 51.7 KiB]                                                \n",
      "Operation completed over 1 objects/51.7 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT_PATH}/pipeline_spec.json'\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "!gsutil cp $PIPELINE_JSON_SPEC_LOCAL $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58204b23-c1d7-44ff-a678-0274182e94ce",
   "metadata": {},
   "source": [
    "## copy artifacts to gcs\n",
    "\n",
    "> helps with tracking, and accessing files in pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "01118ced-6e6e-47fe-9e7d-0c835f9e0770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://column_specs.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  892.0 B/  892.0 B]                                                \n",
      "Operation completed over 1 objects/892.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "LOCAL_COL_SPEC_FILE='column_specs.pkl'\n",
    "\n",
    "!gsutil cp $LOCAL_COL_SPEC_FILE $EXPERIMENT_GCS_DIR/$LOCAL_COL_SPEC_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "699968b0-c1e2-4ca2-8265-f3a387d61138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://vertex-forecast-22/m5-simple-pipe-v1/run-20230329-180343/column_specs.pkl\n",
      "gs://vertex-forecast-22/m5-simple-pipe-v1/run-20230329-180343/pipeline_root/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $EXPERIMENT_GCS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4b6c8-48b9-460a-966c-599a209e5ae9",
   "metadata": {},
   "source": [
    "# submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1aee641d-550e-4877-bc3a-8fd2dda11654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW_BQ_DATASET: m5_nb7\n",
      "COL_SPEC_BQ_BLOB_PATH: m5-simple-pipe-v1/run-20230329-180343/column_specs.pkl\n",
      "\n",
      "{'bq_location': 'US',\n",
      " 'bq_source_train_uri': 'bq://hybrid-vertex.m5_us.sdk_train_prepped',\n",
      " 'budget_milli_node_hours': 1000,\n",
      " 'column_spec_gcs_blob_path': 'm5-simple-pipe-v1/run-20230329-180343/column_specs.pkl',\n",
      " 'column_spec_gcs_bucket_name': 'vertex-forecast-22',\n",
      " 'context_window': 14,\n",
      " 'data_granularity_count': 1,\n",
      " 'data_granularity_unit': 'day',\n",
      " 'forecast_horizon': 14,\n",
      " 'location': 'us-central1',\n",
      " 'model_name': 'm5-nb7',\n",
      " 'new_bq_dataset': 'm5_nb7',\n",
      " 'optimization_objective': 'minimize-rmse',\n",
      " 'version': 'v1',\n",
      " 'vertex_project': 'hybrid-vertex'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# TODO: Service Account address\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com' \n",
    "\n",
    "MODEL_NAME = 'm5-nb7'\n",
    "NEW_BQ_DATASET = MODEL_NAME.replace(\"-\", \"_\")\n",
    "\n",
    "BQ_TRAIN_SOURCE='bq://hybrid-vertex.m5_us.sdk_train_prepped'\n",
    "COL_SPEC_BQ_BLOB_PATH=f'{EXPERIMENT_NAME}/{RUN_NAME}/{LOCAL_COL_SPEC_FILE}'\n",
    "\n",
    "# model config\n",
    "OPTIMIZATION_OBJECTIVE=\"minimize-rmse\"\n",
    "MILLI_NODE_HRS=1000\n",
    "# HOLIDAY_REGIONS=['GLOBAL', 'NA', 'US']\n",
    "FORECAST_GRANULARITY = 'DAY'\n",
    "DATA_GRANULARITY_COUNT=1\n",
    "FORECAST_HORIZON = 14\n",
    "CONTEXT_WINDOW = 14\n",
    "\n",
    "\n",
    "parameter_values={\n",
    "    'vertex_project': PROJECT_ID,\n",
    "    'location': LOCATION,\n",
    "    'version': VERSION,\n",
    "    'new_bq_dataset': NEW_BQ_DATASET,\n",
    "    'bq_location': BQ_LOCATION,\n",
    "    'bq_source_train_uri': BQ_TRAIN_SOURCE,\n",
    "    'column_spec_gcs_bucket_name': BUCKET_NAME,\n",
    "    'column_spec_gcs_blob_path':COL_SPEC_BQ_BLOB_PATH,\n",
    "    'context_window': CONTEXT_WINDOW,\n",
    "    'forecast_horizon': FORECAST_HORIZON,\n",
    "    'budget_milli_node_hours': MILLI_NODE_HRS,\n",
    "    'data_granularity_unit': FORECAST_GRANULARITY.lower(),\n",
    "    'data_granularity_count': DATA_GRANULARITY_COUNT,\n",
    "    'optimization_objective': OPTIMIZATION_OBJECTIVE,\n",
    "    'model_name': MODEL_NAME,\n",
    "}\n",
    "print(f\"NEW_BQ_DATASET: {NEW_BQ_DATASET}\")\n",
    "print(f\"COL_SPEC_BQ_BLOB_PATH: {COL_SPEC_BQ_BLOB_PATH}\\n\")\n",
    "pprint(parameter_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eb1bb96c-27e8-4a56-8650-6a285eaac1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/train-m5-simple-pipe-v1-20230329202744\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/train-m5-simple-pipe-v1-20230329202744')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-m5-simple-pipe-v1-20230329202744?project=934903580331\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-simple-pipe-v1-20230329202744 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-simple-pipe-v1-20230329202744 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-simple-pipe-v1-20230329202744 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/train-m5-simple-pipe-v1-20230329202744 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINES_FILEPATH,\n",
    "    pipeline_root=f'{PIPELINE_ROOT_PATH}',\n",
    "    failure_policy='fast', # slow | fast\n",
    "    # enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe312e-7ae1-4ffa-91fb-e0943d43f088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
