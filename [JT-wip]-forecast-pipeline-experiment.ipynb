{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d059dd-7c69-4295-935d-0a80e2672ccb",
   "metadata": {},
   "source": [
    "# Run multiple experiments with pipeline runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278ac06-aba4-4113-9d4f-92a1993e51f3",
   "metadata": {},
   "source": [
    "* some ideas [from here](https://codelabs.developers.google.com/vertex_experiments_pipelines_intro#5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4658d0ed-ed52-476c-a2a3-0e7ce224d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1'\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4afb64a-826a-4a37-a648-d91d35ed1f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "REGION: us-central1\n",
      "env: GOOGLE_CLOUD_PROJECT=hybrid-vertex\n"
     ]
    }
   ],
   "source": [
    "# GCP Project Configuration:\n",
    "# project where pipeline and vertex jobs are executed\n",
    "\n",
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "REGION = 'us-central1'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"REGION: {REGION}\")\n",
    "\n",
    "assert LOCATION, 'the value for this variable must be set'\n",
    "assert PROJECT_ID, 'the value for this variable must be set'\n",
    "# assert PROJECT_NUMBER, 'the value for this variable must be set'\n",
    "\n",
    "%env GOOGLE_CLOUD_PROJECT={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4225a216-ba2f-49a7-b999-c42df7d5c76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertex_ai SDK version: 1.21.0\n",
      "bigquery SDK version: 2.34.4\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "# from matplotlib import dates as mdates\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# google cloud\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental import forecasting as gcc_aip_forecasting\n",
    "\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# kfp\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "print(f'vertex_ai SDK version: {vertex_ai.__version__}')\n",
    "print(f'bigquery SDK version: {bigquery.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924d94c-322c-4ae0-8e73-f02b6ae47f4b",
   "metadata": {},
   "source": [
    "### setup clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ceea662-946a-44ef-9a48-15f38c88e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID, \n",
    "    # credentials=credentials\n",
    ")\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696112fb-51b7-48fe-a87e-e77a6cf05d62",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b76450c-d203-4db5-8e6a-76cf6ac8f278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-forecas-repo\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf603e6-557f-4267-9e92-15abf08a16c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3391641767.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/tmp/ipykernel_7079/3391641767.py\"\u001b[0;36m, line \u001b[0;32m42\u001b[0m\n\u001b[0;31m    create_input_table_specs_op = (\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from src import (\n",
    "    create_bq_dataset,create_input_table_specs, get_eval_dataset_path_uri,\n",
    "    create_combined_preds_table, create_forecast_input_table_specs, get_predict_table_path,\n",
    "    model_batch_prediction_job, create_combined_preds_forecast_table, get_model_path,\n",
    "    create_final_pred_table, args_generator_op_1\n",
    ")\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=PIPELINE_NAME\n",
    ")\n",
    "def pipeline(\n",
    "    vertex_project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    data_source_dataset: str,\n",
    "    eval_destination_dataset: str,\n",
    "    preprocess_dataset_us: str,\n",
    "    model_version: str,\n",
    "    model_display_name: str,\n",
    "    context_window: str,\n",
    "    forecast_horizon: str,\n",
    "    budget_milli_node_hours: str,\n",
    "    optimization_objective: str,\n",
    "\n",
    "# TODO\n",
    "\n",
    "\n",
    "    # create BQ dataset\n",
    "    create_train_dataset_op = (\n",
    "      create_bq_dataset.create_bq_dataset(\n",
    "          project=vertex_project,\n",
    "          vertex_dataset=data_source_dataset,\n",
    "          new_bq_dataset=eval_destination_dataset,\n",
    "          bq_location=location\n",
    "      )\n",
    "    )\n",
    "\n",
    "    # ======================================\n",
    "    # prep train jobs\n",
    "    # ======================================\n",
    "\n",
    "    create_input_table_specs_op = (\n",
    "        create_input_table_specs.create_input_table_specs(\n",
    "            products_table_uri=products_table_uri,\n",
    "            activities_table_uri=activities_table_uri,\n",
    "            locations_table_uri=locations_table_uri,\n",
    "            time_granularity_unit=time_granularity_unit,\n",
    "            time_granularity_quantity=time_granularity_quantity,\n",
    "            # train_data_bq_source=train_data_bq_source,\n",
    "        )\n",
    "        .after(create_train_dataset_op)\n",
    "    )\n",
    "\n",
    "    forecasting_validation_op = (\n",
    "        gcc_aip_forecasting.ForecastingValidationOp(\n",
    "            input_tables=str(create_input_table_specs_op.outputs['input_table_specs']),\n",
    "            validation_theme='FORECASTING_TRAINING',\n",
    "      )\n",
    "    )\n",
    "\n",
    "    forecasting_preprocessing_op = (\n",
    "      gcc_aip_forecasting.ForecastingPreprocessingOp(\n",
    "          project=vertex_project,\n",
    "          input_tables=str(create_input_table_specs_op.outputs['input_table_specs']),\n",
    "          preprocessing_bigquery_dataset=data_source_dataset,\n",
    "      )\n",
    "      .after(forecasting_validation_op)\n",
    "    )\n",
    "\n",
    "    prepare_data_for_train_op = (\n",
    "      gcc_aip_forecasting.ForecastingPrepareDataForTrainOp(\n",
    "          input_tables=(\n",
    "              str(create_input_table_specs_op.outputs['input_table_specs'])\n",
    "          ),\n",
    "          preprocess_metadata=(\n",
    "              forecasting_preprocessing_op.outputs['preprocess_metadata']\n",
    "          ),\n",
    "          model_feature_columns=(\n",
    "              str(create_input_table_specs_op.outputs['model_feature_columns'])\n",
    "          )\n",
    "      )\n",
    "    )\n",
    "\n",
    "    time_series_dataset_create_op = (\n",
    "      gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "          display_name=f'train_ds_full_m5_{VERSION}',\n",
    "          bq_source=prepare_data_for_train_op.outputs['preprocess_bq_uri'],\n",
    "          project=vertex_project,\n",
    "          location=location,\n",
    "      )\n",
    "    )\n",
    "\n",
    "    mape_model_version = f'{VERSION}-seq2seq-mape'\n",
    "    rmse_model_version = f'{VERSION}-seq2seq-rmse' \n",
    "\n",
    "    get_eval_dataset_path_uri_op = (\n",
    "      get_eval_dataset_path_uri.get_eval_dataset_path_uri(\n",
    "          project=vertex_project,\n",
    "          eval_bq_dataset=create_train_dataset_op.outputs['bq_dataset_name'],\n",
    "          model_1_table=mape_model_version,\n",
    "          model_2_table=rmse_model_version,\n",
    "      )\n",
    "    )\n",
    "    \n",
    "    ## training\n",
    "    train_model_op = (\n",
    "      gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "          display_name=f'train-{model_version}',\n",
    "          model_display_name=model_display_name,\n",
    "          model_labels={'model_override' : 'se2seq-hier'}, # model_override : se2seq-hier, tft\n",
    "          # model_labels={'model_type' : 'l2l'},\n",
    "          dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "          context_window=context_window,\n",
    "          forecast_horizon=forecast_horizon,\n",
    "          budget_milli_node_hours=budget_milli_node_hours,\n",
    "          project=vertex_project,\n",
    "          location=location,\n",
    "          export_evaluated_data_items=True,\n",
    "          export_evaluated_data_items_bigquery_destination_uri=get_eval_dataset_path_uri_op.outputs['model_1_bigquery_table_uri'], # must be format:``bq://<project_id>:<dataset_id>:<table>``\n",
    "          export_evaluated_data_items_override_destination=True,\n",
    "          target_column=prepare_data_for_train_op.outputs['target_column'],\n",
    "          time_column=prepare_data_for_train_op.outputs['time_column'],\n",
    "          time_series_identifier_column=prepare_data_for_train_op.outputs['time_series_identifier_column'],\n",
    "          time_series_attribute_columns=prepare_data_for_train_op.outputs['time_series_attribute_columns'],\n",
    "          unavailable_at_forecast_columns=prepare_data_for_train_op.outputs['unavailable_at_forecast_columns'],\n",
    "          available_at_forecast_columns=prepare_data_for_train_op.outputs['available_at_forecast_columns'],\n",
    "          data_granularity_unit=prepare_data_for_train_op.outputs['data_granularity_unit'],\n",
    "          data_granularity_count=prepare_data_for_train_op.outputs['data_granularity_count'],\n",
    "          predefined_split_column_name= '', # prepare_data_for_train_op.outputs['predefined_split_column'],\n",
    "          column_transformations=prepare_data_for_train_op.outputs['column_transformations'],\n",
    "          weight_column=prepare_data_for_train_op.outputs['weight_column'],\n",
    "          optimization_objective=optimization_objective,\n",
    "          additional_experiments={\n",
    "              'forecasting_model_type_override': 'seq2seq',\n",
    "              'forecasting_hierarchical_group_column_names':'dept_id, cat_id'},\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60a3cc-27a3-48c8-a3ab-08960b69da99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
